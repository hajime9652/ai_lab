{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の処理を実装してください\n",
    "\n",
    "- センチメント分析(\"daigo/bert-base-japanese-sentiment\")\n",
    "- 穴埋め(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "- 要約(\"tsmatz/mt5_summarize_japanese\")\n",
    "- テキスト生成(\"rinna/japanese-gpt2-medium\")\n",
    "\n",
    "[Hubで日本語モデルを検索](https://huggingface.co/models?search=japanese)すると、各モデルの詳細が記載されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'ポジティブ', 'score': 0.9843042492866516}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "analyzer = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\")\n",
    "analyzer(\"私は幸福である。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0572928860783577,\n",
       "  'token': 4073,\n",
       "  'token_str': '医学',\n",
       "  'sequence': '東北大学 で 医学 の 研究 を し て い ます 。'},\n",
       " {'score': 0.05241795629262924,\n",
       "  'token': 11424,\n",
       "  'token_str': '考古学',\n",
       "  'sequence': '東北大学 で 考古学 の 研究 を し て い ます 。'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "unmasker(\"東北大学で[MASK]の研究をしています。\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '人間の知能をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステムが、世界各地で注目を集めている。この技術は、人間にとって最も重要な役割を果たしている。'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"tsmatz/mt5_summarize_japanese\")\n",
    "\n",
    "text = \"人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム。\\\n",
    "    応用例としては、自然言語処理（機械翻訳・かな漢字変換・構文解析・文章要約等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析し特定のパターンを検出・抽出する画像認識等がある。\\\n",
    "    1956年にダートマス会議でジョン・マッカーシーにより命名された。\\\n",
    "    現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている。\\\n",
    "    家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある。\\\n",
    "    プログラミング言語 LISP による「ELIZA」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、\\\n",
    "    計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、当時のその技術では実用化は困難と考えられていた。\\\n",
    "    人工的な知能の実現へのアプローチとしては、「ファジィ理論」や「ニューラルネットワーク」などのようなアプローチも知られているが、従来の人工知能であるGOFAI（Good Old Fashioned AI）との差は記述の記号的明示性にある。\\\n",
    "    その後「サポートベクターマシン」が注目を集めた。また、自らの経験を元に学習を行う強化学習という手法もある。\\\n",
    "    「この宇宙において、知性とは最も強力な形質である（レイ・カーツワイル）」という言葉通り、知性を機械的に表現し実装するということは極めて重要な作業である。\\\n",
    "    画像処理におけるディープラーニングの有用性が競技会で世界的に認知された2012年頃から急速に研究が活発となり、第3次人工知能ブームが到来。2016年から2017年にかけて、\\\n",
    "    ディープラーニングを導入したAIが完全情報ゲームである囲碁などのトップ棋士、さらに不完全情報ゲームであるポーカーの世界トップクラスのプレイヤーも破り、\\\n",
    "    麻雀では「Microsoft Suphx（Super Phoenix）」がオンライン対戦サイト「天鳳」でAIとして初めて十段に到達するなど最先端技術として注目されている。\\\n",
    "    第3次人工知能ブームの主な革命は、自然言語処理、センサーによる画像処理など視覚的側面が特に顕著であるが、社会学、倫理学、技術開発、経済学などの分野にも大きな影響を及ぼしている。\"\n",
    "\n",
    "summarizer(text, max_length=100, min_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '生命、宇宙、そして万物についての究極の疑問の答えは、すべてがひとつに結ばれているという宇宙原理にある。この究極の疑問の答えが何であるのか、それを確かめるには、あらゆるものごとが、無き、無害、無害、無害であるといえない世界が存在することを確信するしか方法はない。「人間は、どんな者でも、自分が何者であるかを知り、それを確信することで、自己(自己)の内なる、自己のための、'},\n",
       " {'generated_text': '生命、宇宙、そして万物についての究極の疑問の答えは、この世界を生命が支配していると主張することで得られるのかも知れない。 科学がなければ生命の謎は解けない。また、この世界を科学の力で解明する試みもまた、科学の延長にあるといえるだろう。 宇宙の謎は、宇宙の誕生、地球誕生から現代に至る大きな流れである宇宙の根源的な謎と言える。 この記事へのトラックバック一覧です: 科学と芸術:地球からの'},\n",
       " {'generated_text': '生命、宇宙、そして万物についての究極の疑問の答えは、われわれに「神の力」という不思議な力があることを思い出させる。 いまや、われわれは神に導かれるように、その力を活用して生を享受し、生から死までの全ての過程を体験している。 それは、神の存在自体を否定するものではなく、あくまでもそれを受容した経験が神の教えを実践する根拠となっている。 聖書には、「神は宇宙をひとつの存在としておられる」という一つの大きな'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "\n",
    "generator = pipeline('text-generation', model=\"rinna/japanese-gpt2-medium\", tokenizer=tokenizer)\n",
    "\n",
    "generator(\"生命、宇宙、そして万物についての究極の疑問の答えは、\", max_length = 100, num_return_sequences=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling multiple sequences\n",
    "- “I’ve been waiting for a HuggingFace course my whole life.” と “I hate this so much!”に手動でトークン化を適用してください。\n",
    "- これらをモデルに通して、セクション2と同じロジットが得られることを確認してください。(tensor([[-1.5607,  1.6123],[ 4.1692, -3.3464]], grad_fn=<AddmmBackward>))\n",
    "- 次に、パディングトークンを使ってそれらをバッチ処理し、適切なアテンションマスクを作成します。モデルを通したときに、同じ結果が得られるかどうかチェックしてください(waiting for と hateを含むそれぞれの文が適切に分類されていますか？)\n",
    "\n",
    "\n",
    "MRPC\n",
    "- 訓練集合の要素15と検証集合の要素87を確認してください。それらのラベルは何ですか？\n",
    "- トレーニングセットの要素15を取り出し、2つの文章を別々に、そしてペアとしてトークン化します。2つの結果の違いは何ですか？\n",
    "\n",
    "GLUE SST-2\n",
    "- MRPCと同等な前処理を再現してください。ペアではなく単文で構成されているので少し違いますが、それ以外の部分は同じように見えるはずです。\n",
    "- GLUEタスクのいずれでも動作する前処理関数を書いてみてください。\n",
    "- モデルのファインチューニングを行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eaac4ee8e735b6cee17c1b0358cfc8773b6176eb143c6cf3f1e52e7612ffbee2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
