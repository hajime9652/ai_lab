{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensors\n",
        "\n",
        "テンソルは配列や行列に非常によく似た特殊なデータ構造です。\n",
        "PyTorchでは、モデルの入力と出力、およびモデルのパラメータをエンコードするためにテンソルを使用します。\n",
        "\n",
        "テンソルは[NumPy](https://numpy.org/)のndarraysに似ていますが、テンソルはGPUや他のハードウェアアクセラレータ上で動作することができる点が異なります。実際、テンソルとNumPyの配列はしばしば同じメモリを共有することができ、データをコピーする必要がありません（`bridge-to-np-label`を参照）。\n",
        "テンソルはまた、自動微分のために最適化されています（これについては後ほどAutogradのセクションで詳しく説明します）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensorの初期化\n",
        "\n",
        "\n",
        "テンソルは様々な方法で初期化することができる。以下の例を見てみよう。\n",
        "\n",
        "**データから直接作成する**\n",
        "\n",
        "テンソルはデータから直接作成することができる。データ型は自動的に推論される。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**NumPy array から生成**\n",
        "\n",
        "テンソルはNumPyの配列から作成することができます（逆も同様です - `bridge-to-np-label` を参照してください）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**another tensorから生成**\n",
        "\n",
        "新しいテンソルは、明示的に上書きされない限り、引数のテンソルの特性(形状、データ型)を保持する。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.5684, 0.3889],\n",
            "        [0.2315, 0.3813]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ランダムまたは定数値:**\n",
        "\n",
        "``shape`` はテンソルの次元を表すタプルである。以下の関数では、これは出力されるテンソルの次元を決定する。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.8371, 0.0164, 0.8186],\n",
            "        [0.6017, 0.4221, 0.9180]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## テンソルの属性\n",
        "\n",
        "テンソルの属性は、その形状、データ型、保存されるデバイスを記述する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## テンソルの演算\n",
        "\n",
        "算術、線形代数、行列操作（転置、インデックス、スライス）、サンプリングなど、100以上のテンソル演算が可能。\n",
        "インデックス付け、スライス）、サンプリングなど、100以上のテンソル演算が\n",
        "包括的に記述されている[こちら](https://pytorch.org/docs/stable/torch.html)。\n",
        "\n",
        "これらの各操作はGPU上で実行することができます（通常、CPU上よりも高速に）。\n",
        "これらの操作はそれぞれGPUで実行できます(一般にCPUより高速)。Colabを使っている場合、Runtime > Change runtime type > GPUでGPUを割り当てます。\n",
        "\n",
        "デフォルトでは、テンソルは CPU 上で作成されます。このため、テンソルを明示的にGPUに移動させる必要がある。\n",
        "``.to`` メソッドを用いて明示的に GPU に移動する必要があります。大きなテンソルをデバイス間でコピーすることはデバイス間で大きなテンソルをコピーすると、時間とメモリの面で高くつくことに留意してください!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to(\"cuda\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "リストの中からいくつかの操作を試してみてください。\n",
        "NumPy APIに慣れている人なら、Tensor APIは簡単に使えます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Standard numpy-like indexing and slicing:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Joining tensors**\n",
        "\n",
        "``torch.cat`` を使うと、一連のテンソルを与えられた次元に沿って連結することができます。\n",
        "[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)も参照してください。\n",
        "また、``torch.cat`` とは微妙に異なるテンソル結合オペもあります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4884, 0.1629, 0.9538, 0.1861, 0.4884, 0.1629, 0.9538, 0.1861, 0.4884,\n",
            "         0.1629, 0.9538, 0.1861],\n",
            "        [0.0471, 0.8741, 0.9031, 0.0081, 0.0471, 0.8741, 0.9031, 0.0081, 0.0471,\n",
            "         0.8741, 0.9031, 0.0081],\n",
            "        [0.7120, 0.4729, 0.4292, 0.8756, 0.7120, 0.4729, 0.4292, 0.8756, 0.7120,\n",
            "         0.4729, 0.4292, 0.8756]])\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Arithmetic operations**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2.3856e-01, 2.6530e-02, 9.0974e-01, 3.4650e-02],\n",
              "        [2.2164e-03, 7.6412e-01, 8.1562e-01, 6.4827e-05],\n",
              "        [5.0692e-01, 2.2368e-01, 1.8423e-01, 7.6660e-01]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2つのテンソル間の行列の掛け算を計算する。y1, y2, y3 は同じ値になる。\n",
        "# ``tensor.T`` はテンソルの転置を返す。\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# これは要素ごとの積を計算します。z1, z2, z3 は同じ値になる。z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**単元テンソル**\n",
        "単元テンソルがある場合、例えば全ての値を1つに集約することで\n",
        "テンソルのすべての値を 1 つの値に集約することで、1 要素のテンソルを得た場合、それを Python の数値に変換することができます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "66.11334991455078 <class 'float'>\n"
          ]
        }
      ],
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**インプレース演算**\n",
        "\n",
        "結果をオペランドに格納する演算をインプレース演算と呼びます。これらは接尾辞 ``_`` で表現されます。\n",
        "例えば、 ``x.copy_(y)`` や ``x.t_()`` は ``x`` を変更することになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4884, 0.1629, 0.9538, 0.1861],\n",
            "        [0.0471, 0.8741, 0.9031, 0.0081],\n",
            "        [0.7120, 0.4729, 0.4292, 0.8756]]) \n",
            "\n",
            "tensor([[5.4884, 5.1629, 5.9538, 5.1861],\n",
            "        [5.0471, 5.8741, 5.9031, 5.0081],\n",
            "        [5.7120, 5.4729, 5.4292, 5.8756]])\n"
          ]
        }
      ],
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "インプレース演算はメモリを節約できますが、微分計算をするときに履歴がすぐに失われるため、問題が生じる可能性があります。\n",
        "そのため、使用はお勧めしません。\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NumPyとのブリッジ\n",
        "CPU上のテンソルとNumPyのアレイは、メモリを共有することができます。\n",
        "一方を変更するともう一方も変更されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensor to NumPy array\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "テンソルの変化は、NumPyの配列に反映される。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NumPy array to Tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NumPy配列の変更はテンソルに反映される。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Datasets & DataLoaders\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "データサンプルを処理するコードは煩雑になりやすく、メンテナンスも大変です。\n",
        "読みやすさとモジュール化のために、モデル学習コードから切り離されることが理想です。\n",
        "PyTorchは2つのデータプリミティブを提供しています。PyTorch には ``torch.utils.data.DataLoader`` と ``torch.utils.data.Dataset`` という2つのデータプリミティブがあり、あらかじめロードされたデータセットを利用することができます。\n",
        "の2つのデータプリミティブがあり、あらかじめロードされたデータセットや独自のデータを使用することができる。\n",
        "Dataset`` にはサンプルとそれに対応するラベルが格納され、 ``DataLoader`` はイテラブルにラップしている。\n",
        "データローダーは、サンプルに簡単にアクセスできるように ``Dataset`` を反復処理できるようにラップします。\n",
        "\n",
        "PyTorchのドメインライブラリには、あらかじめロードされたデータセット（FashionMNISTなど）が用意されています。\n",
        "をサブクラスとし、特定のデータに特化した関数を実装しています。\n",
        "これらはあなたのモデルのプロトタイプやベンチマークに使用することができます。これらのデータは\n",
        "ここで見つけることができます。[Image Datasets](https://pytorch.org/vision/stable/datasets.html),\n",
        "[テキストデータセット](https://pytorch.org/text/stable/datasets.html)、そして\n",
        "[オーディオデータセット](https://pytorch.org/audio/stable/datasets.html)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データセットの読み込み\n",
        "\n",
        "以下は、TorchVisionの[Fashion-MNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist/)データセットを読み込む例です。\n",
        "Fashion-MNISTは、Zalandoの記事画像のデータセットで、60,000の学習例と10,000のテスト例から構成されています。\n",
        "各例は、28×28のグレースケール画像と、10クラスのうちの1つのラベルから構成される。\n",
        "\n",
        "[FashionMNIST Dataset](https://pytorch.org/vision/stable/datasets.html#fashion-mnist)を以下のパラメータでロードする。\n",
        " - ``root`` は学習/テストデータが格納されているパスである。\n",
        " - ``root`` はトレーニング/テストデータが格納されているパス、 ``train`` はトレーニングデータセットまたはテストデータセットを指定します。\n",
        " - ルートディレクトリにデータがない場合は ``download=True`` が指定され、インターネットからデータをダウンロードする。\n",
        " - ``transform`` と ``target_transform`` は、特徴量とラベルの変換を指定する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5098cb7979b7468c8ab3f1cf9cd727c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/26421880 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17215747df754387adaf1c93bdd8fc7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/29515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d2efb29808e473ca1f6788d25dcc6d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4422102 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7bf3cedf7d245588332a63a4e40ff3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データセットの反復処理と可視化\n",
        "\n",
        "データセット ``Datasets`` をリストのように手動でインデックス化することができます．training_data[index]`` です。\n",
        "ここでは、 ``matplotlib`` を使用して、トレーニングデータのサンプルを可視化します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKSCAYAAABMVtaZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAByEElEQVR4nO3deXhV5bX48RUSMs8hAxBIICCzyowFBAGNgOBVQaAOgFaoxdlbq7Zetd5qqXO1gFqriKkIlkllMAo4IAiigMwhzFMgkImEJCTZvz/4kWvIu144xwAJ+/t5Hp/HrH3WOTvn7GGxc9baPo7jOAIAAICLXr0LvQIAAAA4Pyj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/ABclHx8fOSee+454+Peffdd8fHxkZ07d577lQJqsTFjxkhoaOgZH9e3b1/p27fvuV8hnBMUfgofH5+z+m/p0qUXelUB1/npp59k2LBhkpSUJIGBgdK4cWO5+uqr5bXXXjvnr/3ss8/KnDlzzvnrAGdj0qRJ4uPjI927d7/Qq+K1MWPGVDmv+vn5SZMmTWTkyJGycePGc/raRUVF8tRTT7nqXO53oVegtpo2bVqVn9977z1JT0+vFm/Tps35XC3A9b799lu56qqrpGnTpnLXXXdJQkKC7NmzR1asWCGvvvqq3HvvvR4932233SYjR46UgICAs3r8s88+K8OGDZP/+q//8mLtgZqVlpYmycnJsnLlStm2bZu0aNHiQq+SVwICAuSf//yniIiUlZVJZmamTJkyRRYuXCgbN26URo0anZPXLSoqkqefflpExDVXMSn8FLfeemuVn1esWCHp6enV4qcrKiqS4ODgc7lq50RhYaGEhIRc6NUAzugvf/mLREREyKpVqyQyMrLKskOHDnn8fL6+vuLr62t9jOM4UlxcLEFBQR4/P3Cu7NixQ7799luZNWuWjB8/XtLS0uTJJ5+80KvlFT8/v2rn1x49esh1110nn376qdx1110XaM0uPvyp9xfo27evtG/fXlavXi1XXnmlBAcHy+OPPy4iJ09Ad955p8THx0tgYKBcdtllMnXq1Cr5S5cuNf65eOfOneLj4yPvvvtuZezgwYMyduxYSUxMlICAAGnYsKFcf/311b6XtGDBAundu7eEhIRIWFiYDB48WDZs2FDlMae+x5GZmSmDBg2SsLAwueWWW2rsfQHOpczMTGnXrl21ok9EJC4urlpszpw50r59ewkICJB27drJwoULqyw3fccvOTlZrrvuOlm0aJF06dJFgoKC5I033hAfHx8pLCyUqVOnVv5ZasyYMTX8GwJnJy0tTaKiomTw4MEybNgwSUtLq/aYU+eTF154Qd58801JSUmRgIAA6dq1q6xateqMr7FmzRqJjY2Vvn37yrFjx9THlZSUyJNPPiktWrSQgIAAadKkiTzyyCNSUlLi9e+XkJAgIieLwp/bvn27DB8+XKKjoyU4OFh69Oghn376abX8M52Hd+7cKbGxsSIi8vTTT1fu00899ZTX61wXcMXvFzpy5IgMHDhQRo4cKbfeeqvEx8fL8ePHpW/fvrJt2za55557pFmzZjJz5kwZM2aM5Obmyv333+/x69x0002yYcMGuffeeyU5OVkOHTok6enpsnv3bklOThaRk3+eHj16tKSmpsrEiROlqKhIJk+eLL169ZIff/yx8nEiJy+lp6amSq9eveSFF16ok1cp4U5JSUmyfPlyWb9+vbRv39762G+++UZmzZolv/vd7yQsLEz+/ve/y0033SS7d++WmJgYa+6WLVtk1KhRMn78eLnrrrukVatWMm3aNPnNb34j3bp1k3HjxomISEpKSo39boAn0tLS5MYbbxR/f38ZNWqUTJ48WVatWiVdu3at9th///vfUlBQIOPHjxcfHx/529/+JjfeeKNs375d6tevb3z+VatWSWpqqnTp0kXmzp2rXvGuqKiQoUOHyjfffCPjxo2TNm3ayE8//SQvv/yybN269ay/E5udnS0iIuXl5bJ9+3b5wx/+IDExMXLddddVPiYrK0t+9atfSVFRkdx3330SExMjU6dOlaFDh8pHH30kN9xwg4jIWZ2HY2NjZfLkyXL33XfLDTfcIDfeeKOIiFx66aVntb51loOzMmHCBOf0t6tPnz6OiDhTpkypEn/llVccEXHef//9ylhpaalzxRVXOKGhoU5+fr7jOI6zZMkSR0ScJUuWVMnfsWOHIyLOO++84ziO4+Tk5Dgi4jz//PPq+hUUFDiRkZHOXXfdVSV+8OBBJyIiokp89OjRjog4jz766Fn//kBt8dlnnzm+vr6Or6+vc8UVVziPPPKIs2jRIqe0tLTK40TE8ff3d7Zt21YZW7t2rSMizmuvvVYZe+eddxwRcXbs2FEZS0pKckTEWbhwYbXXDwkJcUaPHl3jvxfgie+//94RESc9Pd1xHMepqKhwEhMTnfvvv7/K406dT2JiYpyjR49WxufOneuIiPPxxx9XxkaPHu2EhIQ4juM433zzjRMeHu4MHjzYKS4urvKcffr0cfr06VP587Rp05x69eo5X3/9dZXHTZkyxRERZ9myZdbf5dQ56fT/Gjdu7KxevbrKYx944AFHRKq8VkFBgdOsWTMnOTnZKS8vdxzn7M/Dhw8fdkTEefLJJ63reDHhT72/UEBAgIwdO7ZKbP78+ZKQkCCjRo2qjNWvX1/uu+8+OXbsmHz55ZcevUZQUJD4+/vL0qVLJScnx/iY9PR0yc3NlVGjRkl2dnblf76+vtK9e3dZsmRJtZy7777bo/UAaoOrr75ali9fLkOHDpW1a9fK3/72N0lNTZXGjRvLvHnzqjx2wIABVa7IXXrppRIeHi7bt28/4+s0a9ZMUlNTa3z9gZqQlpYm8fHxctVVV4nIyUkUI0aMkOnTp0t5eXm1x48YMUKioqIqf+7du7eIiHFfWLJkiaSmpkr//v1l1qxZZ2x8mjlzprRp00Zat25d5fzTr1+/yuc7k8DAQElPT5f09HRZtGiRvPHGGxIaGiqDBg2SrVu3Vj5u/vz50q1bN+nVq1dlLDQ0VMaNGyc7d+6s7AKu6fPwxYQ/9f5CjRs3Fn9//yqxXbt2ScuWLaVevap19akO4F27dnn0GgEBATJx4kR5+OGHJT4+vvILr7fffnvldyAyMjJERCp3tNOFh4dX+dnPz08SExM9Wg+gtujatavMmjVLSktLZe3atTJ79mx5+eWXZdiwYbJmzRpp27atiIg0bdq0Wm5UVJT6D6ifa9asWY2vN1ATysvLZfr06XLVVVfJjh07KuPdu3eXF198Ub744gu55pprquScvi+cKgJP3xeKi4tl8ODB0rlzZ5kxY0a179eZZGRkyKZNmyq/L3e6s2m68vX1lQEDBlSJDRo0SFq2bCmPPfaY/Oc//xGRk+dP0+ian59f27dvX+Pn4YsJhd8v9Eu6/Hx8fIxx07/WHnjgARkyZIjMmTNHFi1aJE888YQ899xzsnjxYunYsaNUVFSIyMnv+Z0qBn/u9J03ICCg2g4B1DX+/v7StWtX6dq1q1xyySUyduxYmTlzZmVno9at6zjOGZ+bDl7UVosXL5YDBw7I9OnTZfr06dWWp6WlVSv8znZfCAgIkEGDBsncuXNl4cKFVb5fp6moqJAOHTrISy+9ZFzepEmTMz6HSWJiorRq1Uq++uorr/JhRuF3DiQlJcm6deukoqKiSnG1efPmyuUi//cvrtzc3Cr52r9EUlJS5OGHH5aHH35YMjIy5PLLL5cXX3xR3n///co/Z8XFxVX7VxPgBl26dBERkQMHDpzT19H+wQacL2lpaRIXFyf/+Mc/qi2bNWuWzJ49W6ZMmeLVP158fHwkLS1Nrr/+ehk+fLgsWLDgjPPtUlJSZO3atdK/f/8a3z/KysqqdBMnJSXJli1bqj3u9PPr2Z6H3bg/c8nnHBg0aJAcPHhQPvzww8pYWVmZvPbaaxIaGip9+vQRkZMbnq+vb7V/zUyaNKnKz0VFRVJcXFwllpKSImFhYZWt8qmpqRIeHi7PPvusnDhxoto6HT58uEZ+N+BCW7JkifGK3fz580VEpFWrVuf09UNCQqr9Yw04X44fPy6zZs2S6667ToYNG1btv3vuuUcKCgqqfd/VE/7+/jJr1izp2rWrDBkyRFauXGl9/M033yz79u2Tt956y7i+hYWFXq3H1q1bZcuWLXLZZZdVxgYNGiQrV66U5cuXV8YKCwvlzTfflOTk5MqveZztefjURAs37dNc8TsHxo0bJ2+88YaMGTNGVq9eLcnJyfLRRx/JsmXL5JVXXpGwsDAREYmIiJDhw4fLa6+9Jj4+PpKSkiKffPJJte9DbN26Vfr37y8333yztG3bVvz8/GT27NmSlZUlI0eOFJGT3+GbPHmy3HbbbdKpUycZOXKkxMbGyu7du+XTTz+Vnj17yuuvv37e3wugpt17771SVFQkN9xwg7Ru3VpKS0vl22+/lQ8//FCSk5OrNVvVtM6dO8vnn38uL730kjRq1EiaNWtWp2+Xhbpl3rx5UlBQIEOHDjUu79Gjh8TGxkpaWpqMGDHC69cJCgqSTz75RPr16ycDBw6UL7/8Uh2fdNttt8mMGTPkt7/9rSxZskR69uwp5eXlsnnzZpkxY0blPEybsrIyef/990Xk5J+Od+7cKVOmTJGKiooqQ6kfffRR+eCDD2TgwIFy3333SXR0tEydOlV27Ngh//nPfyqv7p3teTgoKEjatm0rH374oVxyySUSHR0t7du3P+OoqDrtAncV1xnaOJd27doZH5+VleWMHTvWadCggePv7+906NChcjzLzx0+fNi56aabnODgYCcqKsoZP368s379+irjXLKzs50JEyY4rVu3dkJCQpyIiAine/fuzowZM6o935IlS5zU1FQnIiLCCQwMdFJSUpwxY8Y433//feVjft6yD9Q1CxYscO644w6ndevWTmhoqOPv7++0aNHCuffee52srKzKx4mIM2HChGr5SUlJVcaxaONcBg8ebHz9zZs3O1deeaUTFBTkiAijXXBeDRkyxAkMDHQKCwvVx4wZM8apX7++k52dXTnOxTQOTE4bY2I6N2RnZztt27Z1EhISnIyMDMdxqo9zcZyTo1ImTpzotGvXzgkICHCioqKczp07O08//bSTl5dn/Z1M41zCw8Od/v37O59//nm1x2dmZjrDhg1zIiMjncDAQKdbt27OJ598Uu1xZ3se/vbbb53OnTs7/v7+rhjt4uM4Z/EtZwAAANR5fMcPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJc76zh1uvJ/d6Z544gljPCcnR835+T0Gf65+/fpqzvbt243xhIQENSc/P98Yt92qrWnTpsb44sWL1Zzs7Gx1WV1UG8dYsq/hYsS+BpwfZ9rXuOIHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5x1s0dbuHv768ue/DBB43xgoICNSc0NNQYr1dPr7kzMzON8csuu0zNOXr0qDH+9ddfqzkhISHG+O7du9Wci625AzUjLi5OXRYVFWWMe/PFett+ozVMlZeXe/x8ttfxpkkhLy/PGD948KCaU1JSUmOvX9O0z642rBsunEsuuURdpp0/165dq+Zo21lubq6ac+TIEWP8xIkTas7x48eNcdv2rL2OraFSOw6cb1zxAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl2Ccy2mGDBmiLtuwYYMx/tNPP6k5YWFhxnhkZKSao41ZmTZtmpqj3cc3IyNDzdFayxnJcGF5MypDG6fSpk0bNUcbc6KNX7GtW3FxsZqjjV6wjVnx9fU1xisqKtQcbVyDbTSLxvZea++Bn59+ONU+H9v9tzW219HeH+04JCKyevVqY5zRTe6m7YMi+r47cOBANee3v/2tMW4bh2bbbi8m27dvV5dpI+Zeeuklr1+PK34AAAAuQeEHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5BV+9p2rZtqy7bu3evMX755ZerOdpN2LUbyouINGrUyBgvLCxUc7TOyauuukrNycnJMcazsrLUnO+++05dhprhTVe1dnN0b7ZnrWtVRO/A025yLqJ3AJaUlKg53nTiarSuOBGRsrIyY9zWPax11do+N+09sHXoautWWlqq5mjrnZSUpOZs27bNGKerF56ynaM02nQJEf28FhwcrOZkZmYa47bJA9okA9uEA236ho12HLAdo8LDw41x2zSRM+GKHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuATjXE7TpUsXdZnWim0be+DNTab3799vjEdGRqo5QUFBxritvV5bZnsdnHvaOBVvxrwcOHBAXXbkyBFj3DYqQRs7YBt/oo16sY1s0Z7PlqO9b7bxJ9rIFBtvcrTxTbbROdpYp6NHj6o52rrZRmZ4s13h4ufNdhEQEOBxTkREhLrM19fXGLcdo95++21j/LLLLlNzunbtaozn5+erOdp4GNuYF9txUqOtw7p16zx+rlO44gcAAOASFH4AAAAuQeEHAADgEhR+AAAALkHhBwAA4BJ09Z5Guym0iEhoaKjHOVqXna3zp3Xr1sZ4YGCgmrN27VqPXl/EuxtGo3Y6fPiwMW67yXiTJk2M8ZKSEjVH69C1dY/7+ZkPM7Zt05uuXm17ttHWzdZ9py2z5WjvaXx8vJoTExNjjNu6LbVO7ZycHDXHdlzRaJ+DN58Baidvunqjo6M9zikqKlKXafuUratXW++UlBQ1x5upGNo+besE1s6t2nFIRD+GHzp0SM05E674AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4BIUfAACASzDO5TTajZdF9PETtnZ0bbzBnj171JwFCxYY488++6yas3//fmNcuzm8iD7iwTb6AeeeNo7Ax8dHzbn88suNcdt4hb179xrjtrEHmZmZxrhtBJB243bbdqZtt7b3wDbqxVO25/LmRuva+2M7dowYMcIYf+ONN9QcbQSMbXyUr6+vMb5x40Y1Bxc/b8a5aNufjW1/so1t0SQkJBjjGRkZas769euN8SuvvFLNiYuLM8ZtY6qOHTtmjNuOa9q5/Zfgih8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuASFHwAAgEvQ1XuaHTt2qMu0zknbjZw1YWFh6rI5c+YY4xMnTlRztBs5Z2dnqzlaJ9GJEyfUHFw4WreaiEiLFi2McVtnptZ9pnXhiohcdtllxviSJUvUHK2jVesmFdG3TW86d7XOetvreNPRaPt9tPfUdhzo2rWrMb5w4UI1R/u8bV2DoaGh6jKN7T2Fe9m6+zW2401gYKAxnpubq+bceOONxnhpaamaU5PdyLYuZa2LXzt/i4isWbPGo/U6G1zxAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl2Ccy2m0m9CLiHTr1s0Yt40/0Vq7bTdN37lzpzFuGzGh3dS+uLhYzdFGYxw+fFjNwYXTvXt3ddnBgweN8aysLDWnZcuWxrhtZMoDDzxgjC9evFjN0bZb283MtfEjtpEp3tDWwbavactsYxy03ycnJ0fN8fMzH57j4+PVHG2ci+19Y3wTakpkZKS6TNtvCgoK1BxtPIxt/ywpKTHGtdEwtnXwZmxRfn6+ukx7Ptu6LV++3ON1OBOu+AEAALgEhR8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuARdvafZvn27uky7obqt80db5k13ou1m1raOQo3Wabh3716Pnwvnnq3za/Pmzca4rUM3PDzcGA8NDVVzfvjhB2Pc1qHrzbbpzf6hbc/evI433Xy2TkOti3/Xrl1qjvae2rpw27RpY4wvWrRIzenSpYsxbvsMvHl/cPELCgpSl2nbjDaRwsa2r2mdwEVFRWpOYWGhMW47fmrrbTtOa/uubX9aunSpusxbXPEDAABwCQo/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXYJzLafbs2aMu09q0bS3sWtv5sWPHPFsxETly5Ii6zJtRFlqruu09wLmnfZa27cy2TNO8eXNjfOfOnWrOmjVrjHHbSIbzNfrDNuJBo73XtlEmWo5tn9b2NduoG2/2w+DgYGM8OjpazdHWu0GDBmpOVlaWZysGV4iLi1OX+fmZyw3bPnD48GFj3JvxUdrri+ij0rw5DthytONAaWmpmmMb+eQtrvgBAAC4BIUfAACAS1D4AQAAuASFHwAAgEtQ+AEAALgEXb2nOXTokLpM6yTSOoJE9A6jkpISz1ZMRHJyctRl2rrZbhhdUFBgjNu6h3Hh2LrFtM5ZW7dYUlKSMb5w4UI1Z//+/ca4dmN0EZH8/Hxj3Jubs58v3nTJ27oGbZ+DRvu8bZ2T8+fPN8YvvfRSNWfdunXGuO0zBUwSEhLUZdo+YOvG1/YB2/6p7Ye27VnL8WZaQl5enrpM+121c7GIyN69ez1ehzPhih8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuASFHwAAgEtQ+AEAALgE41w8cPToUWPcNjKlsLDQGLfdZFpz/PhxdZn2fNqYDxGR3Nxcj9cB5552I2/bZ6mxjT2IiIgwxi+//HI1Rxv9YaONO/JmxMn5Yhsxob2ntuOANr7JNgImIyPDGG/evLmas23bNmP86quvVnN27NhhjNvGB+HioW2DtnNUjx49jHHbqKHi4mJjPCoqSs3R9inbtqmdJ20j1LzZ1rXXKSoqUnO0Y0fDhg3VHG9qhTPhih8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuASFHwAAgEvQ1euBjRs3GuMtWrRQc7SOHFuHrsbWaah1fGodlSJ6Nx8uLO2zPHHihMfPVb9+fXWZ1mWndRWLiOTn5xvjtpuZa53ttm2zoqJCXeYp2+9Tk69v677zpnNS+7ybNGmi5midi7YO/sTERGN806ZNag4uHt5MC3j44YeNcX9/fzVH2wZt3f2RkZHG+Pr169WcVq1aGeO2401BQYExrh3vRPR913acDg8PN8a1iSHnClf8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJRjn4gHbSASN1kJ+7Ngxj5/LNpZCG/ViuzH17t27PV4H1E7aTcZtoxq00QL79u1Tc7KysozxNm3aqDnaPuDNmBVv2EbaaPuNbZyLNsbBdqN37WbzNjk5Oca4Nn5FRCQkJMQY37Jli5oTFRVljNuOHbh42MaEaQYPHmyM2/YbbVnjxo3VnDVr1hjjy5cvV3Oys7ONcdv+2aFDB2PcdvzUltlytLFO5/tczBU/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoKvXA9rNl318fNQcraPQdnN2TUREhLrs+PHjxrjthtFHjhzxeB1w4di6bYODg41x2+evdei2aNFCzdm6dasxbutS1zrZbB2A3nT8as/nTdeiN2xdg8XFxca4reP4u+++M8bHjx+v5oSGhhrj2vFBRO+69vf3V3O86SLHuaedi7zZBx555BF1WUFBgTFeWlqq5mhTBGzmzZtnjHfu3FnNue6664zxzMxMNUc7RsXHx6s53nQPa8u0Dv5zhSt+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgEoxz8YDWim1r39YcPHjQ45y8vDx1mdaubxs1k5ub6/E64MIJDAys0ZyjR48a41FRUWqON2OItBxthIKIfdSLxpsRMOeLtn/a3uuvvvrKGNdG6oiItGrVyhjPyMhQc5o0aWKM20bNXCxjW7Rtxjb+RDumerM9ezNmxfbee/N8v/nNb4zxP/7xj2qONh7Itg8ePnzYGLeNWWndurUxro1sEdFHJzVo0EDNKSwsNMb379+v5mjjjmzbgUYbw3Su1N4jJQAAAGoUhR8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuARdvR5ITEz0OCc4ONgYX716tcfPtW/fPnVZZGSkMa51OIl41zmJCyciIkJdpt0w3NY1qnVzvvvuu2qOdhN2W2e71h1q60DUns+brkVveLNv2G5Qrx0HbF332u+6fPlyNadr167G+A8//KDmaMLDwz3OqWu0z9nWnarl2D7/86Vhw4bG+KRJk9Sc5s2bG+P//Oc/1Zxu3boZ4126dFFz8vPzjfFDhw6pOVon8OzZs9Wc9u3bG+O2rl5tP7Qd17TPOycnR81p1KiRMa51CJ8rXPEDAABwCQo/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXYJyLB7RW7MDAQDVHW6aN37DRbiQtoo/6sI2lCAsL83gdcO5prf3ayB4Rfduw3TBcuzH4hg0b1BxtHbSbtovYbyqvsY3TqMnn0kamePP6ttEP2n5o+3yio6ON8Tlz5qg5I0aMMMa1Y5eIyJ49e4xx2/igi50343zi4+PVZdq+FhQUpOa0adPGGL/xxhvVnP79+xvju3fvVnO0UT/amBcRkTVr1hjjtuNAp06djPEePXqoOVdccYUxvnLlSjVH+10TEhLUnICAAGN8//79ao7Gtk9r25U39cAvwRU/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoKvXA9qNnG0dYFpXr63zRxMSEqIu07oQtW4yEZH69et7vA4497Sbf2dlZak52nZm6wTWbuhuu8l4bGysMW7r5tO2ddt+o+1rNjWZY+vQ1fY1Wyew7fk02v5uu6m91h14ySWXqDlaF6TtM9V48xnURkOHDlWX/fnPfzbGbV29xcXFxrjtPKB1mu7YsUPNyczMNMajoqLUnNatWxvjBw8e9HjdVq1apeb8+OOPHj2XiP6edu3aVc3Rfp9jx46pOVp3v3aMFBE5ceKEMa4dv0X07UDrrD9XuOIHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuwTgXD+Tn5xvjtpEZttEYnrKNStBGs9huAu7NSBlcONrIFhGR4OBgY7xDhw5qTklJiTFeVlam5mhjSWp6lIk3r1OTtFENNt78nrYRE9ooi9zcXDVHGwth+0xrcuSUNhajrpk7d666bOXKlcb4p59+quZox+ewsDA1R/tcbNuZNkpk+/btao42tkU7Pthceuml6rKWLVsa47ZtJiYmxhj39/dXc7SRKd5sm7aRRuXl5ca4bZyLtk8tXbrUo/X6pbjiBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgErR1ekDrfkpJSVFztBut79u3z+PX37lzp7qsS5cuxritk0m7oTtqJ9tnqXXtDRkyRM2xbU8arTvR1nGudZTacjQVFRXqsvPV8aup6Y5Wb34frYv7+++/V3O0Y5St47h9+/bG+IX+DDyldchu3bpVzWnVqpUxHhUVpeYUFRUZ41oXtojIgQMHjHGtm1RE7xrV9lsRkdatWxvjycnJak6jRo2McduEC23/sHUPa1MxbDnevI6WY+se9mZ/16ZsFBQUePxcv0Td2ksBAADgNQo/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXYJyLB2w3bNZoI1O8uQG2rfW/R48exnhAQICak5+f7/E64NzTRrDcdtttas7jjz9ujNtuzq6NeLBtMydOnFCXabSRCLabmdtGVtQk7XVsoxq0ZbbxNN6Mi9BGgGjjV2zP17x5czVn3bp1xviIESPUnPXr1xvjP/30k5pTG91www3GeGFhoZqjjeIKDw9Xc5KSkoxxbfyOiHfjjmryuWz7oHb+2rVrl5qTl5fn8eto42m0MTwi+ucQFham5mjroL2+iEhwcLAxbtt2IiIijHHbuB3NL9k+uOIHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASdPV6IDc31xi3ddlpnbO2m81rbF1JgYGBxritc5Ku3tqpY8eOxnhMTIyao3XbHj16VM154IEHjPHJkyerOd50v3mzrWtde/Xq1ey/VbXn86Z72dadqE0ECA0NVXO0dbO9n1oXt9aBKCJy8OBBY3z//v1qzoIFC9RldcmgQYOMcW0fFBHJyMgwxm3HU226g60zU9sHbB2g2vPZXsebfUrb1m3bZnR0tDEeGxur5mgTBmz757Fjx4xxWyew1nVv+320dbCtm/Ze287T5wJX/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUY5+IB7ebctnEu2s2sy8rKPH59rU1dRG9HLygoUHO8WQece3PmzDHGte1PRMTf398Yb9GihZqjjR2w3WxeG29gGzFRXFxsjNtGTNhGo2hqcjSLtj/Zns82+kEbd2N7He0zte3T2rrZjh1NmjQxxtPT09Wci8Xvf/97Y7xr165qTmRkpDHetGlTNUfb1r0ZpWI7bmvb4PkaG2OjbYO2MThvvPGGMd62bVs158YbbzTGbe+b9h7Y9mnts7ONXdOOA9o4NhvbCK0z4YofAACAS1D4AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBL0NXrAa0Dz3aDZe0m01pnmIhIbm6uMe5NF8+RI0c8zsGFtW7dOo/iNo888oi67KGHHjLG+/fvr+bk5eUZ47bOWa1jzpuORtuN1r15PlvXnkY7Dti6IL1Zt+DgYGM8KipKzfnoo4+M8Q0bNqg52nt6vm8cfyFox8cOHTqoOdrn3LdvXzVnyJAhxnjr1q3VnPbt2xvjcXFxao5t/9AcP37cGP/pp5/UnB9//NEYnzlzpprzzTffGONFRUWWtTOzTR7QJhls3LhRzdE+U1v3cmFhoTG+Y8cONWfTpk3G+IwZM9QczS+ZysEVPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcAnGuXggIyPDGLeNWQkNDTXGvblxfGJiorosPj7eGA8JCfH4dVD3aGMcDh065PFzaSMHRESKi4uNcdt25s0N3TXejKuw3QBdWzfbmBdtmTbqRkTk6NGjxrhtzEtJSYkxrt3s3rZMG0EjIlJeXq4uQ3XasTs9PV3NsS2D5/Lz89Vll1122Xlck3NPO0bZ9ukz4YofAACAS1D4AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBL0NXrgc2bNxvjthssax2F2g2ebZ555hl12eHDh41x7cbYuLjUZGfm+vXra+y5AADe+yXduxqu+AEAALgEhR8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuASFHwAAgEv4OOeiVxgAAAC1Dlf8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCD0CtNmbMGAkNDT3j4/r27St9+/Y99ysEoMa9++674uPjIzt37vQ4d8yYMZKcnFzj63SxovBT+Pj4nNV/S5cuvdCrCtQ6kyZNEh8fH+nevfuFXhWvjRkzpsq+7ufnJ02aNJGRI0fKxo0bz+lrFxUVyVNPPcXxBefUTz/9JMOGDZOkpCQJDAyUxo0by9VXXy2vvfbahV41nEN+F3oFaqtp06ZV+fm9996T9PT0avE2bdqcz9UC6oS0tDRJTk6WlStXyrZt26RFixYXepW8EhAQIP/85z9FRKSsrEwyMzNlypQpsnDhQtm4caM0atTonLxuUVGRPP300yIiXMXEOfHtt9/KVVddJU2bNpW77rpLEhISZM+ePbJixQp59dVX5d57773Qq4hzhMJPceutt1b5ecWKFZKenl4tfrqioiIJDg4+l6t2ThQWFkpISMiFXg1cBHbs2CHffvutzJo1S8aPHy9paWny5JNPXujV8oqfn1+1fb5Hjx5y3XXXyaeffip33XXXBVoz4Jf5y1/+IhEREbJq1SqJjIyssuzQoUMXZqVwXvCn3l+gb9++0r59e1m9erVceeWVEhwcLI8//riInNxx7rzzTomPj5fAwEC57LLLZOrUqVXyly5davxz8c6dO8XHx0fefffdytjBgwdl7NixkpiYKAEBAdKwYUO5/vrrq30fYsGCBdK7d28JCQmRsLAwGTx4sGzYsKHKY059ZyozM1MGDRokYWFhcsstt9TY+wJ3S0tLk6ioKBk8eLAMGzZM0tLSqj3m1Db+wgsvyJtvvikpKSkSEBAgXbt2lVWrVp3xNdasWSOxsbHSt29fOXbsmPq4kpISefLJJ6VFixYSEBAgTZo0kUceeURKSkq8/v0SEhJE5GRR+HPbt2+X4cOHS3R0tAQHB0uPHj3k008/rZZ/pmPDzp07JTY2VkREnn766co/NT/11FNerzNwuszMTGnXrl21ok9EJC4urvL/33nnHenXr5/ExcVJQECAtG3bViZPnlwtJzk5Wa677jr55ptvpFu3bhIYGCjNmzeX9957r9pjN2zYIP369ZOgoCBJTEyU//3f/5WKiopqj5s7d64MHjxYGjVqJAEBAZKSkiLPPPOMlJeX/7Jf3uW44vcLHTlyRAYOHCgjR46UW2+9VeLj4+X48ePSt29f2bZtm9xzzz3SrFkzmTlzpowZM0Zyc3Pl/vvv9/h1brrpJtmwYYPce++9kpycLIcOHZL09HTZvXt35Zdap02bJqNHj5bU1FSZOHGiFBUVyeTJk6VXr17y448/Vvnya1lZmaSmpkqvXr3khRdeqJNXKVE7paWlyY033ij+/v4yatQomTx5sqxatUq6du1a7bH//ve/paCgQMaPHy8+Pj7yt7/9TW688UbZvn271K9f3/j8q1atktTUVOnSpYvMnTtXgoKCjI+rqKiQoUOHyjfffCPjxo2TNm3ayE8//SQvv/yybN26VebMmXNWv092draIiJSXl8v27dvlD3/4g8TExMh1111X+ZisrCz51a9+JUVFRXLfffdJTEyMTJ06VYYOHSofffSR3HDDDSIiZ3VsiI2NlcmTJ8vdd98tN9xwg9x4440iInLppZee1foCZyMpKUmWL18u69evl/bt26uPmzx5srRr106GDh0qfn5+8vHHH8vvfvc7qaiokAkTJlR57LZt22TYsGFy5513yujRo+Vf//qXjBkzRjp37izt2rUTkZMXMa666iopKyuTRx99VEJCQuTNN9807sfvvvuuhIaGykMPPSShoaGyePFi+Z//+R/Jz8+X559/vmbfEDdxcFYmTJjgnP529enTxxERZ8qUKVXir7zyiiMizvvvv18ZKy0tda644gonNDTUyc/PdxzHcZYsWeKIiLNkyZIq+Tt27HBExHnnnXccx3GcnJwcR0Sc559/Xl2/goICJzIy0rnrrruqxA8ePOhERERUiY8ePdoREefRRx89698fOBvff/+9IyJOenq64ziOU1FR4SQmJjr3339/lced2sZjYmKco0ePVsbnzp3riIjz8ccfV8ZGjx7thISEOI7jON98840THh7uDB482CkuLq7ynH369HH69OlT+fO0adOcevXqOV9//XWVx02ZMsUREWfZsmXW3+XUfnL6f40bN3ZWr15d5bEPPPCAIyJVXqugoMBp1qyZk5yc7JSXlzuOc/bHhsOHDzsi4jz55JPWdQS89dlnnzm+vr6Or6+vc8UVVziPPPKIs2jRIqe0tLTK44qKiqrlpqamOs2bN68SS0pKckTE+eqrrypjhw4dcgICApyHH364MnZqX/nuu++qPC4iIsIREWfHjh3W1x4/frwTHBxcZf8fPXq0k5SUdNa/u9vxp95fKCAgQMaOHVslNn/+fElISJBRo0ZVxurXry/33XefHDt2TL788kuPXiMoKEj8/f1l6dKlkpOTY3xMenq65ObmyqhRoyQ7O7vyP19fX+nevbssWbKkWs7dd9/t0XoAZ5KWlibx8fFy1VVXicjJ7vgRI0bI9OnTjX+eGTFihERFRVX+3Lt3bxE5+WfT0y1ZskRSU1Olf//+MmvWLAkICLCuy8yZM6VNmzbSunXrKvtEv379Kp/vTAIDAyU9PV3S09Nl0aJF8sYbb0hoaKgMGjRItm7dWvm4+fPnS7du3aRXr16VsdDQUBk3bpzs3Lmzsgu4po8NgLeuvvpqWb58uQwdOlTWrl0rf/vb3yQ1NVUaN24s8+bNq3zcz6/E5eXlSXZ2tvTp00e2b98ueXl5VZ6zbdu2lfuwiEhsbKy0atWqyv48f/586dGjh3Tr1q3K40xfN/r5axcUFEh2drb07t1bioqKZPPmzb/sDXAx/tT7CzVu3Fj8/f2rxHbt2iUtW7aUevWq1tWnOoB37drl0WsEBATIxIkT5eGHH5b4+PjKL5fffvvtld83ysjIEBGpPKmdLjw8vMrPfn5+kpiY6NF6ADbl5eUyffp0ueqqq2THjh2V8e7du8uLL74oX3zxhVxzzTVVcpo2bVrl51NF4On/wCkuLpbBgwdL586dZcaMGdW+X2eSkZEhmzZtqvy+3OnO5gvsvr6+MmDAgCqxQYMGScuWLeWxxx6T//znPyJycp82ja75+T7fvn37Gj82AL9E165dZdasWVJaWipr166V2bNny8svvyzDhg2TNWvWSNu2bWXZsmXy5JNPyvLly6WoqKhKfl5enkRERFT+fPr+LHJyn/75/qztK61ataoW27Bhg/zpT3+SxYsXS35+frXXhnco/H4h7ftFZ8PHx8cYN10ZeeCBB2TIkCEyZ84cWbRokTzxxBPy3HPPyeLFi6Vjx46VX4ydNm1aZTH4c6efKAMCAqqdfIBfYvHixXLgwAGZPn26TJ8+vdrytLS0aoWfr6+v8bkcx6nyc0BAgAwaNEjmzp0rCxcurPL9Ok1FRYV06NBBXnrpJePyJk2anPE5TBITE6VVq1by1VdfeZUP1Db+/v7StWtX6dq1q1xyySUyduxYmTlzptx6663Sv39/ad26tbz00kvSpEkT8ff3l/nz58vLL79crSHjbPfns5Gbmyt9+vSR8PBw+fOf/ywpKSkSGBgoP/zwg/zhD38wNoPg7FD4nQNJSUmybt06qaioqFJcnbo0nZSUJCL/d3UjNze3Sr72r/6UlBR5+OGH5eGHH5aMjAy5/PLL5cUXX5T3339fUlJSRORkN9bpVyiA8yEtLU3i4uLkH//4R7Vls2bNktmzZ8uUKVO8+seSj4+PpKWlyfXXXy/Dhw+XBQsWnHG+XUpKiqxdu1b69++v/iPLW2VlZVW6iZOSkmTLli3VHnf6Pn+2x4aaXl/gbHXp0kVERA4cOCAff/yxlJSUyLx586pczTubr0lokpKSKv9C9XOn7z9Lly6VI0eOyKxZs+TKK6+sjP/8rwnwDpd8zoFBgwbJwYMH5cMPP6yMlZWVyWuvvSahoaHSp08fETm5A/j6+la7cjBp0qQqPxcVFUlxcXGVWEpKioSFhVWOpUhNTZXw8HB59tln5cSJE9XW6fDhwzXyuwEmx48fl1mzZsl1110nw4YNq/bfPffcIwUFBVW+O+Qpf39/mTVrlnTt2lWGDBkiK1eutD7+5ptvln379slbb71lXN/CwkKv1mPr1q2yZcsWueyyyypjgwYNkpUrV8ry5csrY4WFhfLmm29KcnKytG3btvJxZ3NsONVlf/o/CoGasmTJEuOVuPnz54vIyT+9nrqC9/PH5eXlyTvvvOP16w4aNEhWrFhRZf89fPhwtbFPptcuLS2tdn6E57jidw6MGzdO3njjDRkzZoysXr1akpOT5aOPPpJly5bJK6+8ImFhYSIiEhERIcOHD5fXXntNfHx8JCUlRT755JNq3z3aunWr9O/fX26++WZp27at+Pn5yezZsyUrK0tGjhwpIie/wzd58mS57bbbpFOnTjJy5EiJjY2V3bt3y6effio9e/aU119//by/F3CHefPmSUFBgQwdOtS4vEePHhIbGytpaWkyYsQIr18nKChIPvnkE+nXr58MHDhQvvzyS3UUxW233SYzZsyQ3/72t7JkyRLp2bOnlJeXy+bNm2XGjBmyaNGiyqsbmrKyMnn//fdF5OSfjnfu3ClTpkyRioqKKkOpH330Ufnggw9k4MCBct9990l0dLRMnTpVduzYIf/5z38qr+6d7bEhKChI2rZtKx9++KFccsklEh0dLe3bt7eO3QA8ce+990pRUZHccMMN0rp1ayktLZVvv/1WPvzwQ0lOTpaxY8dKVlaW+Pv7y5AhQ2T8+PFy7NgxeeuttyQuLk4OHDjg1es+8sgjMm3aNLn22mvl/vvvrxzncupq+Cm/+tWvJCoqSkaPHi333Xef+Pj4yLRp07z6szFOcyFbiusSbZxLu3btjI/Pyspyxo4d6zRo0MDx9/d3OnToUDme5ecOHz7s3HTTTU5wcLATFRXljB8/3lm/fn2VcS7Z2dnOhAkTnNatWzshISFORESE0717d2fGjBnVnm/JkiVOamqqExER4QQGBjopKSnOmDFjnO+//77yMT8fjwHUhCFDhjiBgYFOYWGh+pgxY8Y49evXd7KzsyvHuZhGFMlpY0xM22t2drbTtm1bJyEhwcnIyHAcp/o4F8c5OSpl4sSJTrt27ZyAgAAnKirK6dy5s/P00087eXl51t/JNM4lPDzc6d+/v/P5559Xe3xmZqYzbNgwJzIy0gkMDHS6devmfPLJJ9Ued7bHhm+//dbp3Lmz4+/vz2gX1LgFCxY4d9xxh9O6dWsnNDTU8ff3d1q0aOHce++9TlZWVuXj5s2b51x66aVOYGCgk5yc7EycONH517/+VW30SlJSkjN48OBqr2PaL9etW+f06dPHCQwMdBo3buw888wzzttvv13tOZctW+b06NHDCQoKcho1alQ5ckZOG4PGOBfP+DgO5TMAAIAb8B0/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAlzjrO3dw70gx3vpJRKRz585qTv369Y1x2w2mly1bZozbpvbHxcUZ4wUFBWqO9pn++te/VnO2bt2qLquLauMYS/Y1/T2wfV6NGzc2xm+++WY1R7vv56pVq9Sc9957zxhv2LChmjN37lxjPDQ0VM1p0aKFMX7vvfeqOX5+5kO67fZ0RUVFxviRI0fUHG+wrwHnx5n2Na74AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4xFl39UKkadOmxnhiYqKa4+vra4z7+/urOUlJScZ4aWmpmhMZGWmMr127Vs3JyMgwxtu0aaPmXGxdvaidvOnqbdCggTHerFkzNUfrru/evbua869//csYf+SRR9Qcbf+MiYnx+HWee+45NUfrBLa9b4sWLTLGH3vsMTUHQN3FFT8AAACXoPADAABwCQo/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJxrl4oHPnzsZ4QUGBmlNeXm6MBwUFqTn169c3xk+cOKHmaDdht42NadmypTGujZMBzhdtzIpNZmamMb58+XI1Z9euXcZ4//791ZwHH3zQGF+yZImao41VKisrU3MeeughY3zTpk1qzp/+9Cdj/MiRI2qObUwUgIsPV/wAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCrt7TXHbZZeoy7Ybq3nTF2bptvaGtg3ZzeBGRpk2bGuM//PBDTawS4DU/P/OhydYFO2TIEGP873//u5rz29/+1hiPjo5Wc7Tu/m+//VbNOXz4sDE+fvx4NUfbd7/77js1Z+zYscZ4QECAmvPYY4+pywBcfLjiBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgEhR+AAAALsE4l9M0atTI45zy8nJ1mTa2xTaWwnEcj57rTM+n0UbAJCYmevxcwIUWGhpqjE+dOlXN0fapLl26qDkrVqwwxtetW6fmhIWFGeO2/TY+Pt4YHz58uJrz5ZdfGuO9evVSc4qLi9VlGh8fH2NcO3YBqD244gcAAOASFH4AAAAuQeEHAADgEhR+AAAALkHhBwAA4BJ09Z7m6NGjNfp8WteedhN6EZGIiAhjvKioSM2pV89cw2vddyIix48fN8ZtN3QHzgdbp7xm27ZtxviVV16p5qSmpnr8Ojk5Oca4bV/T9k+tQ1hEJDk52RiPiYlRczp06GCM245rvXv3Nsa19xNA3cYVPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcAnGuZxm//79HuccPnzY45yUlBR1mTYuQrsJvYhIQUGBMW67Abs2FiIrK0vNAc4Hx3E8zvnyyy+NcdvIFm0//PHHH9UcbdzRpZdequYcO3bMGI+Pj1dztHEq0dHRak6XLl2M8Tlz5qg5c+fONcZt42m8+XwA1A5c8QMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcAm6ek9z8OBBdZl2o/MNGzaoObGxscZ4x44d1ZyHHnrIGJ84caKao3XZ7du3T80JCgoyxjdt2qTmAOeD1lFq6ybt1KmTMb527Vo159133zXGL7vsMo9fZ9myZWqO9vsMGzbM43XLzc1Vc/r372+Mr1u3Ts0pLS01xhMTE9WcPXv2qMsA1G5c8QMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcAkKPwAAAJdgnMtpTpw4oS7TRhhERESoOd7czNzf398Yr6ioUHPq1TPX8IWFhWrOkSNHjPH169db1g6oGdqIExF9v9H2DRF9dJK2b4iI+Pr6GuODBg1Sc9asWWOMa2NRRPTRKHPmzFFz7rvvPmO8pKREzXnkkUeMcdtxSHsPysrK1Jz69esb47bjJ2onb0YnXWxCQ0ON8cjISDVH26dt75u272pjmEREvvzyS2P8+++/V3POhCt+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQVevBzZv3myMJycnqzkHDhzw+HWOHz9ujBcXF6s5WkehLUfrMFqyZIll7YCa4U3XoK3TVNtv1q5dq+a0aNHC49f58ccfjfERI0aoObNmzTLGBw4cqOakpKQY42+//baaox2jvOmg1t5PEb27mq7e2smbz98mNTXVGO/atauao+0fts7Z3NxcY7xp06ZqjiYgIMCrZZ46evSoukybstGkSRM1Z/bs2cb4jTfe6NmK/QxX/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUY5+KBffv2GeNxcXFqjm0kgqaoqMgYLy8vV3O0G9HbblCv/T5aCz1woVVUVKjLWrdubYzbxiusWLHCGL/22mvVnEceecQY18YjiYgMGDDAGA8JCVFzvvnmG2NcW2cR/VjUrl07j3M+//xzNScsLMwY18ZV4MLyZpyLbUzZH//4R2N8zZo1as7BgweN8eDgYDUnMDDQGD9y5Iiao41isp0LtWW24432Oto6i4gUFBQY47bPxzaOyltc8QMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcAm6ej2wcuVKY7xTp05qjq+vrzFuuwm81vlj6xosLi42xm2dTAcOHFCX4eJQ0zdnv9BsN2fXund37dql5mj7wIwZM9Sc8ePHG+O291PrkLV19UZFRRnjWreviEivXr2M8T59+qg5WgdzVlaWmnPs2DFjfOfOnWqOm2n7oZ+ffgrWOkpt0x08fS6bsWPHqsv+85//GOPNmzdXc2JjY43xxMRENae0tNQY1853Ivp7bTsXaudpG21/t62bNuXDth3YlnmLK34AAAAuQeEHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASjHPxwJYtW4zxZs2aqTnaeIMTJ054/Pq2cRHazdFtrf/79+/3eB0AT2njFbwZJ2O7obs2bsk2YuTBBx80xq+55ho153//93+N8Z49e6o5/v7+xrhtRJO2f6alpak5jz/+uDH+9NNPqzlvv/22MZ6SkqLmaONc6pqa3DZttOfz5jxQ0/7xj38Y47/73e/UnJdfftkY/9WvfqXmaONUJk2apOZoY2hsY5C0c55tpI02ziUwMFDN+eqrr4zxp556Ss0JDw/3+HW0kTa/BFf8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgq5eD2RmZhrj9evXV3O0bqGysjI1R7uRs63TTOsOs918WuuCRN1Tk92J2nN5+3w1KSAgQF12ww03GOP79u1Tc1q0aGGMa124IiK33HKLMd66dWs1R1sWFham5mj7bmhoqJpz6623GuO7du1Sc7SbyhcVFak52dnZ6jK3ioqKUpdpn2WHDh3UnPbt2xvjCQkJao62nbVt21bNSU5ONsZtUx/GjBljjC9ZskTNuemmm9RlF5MHHnhAXdaoUSNjXDvni4gMHz7cGH/mmWc8Wq+f44ofAACAS1D4AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4BONcPFBQUGCM28YeaOMvbDkbNmzwbMVEvwF1fHy8mnP48GGPXwe1U02OWTlfN6i30cac2EZCaDdN37t3r5oze/ZsY/z1119Xc7TRHD/88IOa07BhQ2PcNtZJW/b444+rOTk5Ocb4sGHD1Jzg4GBjfM+ePWqOm48d2kihkJAQNUcbG3TVVVepOdu2bTPGr776ajWnY8eOxrhtDFJhYaExHh4erubs2LHDGPdmZItt5Jg2Wsq233j6XLZl2nnVRqsTRETKy8uNcW2kkoh95I+3uOIHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASdPXWgGPHjqnLtG4q203gtY45WyeT9jpax5aIyObNm9VlqFtsHWsab7ptz9frvPbaa8a4rWMuNzfX49cfPXq0Md6zZ081R+ve1bowRfTO4u7du6s569atM8YnTJig5sydO9cYnzlzpprTpEkTYzw6OlrN0boT3aCkpMQYt3WP+/mZT7Vr165Vc2655RZjvFmzZmpOZmamMd68eXM1JygoyBg/cuSImqOtg60TOD8/X12m0c55tuOQtr/bjgM1OcnA1oWbl5dnjJ84ccLj12nfvr3HOadwxQ8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCcS41IDAwUF2mjVmx3ZhaY2s5r1fPXMNr7eMi9htD4+LnzWiWmryZ+fjx49VlBw4cMMYfe+wxNadRo0bGuG2k0cqVK43xN954Q8257rrrjPGWLVuqOdpoDNv4C+11PvroIzUnPj7eGH/xxRfVnDVr1hjjixYtUnOys7PVZXWJtj3bxm1p41xs7rjjDmPcNjYoLi7OGK9fv76ao41Zsf0+2rKPP/5YzRk6dKgx/sEHH6g5gwcPNsZto4G085rt2OXNMcqbETADBgwwxm3bh3bOta2bNiaoY8eOas6ZcMUPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAl6OqtAbt27VKXad272k27bWydP9rrBAcHqzlax5Q3HZq4sLy5ybjW/VbTNzNv2LChMd6iRQs15+uvvzbGbV29mtjYWHWZ1oFn2weeeOIJY1y72b2I3sG8adMmNWfatGnG+JAhQ9Sc9PR0Y9x2jHrooYeM8aNHj6o5Wnfitm3b1JzaSPucvenctenRo4cxbttmMjIyjPGkpCQ1R+vQ3b17t5rTtWtXY3zKlClqzuHDh43x+++/X83p37+/Mf7FF1+oObX5XPT8888b46WlpWqON8dPreu5rKzM4+c6hSt+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgEoxzqQHaKBURvV1fG6ViExAQoC47ceKEMR4aGqrmaOtmu6k9Lh7aaIHIyEg1p0+fPsa4bXuOiYkxxm1jD7Qb1Kelpak5RUVFxrhtH7j++uuN8S5duqg5TZo0McY/+eQTNad58+bG+OTJk9UczaBBg9Rl4eHhxrhtnEtgYKAxPm7cODVn586dxnhdG+eijTTy5lirjd0QEVmzZo0x3rNnTzVH+1y0UToi+giexYsXqzm///3vjfGVK1eqOdqyO+64Q82ZMGGCMW4b56IdV0JCQtQcbaSNbfxJXl6eMT548GA1RztO5ubmqjnaetu2Ha2+2L59u5pzJlzxAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCbp6a4DWfVXTOVp3pIje/RQWFqbm1OYbYMMzWrfY8OHD1ZyGDRsa47auXq2b84cfflBzjh8/bow3atRIzfn888+N8fHjx6s5l1xyiTG+aNEiNeeDDz4wxrVuTxGRSy+91Bhv2bKlmvPUU0+pyzTx8fHG+ObNmz1+rlatWqnLtC7hV155Rc1JTEz0eB1qo44dOxrjo0aNUnO0Y7ftcxkwYIAxrnWIi+idnn5++mlb69S3dSlr54j7779fzdE6ZJctW6bmaNvzW2+9peZoXerBwcFqzpEjR4zx2NhYNUfrBLadp7WpBNpnICKSnZ1tjGsTNkRE9u7da4x/9913as6ZcMUPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgnEuNcB2E3jbCBZP2VrYtXb9w4cPqznamA3UPX379jXGX3vtNTXnww8/NMafffZZNUcbldCsWTM1RxtVUFJSouZ07dpVXabR9o/WrVurOf/617+M8VWrVqk5f/jDH4zxm2++2bJ2Nef6669Xl/Xu3dsYHzhwoJqzf/9+Y/yf//ynmrN48WJ1WV2ijSGyjSfSjrXaSCURkQULFhjjtmO6xjaW5MSJE8Z4cXGxmqONWbGNCzl27JgxPmXKFDVHG09TGzRu3NgYt41m0ZbZcrRjnvYZiIjs2LFDXeYtrvgBAAC4BIUfAACAS1D4AQAAuASFHwAAgEtQ+AEAALgEXb01IDc3V13Wtm1bY9zWZeXN6yQkJBjjGRkZHr8O6p7MzExj/I9//KOaM2fOHGO8QYMGas7vf/97Y7x+/fpqzqZNm4zxoUOHqjla95ut0/Cbb74xxgsKCtQcX19fY7xLly5qjp+f+bBp26d9fHyMcW+6BhctWqTmfPDBB8b4smXL1JywsDBjfPz48WrOF198oS672Gnbpq1L/ejRo+dqdVAD9u3bd0FfPycn57y+Hlf8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJRjnUgOKiorUZdqYC228g7evo42YON9t4rgwBg4caIzbxpL07t3bGI+IiFBztJvXJyUlqTnaKAtt9IiISPfu3Y3x9u3bqznPP/+8ukzTqlUrY1wbjyQi8sgjj3j8OtrYmLKyMjVH26evuOIKNadTp07G+LXXXqvmzJgxwxi3HW8aNmxojB86dEjNAVA7cMUPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAl6Oo9ja3bVrtpuu0G3N5072ri4uLUZdq6FRYW1tjro/Y6fvy4MX7s2DE1R+vMDAsLU3O0bT0wMFDNGTdunDE+b948NSc3N9cY37Jli5oTHBxsjHfs2FHNefnll43x6OhoNWfz5s3qMk1FRYXHOZrw8HB1WUlJiTG+evVqNUfrHo6KilJzdu7cqS4DULtxxQ8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCcS6n8WacizZ6wpbjzZiXyMhIj3NsYzZw8dizZ48x7u/vr+b88MMPxnh8fLyac/DgQWN8yJAhas53331njLdq1UrN0UYXPf/882rOkSNHjPFt27apOYmJica4r6+vmlNWVqYu03gzziU/P98Yz8nJUXO0401MTIya06tXL2M8KytLzcnLy1OXAajduOIHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASdPXWANsNy7XuXW86A2052o3W9+3b5/HreNPZjAtr3bp1xvhf//pXNeeTTz4xxo8fP67mZGZmGuMZGRlqjvZ8zZs3V3NKSkqM8f3796s5LVq0MMZ37dql5tx6663G+GeffabmaGp6v0lISDDGf//736s5nTp1MsbffvttNUfrCN+8ebNl7QDUVVzxAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl2Ccy2lsIxk04eHhHudo41dEREaMGOFxjjevo2GcS92jjeTo0KGDmtO+fXtjfNCgQWpOt27djPHOnTurOStWrDDGc3Jy1JznnnvOGI+JiVFzQkJCjPGePXuqOV27djXGx48fr+Zo6tevry7TRjFVVFSoOYGBgcb4lClT1Jzvv//eGF+zZo2as337dmM8NjZWzQFQd3HFDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJXycs2zT9KbbtS7ypqO1S5cuas68efOM8S+++ELNue2224zxtLQ0Nadv377G+NChQ9Wc1atXG+Nu6uqtjb9PXdzXbN3jnTp1Msa1TlcREX9/f2O8SZMmas7x48eN8YiICDXngw8+MMZt3bb16nn+72VtO/Nm+5s6daq6LC8vzxgvKChQc7TftbS0VM155pln1GUa9jXg/DjTvsYVPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcImzHucCAACAuo0rfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4VfLvPvuu+Lj4yPff//9GR/bt29f6du377lfKQAA/j8fHx956qmnKn8+dd7auXPnBVsnnD0Kv7Pk4+NzVv8tXbrUmF9RUSHvvfeedO/eXaKjoyUsLEwuueQSuf3222XFihXnfP03btwoTz31FDsm6pxTJ5VT/wUGBkqjRo0kNTVV/v73v0tBQcGFXkWgVjPtQ5dcconcc889kpWVdaFXD+eZ34Vegbpi2rRpVX5+7733JD09vVq8TZs2xvz77rtP/vGPf8j1118vt9xyi/j5+cmWLVtkwYIF0rx5c+nRo4fH6/TZZ5+d9WM3btwoTz/9tPTt21eSk5M9fi3gQvvzn/8szZo1kxMnTsjBgwdl6dKl8sADD8hLL70k8+bNk0svvfRCryJQq53ah4qLi+Wbb76RyZMny/z582X9+vUSHBx8oVcP5wmF31m69dZbq/y8YsUKSU9PrxY3ycrKkkmTJsldd90lb775ZpVlr7zyihw+fNirdfL39z/jY4qLi8/qcUBtN3DgQOnSpUvlz4899pgsXrxYrrvuOhk6dKhs2rRJgoKCjLmFhYUSEhJyvlYVqJV+vg/95je/kZiYGHnppZdk7ty5MmrUqAu8ducO+39V/Kn3PNixY4c4jiM9e/astszHx0fi4uKqxUtKSuShhx6S2NhYCQkJkRtuuKFagXj6d/yWLl0qPj4+Mn36dPnTn/4kjRs3luDgYPn73/8uw4cPFxGRq6666ox/lgbqin79+skTTzwhu3btkvfff19ERMaMGSOhoaGSmZkpgwYNkrCwMLnllltE5ORXLl555RVp166dBAYGSnx8vIwfP15ycnKqPO/3338vqamp0qBBAwkKCpJmzZrJHXfcUeUx06dPl86dO0tYWJiEh4dLhw4d5NVXXz0/vzhQA/r16yciJ89R2nfGx4wZ4/VfiSZNmiTt2rWTgIAAadSokUyYMEFyc3Mrl99zzz0SGhoqRUVF1XJHjRolCQkJUl5eXhlbsGCB9O7dW0JCQiQsLEwGDx4sGzZsqLa+2v6Pkyj8zoOkpCQREZk5c6ZxAze59957Ze3atfLkk0/K3XffLR9//LHcc889Z5X7zDPPyKeffir//d//Lc8++6xcc801ct9994mIyOOPPy7Tpk2TadOmqX+WBuqS2267TUSqfvWhrKxMUlNTJS4uTl544QW56aabRERk/Pjx8vvf/1569uwpr776qowdO1bS0tIkNTVVTpw4ISIihw4dkmuuuUZ27twpjz76qLz22mtyyy23VPkubnp6uowaNUqioqJk4sSJ8te//lX69u0ry5YtO4+/OfDLZGZmiohITExMjT/3U089JRMmTJBGjRrJiy++KDfddJO88cYbcs0111TuayNGjJDCwkL59NNPq+QWFRXJxx9/LMOGDRNfX18ROfl1q8GDB0toaKhMnDhRnnjiCdm4caP06tWr2nfXtf0f/58Dr0yYMMHx5O27/fbbHRFxoqKinBtuuMF54YUXnE2bNlV73DvvvOOIiDNgwACnoqKiMv7ggw86vr6+Tm5ubmWsT58+Tp8+fSp/XrJkiSMiTvPmzZ2ioqIqzztz5kxHRJwlS5ac/S8J1AKn9olVq1apj4mIiHA6duzoOI7jjB492hER59FHH63ymK+//toRESctLa1KfOHChVXis2fPPuPr3X///U54eLhTVlbm7a8FnDen9qHPP//cOXz4sLNnzx5n+vTpTkxMjBMUFOTs3bu32vnklNGjRztJSUlVYiLiPPnkk9Wef8eOHY7jOM6hQ4ccf39/55prrnHKy8srH/f66687IuL861//chzHcSoqKpzGjRs7N910U5XnnzFjhiMizldffeU4juMUFBQ4kZGRzl133VXlcQcPHnQiIiKqxLX9H/+HK37nyTvvvCOvv/66NGvWTGbPni3//d//LW3atJH+/fvLvn37qj1+3Lhx4uPjU/lz7969pby8XHbt2nXG1xo9erT6XSfgYhQaGlqtu/fuu++u8vPMmTMlIiJCrr76asnOzq78r3PnzhIaGipLliwREZHIyEgREfnkk08qr0ycLjIyUgoLCyU9Pb3mfxngHBkwYIDExsZKkyZNZOTIkRIaGiqzZ8+Wxo0b1+jrfP7551JaWioPPPCA1Kv3f2XGXXfdJeHh4ZVX+Hx8fGT48OEyf/58OXbsWOXjPvzwQ2ncuLH06tVLRE5eYc/NzZVRo0ZV2Xd9fX2le/fulfvuz52+/+P/UPjVoGPHjsnBgwcr//v5d/Lq1asnEyZMkNWrV0t2drbMnTtXBg4cKIsXL5aRI0dWe66mTZtW+TkqKkpEpNp3kUyaNWv2C38ToG45duyYhIWFVf7s5+cniYmJVR6TkZEheXl5EhcXJ7GxsVX+O3bsmBw6dEhERPr06SM33XSTPP3009KgQQO5/vrr5Z133pGSkpLK5/rd734nl1xyiQwcOFASExPljjvukIULF56fXxbw0j/+8Q9JT0+XJUuWyMaNG2X79u2Smppa469z6gJFq1atqsT9/f2lefPmVS5gjBgxQo4fPy7z5s0TkZP78vz582X48OGVFz8yMjJE5OR3Ek/fdz/77LPKffcU0/6P/0NXbw164YUX5Omnn678OSkpyTg3LyYmRoYOHSpDhw6Vvn37ypdffim7du2q/C6giFR+r+F0juOccT242gc32bt3r+Tl5UmLFi0qYwEBAVWuNIicbOyIi4uTtLQ04/PExsaKyMmrEB999JGsWLFCPv74Y1m0aJHccccd8uKLL8qKFSskNDRU4uLiZM2aNbJo0SJZsGCBLFiwQN555x25/fbbZerUqefulwV+gW7dulXpjP85Hx8f4/nl580V50KPHj0kOTlZZsyYIb/+9a/l448/luPHj8uIESMqH1NRUSEiJ7/nl5CQUO05/PyqljKm/R//h8KvBt1+++2Vl6ZFzq4A69Kli3z55Zdy4MCBKoVfTfv5n42Bi8mpWZpnunKRkpIin3/+ufTs2fOs9s0ePXpIjx495C9/+Yv8+9//lltuuUWmT58uv/nNb0Tk5NWLIUOGyJAhQ6SiokJ+97vfyRtvvCFPPPFElSIUqAuioqJk+/bt1eJn8/Wi0506l23ZskWaN29eGS8tLZUdO3bIgAEDqjz+5ptvlldffVXy8/Plww8/lOTk5CqzbVNSUkREJC4urlouPEdJXIOaN28uAwYMqPzv1PiWgwcPysaNG6s9vrS0VL744gupV6/eOT9RnJph9PNWeqCuW7x4sTzzzDPSrFmzM45suPnmm6W8vFyeeeaZasvKysoq942cnJxqVz4uv/xyEZHKP/ceOXKkyvJ69epVDpD++Z+EgboiJSVFNm/eXOUrSmvXrvWqU33AgAHi7+8vf//736vsS2+//bbk5eXJ4MGDqzx+xIgRUlJSIlOnTpWFCxfKzTffXGV5amqqhIeHy7PPPmv83q23s3Ddiit+58HevXulW7du0q9fP+nfv78kJCTIoUOH5IMPPpC1a9fKAw88IA0aNDin63D55ZeLr6+vTJw4UfLy8iQgIED69etnnCEI1EYLFiyQzZs3S1lZmWRlZcnixYslPT1dkpKSZN68eRIYGGjN79Onj4wfP16ee+45WbNmjVxzzTVSv359ycjIkJkzZ8qrr74qw4YNk6lTp8qkSZPkhhtukJSUFCkoKJC33npLwsPDZdCgQSJycvjt0aNHpV+/fpKYmCi7du2S1157TS6//HLGJKFOuuOOO+Sll16S1NRUufPOO+XQoUMyZcoUadeuneTn53v0XLGxsfLYY4/J008/Lddee60MHTpUtmzZIpMmTZKuXbtWu/FBp06dpEWLFvLHP/5RSkpKqvyZV0QkPDxcJk+eLLfddpt06tRJRo4cKbGxsbJ792759NNPpWfPnvL666//4vfALSj8zoNWrVrJK6+8IvPnz5dJkyZJVlaWBAYGSvv27eWtt96SO++885yvQ0JCgkyZMkWee+45ufPOO6W8vFyWLFlC4Yc643/+539E5OSfWKOjo6VDhw7yyiuvyNixY6s0dthMmTJFOnfuLG+88YY8/vjj4ufnJ8nJyXLrrbdWXqHv06ePrFy5UqZPny5ZWVkSEREh3bp1k7S0tMrGqVtvvVXefPNNmTRpkuTm5kpCQoKMGDFCnnrqKb5bhDqpTZs28t5778n//M//yEMPPSRt27aVadOmyb///W+vhv0/9dRTEhsbK6+//ro8+OCDEh0dLePGjZNnn31W6tevX+3xI0aMkL/85S/SokUL6dSpU7Xlv/71r6VRo0by17/+VZ5//nkpKSmRxo0bS+/evWXs2LHe/Mqu5eOcTbcAAAAA6jz+aQoAAOASFH4AAAAuQeEHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASZz3A2S33erXdw/P48ePGePv27dWcyZMnG+ObN29Wc7Zt22aMn7pRtcmQIUOM8fDwcDXn1G2oThcQEKDmmG6Xc6Z1q81q4xhLt+xrd911l7rs5/e8/rnFixerOVOnTv3F63Q2Bg4caIwPHz5czdHW7csvv6yRdaoL2NfqFttQ5NGjRxvj8fHxak5xcbExbjsX/vGPfzTGTfcUxv85077GFT8AAACXoPADAABwCQo/AAAAl6DwAwAAcAkf5yy/cVubvwSrNSOEhIR4nFNaWqrmlJSUGOPHjh1Tc7QvfL/44otqTl5enjHu56f34gQGBhrjgwcPVnM2btxojDdq1EjNqVfP/G8FrelDROTo0aMe55wvfOG8ZrRu3VpdtnDhQmPctn9qIiIi1GX169c3xteuXavm+Pr6GuO2hi1tm8nNzVVztGNHdHS0mqO9P2VlZWpObca+VjutWLHCGO/evbuak5GRYYxnZWWpOdp2a9unW7ZsaYy/9dZbas5DDz2kLnMLmjsAAAAgIhR+AAAArkHhBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuUWfGudjuORsXF2eMayMURLwb56K9VbbXOXTokLpMo41Mqen74Wr3JQ4ODlZztPEXtu1D+30KCgrUHNuInJrEiImasWnTJnVZQkKCMX7w4EE1R9tmbGJiYjx+Lm0Mko02skK7l7eIvp01bdpUzfnss8+M8ZtuusmydrWXm/c17XVs74l2rLWdB7Tnsx3TZ86caYzbxm1po1lsI5r8/f2N8ZycHDWnSZMmHj2XiEjHjh3VZRrt87FtH7X5/vSMcwEAAICIUPgBAAC4BoUfAACAS1D4AQAAuASFHwAAgEv4XegVOJ3WbXvttdeqOfv37zfGd+3apeYcPXrUGI+KilJztA4fbZ1tz6e9vojeLVRUVKTmNGjQwBi3dSlrnVHedDLZOhqLi4uN8RYtWqg5mZmZxvj56vaFZ2z7QH5+vjFu68zTOnHLy8vVHO04YFs3jW2/0Z6vfv36ao7WZXf48GE1p1mzZuoy1C3edPXatnVPDR06VF2mdQ9nZ2erOYmJicZ4bm6umqN1Cdu67rVjx/vvv6/meEP7HGyfjzefaW3BFT8AAACXoPADAABwCQo/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHCJWjfORbsps238SatWrYzxvXv3qjnaSAZbO7p2A2rbGAfb82m0ETCRkZFqTklJiTFuWzdtXIBtjEBQUJAxHhYWpuZo620bG6O9DuNcLqxevXoZ4+Hh4WrOkSNHjHHbOBdvboCubTPePJdtBIw3Yxy0/dA2Bik+Pt4Y79Kli5rz/fffq8tw4XizDTZs2NAY//Wvf63mJCcnG+O288C+ffuMcT8/vTzQ9l3bGCRtPEyjRo3UnAMHDhjjt9xyi5pz5ZVXGuPbtm1TczZv3myMz549W83R9nfbeBpvtoNzgSt+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuUeu6evv162eM2zpAP//8c2M8MDBQzdG6j7SOWtvzaTefFtG7A22dPxpbt632fLZOQ+3m3LbX0boQtY5KEb07zXaD+piYGGNc6xAVqT0dUxczrdPP1nWvde3ZOt7LysqMcdu+VpM3WrfRtjNbJ3BhYaExnpKSouZkZGR4tmKoc2yTGv7whz94/HzadAfbvqYd722d+tp+qE2+ENGP6bZJDUVFRca4rUM3Pz/fo9cXERk0aJAxrnVJi4i8/PLLxnhdOA9xxQ8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyi1o1zyczMNMZfeeUVNScpKckYf/vtt9Wc6OhoY1wbcSJibzvXaK3dtptm226OrdHGX9hay7XRLNpIABvthvIi+hicFi1aqDnaiIGdO3eqOcXFxeoy1IylS5ca423btlVzrr32WmP8V7/6lZozfvx4Y9w2zkcbp+LNOBfbuCVtFJR2HBIRGTJkiDHeoEEDNefdd99Vl+HicPvtt6vLtHNRTk6OmqMda21jY7RRQ7bznXb+sp0HtGO67XxTUFBgjNtGJ2nnT9s+nZ2dbYx36NBBzWnXrp0xvmHDBjWntuCKHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4BIUfAACAS1yQrl7bjdFbt25tjP/Xf/2XmjNjxgxjPDQ0VM2ZOnWqMW67mXVQUJAxbuuc1ToAbZ27WoeujfZ8WseWiH6jbS0uondI2jqmBgwYYIxPnz5dzfH39zfGbTcBp6v3wrFts5988okxbtvO7r33XmNc2wdF9G3Qtm627VajbZs2q1evNsYPHDjg8XPh4tGyZUt1mXbusE2eKCoqMsZtnbPedMNrXcK2fU3rRradp7Vjen5+vpqj7Z/BwcEe59jO7VdccYUxTlcvAAAAag0KPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl7gg41yaNGmiLktISDDGU1JS1Jz09HRj/Ne//rWa88MPPxjju3btUnMOHz5sjNvaxLX1tt1oW3P8+HGPc2yt/9qNtm0jbbQbd6empqo5Wuv/mjVr1JxWrVoZ4w0bNlRzjhw5oi7DueXNeKJt27apOdpYChtt/IWNNq7BNspC229svBnbou272s3uUffYtvPo6GhjXDsPiYhEREQY47Z9Qzum28ZjafuNbZxLQUGBMa6d823P16NHDzVHOw9kZmaqOdp7bTu3JyYmqstqO674AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4xAXp6rV1/nz99dfGeEhIiJqTkZFhjNu6h9u1a2eMR0VFqTnLly83xm03mQ4PDzfGte4rEb1ry3Zj6piYGGO8Q4cOao7WtZWXl6fmLFy40BgfOHCgmuPNTasLCwuNcW86N3HuedNpauuy0zrYbV3q3rB179ZkjjfO1+vgwrF1iGsdpfHx8WpOdna2MW7ruteUlJSoy7TjcIMGDdScgIAAj19H6zhev369muPv72+MBwYGqjna+Vh7fVtOXcAVPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcIkLMs5l//79Xi3TeNMmfsUVV3j8Otr4E9vNmrVRJrbW8oYNGxrjthtGnzhxwhi3jYTIyckxxn18fNSc7t27G+P//ve/1ZwVK1aoyzR79+71OAcXTr16+r8hvRn1oo188maci20fsG3rnj6fN7+njTfrhrrFNnJs3759xnh0dLSas337dmO8WbNmao42vks7r9pex3Yc0GRlZanLtHE3BQUFao42UsY2CkwbkbNt2zY1RxvVVhdwxQ8AAMAlKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUuSFdvTdO6d99//301Z8yYMcb40aNH1Rytq9fWiax16FZUVKg52g3qY2Ji1JyioiJj3NbZnJCQYIx///33ao7WDb1gwQI1Z8+ePeoyXBxs27M3vOkO1HJs3bZa56yto1ZblpmZaVk7uFlkZKQxrk1jENHPN40bN1Zz1qxZY4zbumC1CRO2ffDgwYPGuHYeEtGnUqxbt07N0TpntW5fEf09tf0+oaGhxrjt/BkREaEuq+244gcAAOASFH4AAAAuQeEHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5RZ8a52MYraDdNt+Xk5OQY4/n5+WpOmzZtjHFba7nWjm67+bN2g3o/P/3j0t6D9evXqzna76O9vog+YsDW2s44l4ufr6+vukzbnmxjKbTxCrZxEdo6aPuGiH6MsOVoy2w3bddGMR05ckTNqekRObhwtJEptnOHtq3bzgMdOnQwxn/88Uc1p1GjRsZ4aWmpmhMWFmaM2/YBb8asaMts79uhQ4eM8Y4dO6o52udTWFio5gQEBHgUF7GPhzmfuOIHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5B4QcAAOASdaar19Zl502Otuzo0aNqTkJCgjGudQSJiCxfvlxdpomNjTXGbR1B+/fvN8YzMjLUHK3T0NYxtXnzZmN87969ag4ufuXl5R7nPPjgg+oyf39/Y9x2s3mtA9DW3e8NrUs5MjJSzRkzZowx/uKLL9bAGqG2s3XiarQuWFsHvXZMt+032vPZusq1c8Tu3bvVHO2ca3tvvNl3tckTSUlJao7Wpax9BiL6+6Y9lwhdvQAAADjPKPwAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyizoxzqWlaW3VwcLCao920esOGDWqO1g6el5fncY5243oRfZSF7QbYWku+rY1fa5W33ZhaY2vV92Z8D8497TPz5vMaPXq0uky7Qb1tlMX5or0H2r4hIjJu3Dhj3DbORXtP2W/qHm1kiTefl7Zv2ISEhKjLjh8/boxHRUWpOTk5Oca4bR/QzmtBQUFqjnb+0s7FIvp50jZKRRt3YzveaM9nG+9WW3DFDwAAwCUo/AAAAFyCwg8AAMAlKPwAAABcgsIPAADAJVzb1bt582ZjPCsrS82JjY01xlu2bKnmaJ1MWieViEjjxo2N8aNHj6o5rVq1MsYPHjyo5mjLGjZsqOYUFhZ6vG64eGhdbmVlZWqO1uV27NgxNUfr5qvprl5vOme1Dk1bt6WWY+sA1Dokbe+B7XPAhaN9zrYuWG2b2b9/v5oTERFhjDdo0EDN0Y7ptm1Tmwhhoz1feXm5mqPth9p5VUQ/fy1btkzN6dChgzHu7++v5mjrYOugri244gcAAOASFH4AAAAuQeEHAADgEhR+AAAALkHhBwAA4BIUfgAAAC5xUY9zCQ8PV5dpLeS2m2Zrrffa+BURvU08ICBAzdFujm0bF5GXl2eMX3LJJWpObm6uMd6oUSM1Z9OmTca4rSUfFw9vbiqvjZKwjT3QRkxoY15sbKNZtGUVFRVqjjcjYLRRFpGRkWqONm7Jm/cAF5a2rdvGesXFxRnjGzZsUHNSUlKM8ZiYGDVn9+7dxnhJSYmao/0+thFN2jnCtj17M9ZJ+11//PFHNadt27bGeHBwsJqj/T7aGJ7ahCMIAACAS1D4AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBL1P72k18gPz9fXaZ1JV166aVqzrZt24zxPXv2eLZiYu841rp3bV292jokJSWpOaGhoca4rTMrKyvLGLd1QcLdtC47W2eetj1501Vs67bVns+2PWvPZ1s3rdPPmxu600Ff92ifc2lpqZqjdXyvWLFCzWnatKkxrnWV25Z5s3/6+/urORrbfqOtg20f0PY1bVKAiMj+/fs9ei4R/bOzTeyoLbjiBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgEhR+AAAALnFRj3Ox0W6+bBuZouXUr19fzdHa0XNzc9UcrYXc1iaurZttXEBZWZkxbhtlUVxcbIzbRsBovBnNgbonISHBGLfdnF3bNmxjHLQc2zgXbZltlIXmxIkT6jJtn46IiPD4dWy/D2onbXuyjQvRRqPYxof17NnTGLeNDdLOX7b9U/t9bPuNtu/azjfejEbRxtPY9psvv/zSGL/55pvVnOzsbGNcOxfXJlzxAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCdd29WqdTLaOnJycHGO8U6dOHr/+3r171WVaV5J2024RvQPM1gV58OBBY9zWzRUfH2+MHz58WM2Bu2n7mq0D0JsuSG1b96ZD10brQrS9jm2fwsVPO694s11kZGSoy7TzQGhoqJqjTXfQnktEX2/bpAYtx7ZPa89nm1ahTZ7Qzl0iIl999ZUxbusq1l7H1kFdW3A0AgAAcAkKPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl3DtOJewsDBjfO3atWrOmjVrjHHbTaavvvpqY9x2c/a8vDxjXBuLIaK35GvPJaK3qttGszRu3NgYz8zMVHOOHz+uLsPF78SJE8a4bfSDtk/ZcrwZ26I9n+11tGW2m8Br74FtLIWnr4/aSxtZoo0EEdGP94cOHVJztLExtmOwNrbFNmrGmzE0tv3D0xzbyJSSkhJj3LbfFBQUGOO2cS7ae2AbnVNbcMUPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlXNvVW1RUZIzbOlrDw8ONcVuX1bZt24zxqKgoNcebDsDs7GyP161hw4bGuNYhLKJ3odk6mbSOMluXF52LF4/CwkJj3NYNr20bNb3NeNPVq3XzlZeXqznaemvHFBtvuiNxYWkdut50ztq2M61D9+jRo2qO1gls2z+92de038f2Hmid+t508EdHR6vLcnJyjHHt/RSp2/shV/wAAABcgsIPAADAJSj8AAAAXILCDwAAwCUo/AAAAFyCwg8AAMAlXDvORRuN0q1bNzVHG/Via6/ftWuXMZ6cnKzmxMXFGeN79uxRc7TxMN6MpbC112s3wLbdBFzDyBZ30G6AbqNtG7YRExrb2AVvxsZoy7wZT6PtT3AH27gQ23lFo43Vys/PV3O0bTMgIEDN0bZ1b/YbbdSNiH4u8mZfi4mJUXO0cTe2z0Bbb20cW23CFT8AAACXoPADAABwCQo/AAAAl6DwAwAAcAkKPwAAAJdwbVdvVlaWMW7r6m3UqJExrt3gWUTk0ksvNca1G9eL6F1J3nR5aR3CIiLHjh0zxoOCgtQc7abymzZt8mzFxN4BRsfvxSMvL88Y96Yzz6Ymb5puey5tvW03jtdycnNzPVovEfaNukjrTrVtM4cOHfL4dbTO1QMHDqg5gYGBxrhtuoO2Pfv56SWFN/untg62c6HWVdukSROPX3/fvn3qspCQEGM8LCzM49c537jiBwAA4BIUfgAAAC5B4QcAAOASFH4AAAAuQeEHAADgEhR+AAAALuHacS5aO7g2ekJEpGPHjsa4rU29uLjYGC8tLVVztDErthxtWVFRkZqjtcp7M2rGlqNhLAU0NTmaxRu2bVO7ObttNIu/v78xfqF/T5wfkZGRxrhtZIptBIsmPz/fGNfOKTa2kSnaGBpbjva72t4DbT+0jYIqKyszxgMCAtQcjW2cS/PmzY1xbcxLbcIVPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl3BtV69my5Yt6jKtWyg6OlrN0Tp8QkND1ZxmzZoZ440aNVJz5syZY4wHBQWpOUePHjXGbZ3ALVq0MMYjIiLUnCNHjqjLcPErKSkxxs9XB6A3bOum3YR969atao7WBakdU3Bx0c4DWoe4iMjx48c9fp3evXsb47buVI22zYro3ei2/VPrxLXta1o3vG2/CQwMNMaTk5PVHI3tXKgdb7TjQ23CFT8AAACXoPADAABwCQo/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJ145z0VrVW7dureZoN8Bu3LixmpOVlWWMr169Ws3p37+/MZ6SkqLmdOjQwRhftWqVmqO1qttuMq3d6NrW+q+x3aC+JkdzoObYPjNNYWGhMW7bZmxjITR+fubDmW30gzZiwvb62g3vbfunNmLCm5EdqHu0c4dtRJc3I1jat29vjF977bVqjjZORds3vOXNaBZtXysuLlZztGNUXl6eZe3Mdu/erS676qqrjPGaft/OBa74AQAAuASFHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4hI9zlu2T3nTz1Wba7zNo0CA1x5ubzWs5+/fvV3O07sQ2bdqoOVoH2MaNG9UcrdPQ1jFlW1YX1cbu4dq8r2mduLZ9QLNs2TJ1WXx8vMevo3XiavuTLcfW1asts3Xoavun1hkooncH2n4fW4fkhebmfU3bnkeMGKHmfPrpp8Z4ZmZmjawTzqx58+bqslGjRhnjc+bMUXM2bNjwS1fprJxpX+OKHwAAgEtQ+AEAALgEhR8AAIBLUPgBAAC4BIUfAACAS1D4AQAAuMRZj3MBAABA3cYVPwAAAJeg8AMAAHAJCj8AAACXoPADAABwCQo/AAAAl6DwAwAAcAkKPwAAAJeg8AMAAHAJCj8AAACX+H/mHQMnttmQOAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "..\n",
        " .. figure:: /_static/img/basics/fashion_mnist.png\n",
        "   :alt: fashion_mnist\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 独自のファイルのためのカスタムデータセットを作成する\n",
        "\n",
        "カスタム Dataset クラスは、3 つの関数を実装する必要があります。init__`、`__len__`、`__getitem__` の3つの関数を実装する必要があります。\n",
        "この実装を見てみましょう。\n",
        "はディレクトリ ``img_dir`` に、ラベルは CSV ファイル ``annotations_file`` にそれぞれ保存されています。\n",
        "\n",
        "次のセクションでは、それぞれの関数で何が起こっているのかを分解して説明します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### __init__\n",
        "\n",
        "__init__ 関数は、Dataset オブジェクトのインスタンスを作成する際に一度だけ実行されます。初期化するのは\n",
        "を含むディレクトリ，アノテーションファイル，および両変換（次のセクションで詳しく説明します）を初期化します．\n",
        "を含むディレクトリを初期化します。）\n",
        "\n",
        "labels.csvファイルは次のようになります：::\n",
        "\n",
        "    tshirt1.jpg, 0\n",
        "    tshirt2.jpg, 0\n",
        "    ......\n",
        "    ankleboot999.jpg, 9\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### __len__\n",
        "\n",
        "__len__ 関数は、データセットのサンプル数を返します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def __len__(self):\n",
        "    return len(self.img_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### __getitem__\n",
        "\n",
        "__getitem__ 関数は、データセットから指定されたインデックス ``idx`` にあるサンプルをロードして返します。\n",
        "このインデックスをもとに、ディスク上の画像の位置を特定し、 ``read_image`` を用いてテンソルへの変換を行い、 ``self.img_labels`` の csv データから対応するラベルを取得します。\n",
        "``self.img_labels`` の csv データから対応するラベルを取得し、(必要に応じて) 変換関数を呼び出して、テンソル画像とそれに対応するラベルを\n",
        "を呼び出し、テンソル画像と対応するラベルをタプルで返す。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path)\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoaders を使って学習用のデータを準備する\n",
        "``Dataset`` は、データセットの特徴量とラベルを一度に取得する。モデルの学習では、通常\n",
        "モデルのオーバーフィッティングを減らすために、エポック毎にデータをシャッフルします。また、Pythonの ``multiprocessing`` を使用して、データの取得を高速化します。\n",
        "Python の ``multiprocessing`` を使用してデータ検索を高速化する。\n",
        "\n",
        "``DataLoader`` はイテラブルで、このような複雑な処理を簡単な API で抽象化したものである。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoader を繰り返し処理する\n",
        "\n",
        "データセットを ``DataLoader`` にロードしたので、必要に応じてデータセットを繰り返し処理することができます。\n",
        "以下の各反復処理では、 ``train_features`` と ``train_labels`` (それぞれ ``batch_size=64`` の特徴とラベルを含む) のバッチが返されます。\n",
        "``shuffle=True`` を指定したため、すべてのバッチを繰り返し処理した後に、データがシャッフルされます（データのロード順序をより細かく制御する場合は\n",
        "より細かいデータロード順序の制御については、[Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler) を参照してください)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhYklEQVR4nO3de2zV9f3H8ddpaU9bKKe2pZcjLRYQUbksQ+kYyg9HA3SJAyGbtyxgDERXzJA5TRcV3ZZ008QZDcN/FGYi3hKBSAyLgpS4AVtRwpizoV03CrRFGtrT62np+f7+IHYeKeDnw+n59PJ8JN+EnnNe/X767bd9cXrOeR+f53meAACIswTXCwAAjE4UEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnxrhewDdFIhGdPn1a6enp8vl8rpcDADDkeZ7a2toUDAaVkHDp+zlDroBOnz6tgoIC18sAAFyl+vp6TZw48ZLXD7kCSk9Pd70EjADTp0+3ym3cuNE4s2vXLuNMZ2encea2224zzhw6dMg4I0nvvPOOVQ74uiv9Ph+0Atq0aZOef/55NTY2avbs2Xr55Zc1d+7cK+b4sxtiITEx0SqXlpZmnElOTjbO9Pb2GmdSU1ONM0lJScaZkcj29wqjMq/OlY77oDwJ4e2339aGDRu0ceNGffrpp5o9e7aWLFmiM2fODMbuAADD0KAU0AsvvKA1a9bogQce0E033aRXXnlFaWlpeu211wZjdwCAYSjmBdTT06PDhw+rpKTkfztJSFBJSYkOHDhw0e3D4bBCoVDUBgAY+WJeQGfPnlVfX59yc3OjLs/NzVVjY+NFt6+oqFAgEOjfeAYcAIwOzl+IWl5ertbW1v6tvr7e9ZIAAHEQ82fBZWdnKzExUU1NTVGXNzU1KS8v76Lb+/1++f3+WC8DADDExfweUHJysubMmaM9e/b0XxaJRLRnzx7Nmzcv1rsDAAxTg/I6oA0bNmjVqlW65ZZbNHfuXL344ovq6OjQAw88MBi7AwAMQ4NSQHfffbe+/PJLPf3002psbNR3vvMd7d69+6InJgAARi+fN8Re6hsKhRQIBFwvY0iwefX2EPt2XqS0tNQ48/3vf984s3r1auOMJGVkZBhn+vr6jDM2EwpspifY/iz96Ec/Ms78/e9/N84M9MxYjBytra0aP378Ja93/iw4AMDoRAEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnGEYaJwkJ5l0fiUSMM/PnzzfOvPbaa8YZyW59WVlZxpnu7m7jjM3aJOnUqVPGmQkTJhhn0tPTjTPHjx83zlxuEOTl2HxNNsNSz507Z5wJh8PGmXXr1hlnJKmqqso4E6+f9eGAYaQAgCGJAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJ5iGbSExMdE409fXZ5yxmar7xRdfxGU/0oXvlamkpCTjjM0p2tnZaZyRpKamJuNMRkaGccbmODQ3Nxtn/H6/cUaSUlNTjTM2X1NycrJxJi0tzThj8zMrSTfffLNxpqenxzgTr98p8cY0bADAkEQBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJ8a4XsBwFIlE4rKfbdu2GWdSUlKMM6dPnzbOSFJWVpZxxmbQbGNjo3EmLy/POCNJ586dM86cOnXKOBMOh40zY8aY/7jm5+cbZyS7IaHp6enGGZvBnWfOnDHOTJw40TgjSa+++qpx5qc//alxJl6/U4Ya7gEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMMI7XgeV5c9nPttdcaZ5KSkowzwWDQOCNJfr/fONPU1GScsRlYmZBg93+rxMRE44zNYFGb/aSlpRlnbI9DTk6Ocaa9vd0409raapzJyMgwzrS1tRlnJGnatGlWOVPx+p0y1HAPCADgBAUEAHAi5gX0zDPPyOfzRW3Tp0+P9W4AAMPcoDwGdPPNN+ujjz76304s3kgLADCyDUozjBkzxvodKQEAo8OgPAZ0/PhxBYNBTZ48Wffff79OnDhxyduGw2GFQqGoDQAw8sW8gIqLi7V161bt3r1bmzdvVl1dnW6//fZLPg2yoqJCgUCgfysoKIj1kgAAQ1DMC6i0tFQ//vGPNWvWLC1ZskQffPCBWlpa9M477wx4+/LycrW2tvZv9fX1sV4SAGAIGvRnB2RkZGjatGmqqakZ8Hq/32/1gkYAwPA26K8Dam9vV21trfLz8wd7VwCAYSTmBfTYY4+psrJS//nPf/TXv/5Vd911lxITE3XvvffGelcAgGEs5n+CO3nypO699141NzdrwoQJuu2223Tw4EFNmDAh1rsCAAxjMS+gt956K9afckSweTFuamqqcebs2bPGmZaWFuOMJKs/q3Z1dcUl09vba5yR7AaL2jyGaTMktLOz0zhjM1TUVnp6unGmsLDQOHPq1CnjzMmTJ40zkqymuNxyyy3GmaqqKuPMSMAsOACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwYtDfkA4XFBcXG2dshlx6nmecSUpKMs5IdsNSU1JSjDM2g1y//PJL44wkRSIR44zt8TN17tw540xHR4fVvkKhkHHGZsCqzTlkcz7YDqf1+XzGmRUrVhhnGEYKAEAcUUAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ATTsOPkjjvuMM4kJiYaZ/r6+owzNpO6JamlpSUuGZsJ1bbTj22On80E8u7ubuNMVlaWcWbs2LHGGUnq6uoyzth8nyZPnmycsVmbzaRu230VFBRY7Ws04h4QAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADjBMNI4mTFjhnGmo6PDONPT02OcOXv2rHFGshuOeeTIEeNMW1ubcWbKlCnGGUny+/3GmfPnzxtnxowx/9HLyckxzoTDYeOMZDeEs7Oz0ziTlpZmnMnIyDDO2Aw9laTW1lbjTDAYtNrXaMQ9IACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwgmGkcXL99dcbZ/r6+owzNkMkx44da5yRpEgkYpwZN26ccaa5udk4097ebpyRpOzsbKucKZthpDZfk833yDZnM4TTZpCrzflqM2RWsjvm1113ndW+RiPuAQEAnKCAAABOGBfQ/v37deeddyoYDMrn82nHjh1R13uep6efflr5+flKTU1VSUmJjh8/Hqv1AgBGCOMC6ujo0OzZs7Vp06YBr3/uuef00ksv6ZVXXtGhQ4c0duxYLVmyRN3d3Ve9WADAyGH8SGhpaalKS0sHvM7zPL344ot68skntWzZMknS66+/rtzcXO3YsUP33HPP1a0WADBixPQxoLq6OjU2NqqkpKT/skAgoOLiYh04cGDATDgcVigUitoAACNfTAuosbFRkpSbmxt1eW5ubv9131RRUaFAINC/FRQUxHJJAIAhyvmz4MrLy9Xa2tq/1dfXu14SACAOYlpAeXl5kqSmpqaoy5uamvqv+ya/36/x48dHbQCAkS+mBVRUVKS8vDzt2bOn/7JQKKRDhw5p3rx5sdwVAGCYM34WXHt7u2pqavo/rqur05EjR5SZmanCwkKtX79ev/3tb3X99derqKhITz31lILBoJYvXx7LdQMAhjnjAqqqqtIdd9zR//GGDRskSatWrdLWrVv1+OOPq6OjQ2vXrlVLS4tuu+027d69WykpKbFbNQBg2PN5nue5XsTXhUIhBQIB18uIOZtpEL29vcaZ1NRU48y1115rnJEuvO7L1MmTJ40zX7/H/W2lp6cbZyRd8rHKy7E5Dj09PcaZhATzv5jbvqzBZhBuZmamcaawsNA4k5ycbJw5deqUcUaSamtrjTNFRUXGmUmTJhlnhoPW1tbLPq7v/FlwAIDRiQICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACeM344BdlOJMzIyjDPnzp0zzti8o2xDQ4NxRpLS0tKMM+PGjTPO2LyVh83EZMnue2szDfv8+fPGGRu2b4PS3d1tnAmHw3HJ+P1+44zN91WymxRvs69rrrnGOGPz+2Go4R4QAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADjBMFIL1113nXEmEAgYZ1paWowzmZmZxplDhw4ZZyQpGAwaZ6ZNm2acsRnu2NfXZ5yRJJ/PZ5xJSkoyzsTrfEhMTDTOSHbDXG2GsnZ1dRln/v3vfxtncnJyjDOS3XBfm8HDU6ZMMc5UVVUZZ4Ya7gEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMMI7VQWFhonLEZcmkz3NFGc3OzVS43N9c4c+7cOeOM3+83ztgcb9tcOBw2ztgMME1LSzPO2Az7lKSUlBTjTCgUMs7YnON79+41zpSVlRlnJKmtrc04Y3MOTZo0yTjDMFIAACxRQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAmGkVrIzs42ztgMNUxIiM//DyKRiFVuzBjz08dmUKPN0NNTp04ZZySpt7fXOGMzWNRmPzZDT22Gv0rS+fPnjTMTJkwwzowdO9Y4E6+BtpLU3t5unOnu7jbO5OXlGWdGAu4BAQCcoIAAAE4YF9D+/ft15513KhgMyufzaceOHVHXr169Wj6fL2pbunRprNYLABghjAuoo6NDs2fP1qZNmy55m6VLl6qhoaF/e/PNN69qkQCAkcf4UeTS0lKVlpZe9jZ+v3/UPqgGAPh2BuUxoH379iknJ0c33HCDHn744cu+5XM4HFYoFIraAAAjX8wLaOnSpXr99de1Z88e/f73v1dlZaVKS0vV19c34O0rKioUCAT6t4KCglgvCQAwBMX8dUD33HNP/79nzpypWbNmacqUKdq3b58WLVp00e3Ly8u1YcOG/o9DoRAlBACjwKA/DXvy5MnKzs5WTU3NgNf7/X6NHz8+agMAjHyDXkAnT55Uc3Oz8vPzB3tXAIBhxPhPcO3t7VH3Zurq6nTkyBFlZmYqMzNTzz77rFauXKm8vDzV1tbq8ccf19SpU7VkyZKYLhwAMLwZF1BVVZXuuOOO/o+/evxm1apV2rx5s44ePao//elPamlpUTAY1OLFi/Wb3/zGehYTAGBkMi6ghQsXyvO8S17/5z//+aoWNBzY/DnRZuCnzeBOG0eOHLHK3XLLLcYZm0GSmZmZxpmUlBTjjCSlpqYaZxITE+OS6enpMc7YsjnHbb4mm3O8o6PDOGMrXt+njIwM48xIwCw4AIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOBHzt+QeDSZOnGicGTPG/FAnJycbZ2zU19db5dLS0owzNl9Td3e3caavr884Y5trbm42zti8PUlSUpJxxnYq+JdffmmcsfmacnJyjDM2X9P58+eNM7Z6e3uNMzYT30cC7gEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMMI7WQlZVlnOns7DTOjB071jjT3t5unLEZnihJiYmJxpmzZ88aZ1paWowzNsNfJampqck4M2HCBONMamqqcSYUChlnbL5HktTW1macsTkONsNfExLM/9/c09NjnLHdV0dHh3HG5nwYCbgHBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOMIzUwrhx44wzNgM18/PzjTNdXV3GGZuBi5LdoEubYak+n884YzPkUpKSk5ONM93d3cYZm/XZfG9tj0NKSopxJhwOG2dshp7edNNNxhnbgbvxGpaak5NjnBkJuAcEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE4wjNRCWlqa6yVcUkdHh3HmJz/5idW+gsGgceb8+fNxyXR2dhpnJCkjI8M4k56ebpyxGXra2tpqnLEZYCrZfU2BQMA4YzNwNysryzhjcw5JdueRzQBTv99vnBkJuAcEAHCCAgIAOGFUQBUVFbr11luVnp6unJwcLV++XNXV1VG36e7uVllZmbKysjRu3DitXLlSTU1NMV00AGD4MyqgyspKlZWV6eDBg/rwww/V29urxYsXRz3u8Oijj+r999/Xu+++q8rKSp0+fVorVqyI+cIBAMOb0ZMQdu/eHfXx1q1blZOTo8OHD2vBggVqbW3Vq6++qm3btukHP/iBJGnLli268cYbdfDgQX3ve9+L3coBAMPaVT0G9NWzcjIzMyVJhw8fVm9vr0pKSvpvM336dBUWFurAgQMDfo5wOKxQKBS1AQBGPusCikQiWr9+vebPn68ZM2ZIkhobG5WcnHzRU1lzc3PV2Ng44OepqKhQIBDo3woKCmyXBAAYRqwLqKysTMeOHdNbb711VQsoLy9Xa2tr/1ZfX39Vnw8AMDxYvRB13bp12rVrl/bv36+JEyf2X56Xl6eenh61tLRE3QtqampSXl7egJ/L7/eP2hdhAcBoZnQPyPM8rVu3Ttu3b9fevXtVVFQUdf2cOXOUlJSkPXv29F9WXV2tEydOaN68ebFZMQBgRDC6B1RWVqZt27Zp586dSk9P739cJxAIKDU1VYFAQA8++KA2bNigzMxMjR8/Xo888ojmzZvHM+AAAFGMCmjz5s2SpIULF0ZdvmXLFq1evVqS9Ic//EEJCQlauXKlwuGwlixZoj/+8Y8xWSwAYOTweZ7nuV7E14VCIauhhvH0wQcfGGe++efKbyM3N9c4k5SUZJyxGTwpSVOnTjXOfPWUfRORSMQ4Y/s15eTkGGfGjRtnnLEZLNrd3W2csVmbFL/1/eMf/zDOnD171jgTDoeNM5LU0NBgnOnt7TXO/POf/zTOLF++3DgTb62trRo/fvwlr2cWHADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJywekfU0S45Odk44/P5jDM2E3y//k60g62mpiZu+wKuRkKC3f+1ExMTjTM2vx9s1zfcjc6vGgDgHAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcYBjpENbb22uc6e7uHoSVxE5SUpJxJhKJGGfGjLE7tfv6+qxypjzPi8t+4slm4O758+cHYSUXa21ttcrZDCPt6Ogwztier8Md94AAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwInROQHvKnV1dRlnenp6jDMpKSnGmc8//9w4E082wydtBnfGa6gohoeqqiqr3NSpU40zNgN3R+Jw2m+De0AAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ATDSC3YDBb1+XzGmUgkYpwJh8PGGWCksx1Oa5NLTEw0zqSlpRlnRgLuAQEAnKCAAABOGBVQRUWFbr31VqWnpysnJ0fLly9XdXV11G0WLlwon88XtT300EMxXTQAYPgzKqDKykqVlZXp4MGD+vDDD9Xb26vFixero6Mj6nZr1qxRQ0ND//bcc8/FdNEAgOHP6EkIu3fvjvp469atysnJ0eHDh7VgwYL+y9PS0pSXlxebFQIARqSregyotbVVkpSZmRl1+RtvvKHs7GzNmDFD5eXl6uzsvOTnCIfDCoVCURsAYOSzfhp2JBLR+vXrNX/+fM2YMaP/8vvuu0+TJk1SMBjU0aNH9cQTT6i6ulrvvffegJ+noqJCzz77rO0yAADDlHUBlZWV6dixY/rkk0+iLl+7dm3/v2fOnKn8/HwtWrRItbW1mjJlykWfp7y8XBs2bOj/OBQKqaCgwHZZAIBhwqqA1q1bp127dmn//v2aOHHiZW9bXFwsSaqpqRmwgPx+v/x+v80yAADDmFEBeZ6nRx55RNu3b9e+fftUVFR0xcyRI0ckSfn5+VYLBACMTEYFVFZWpm3btmnnzp1KT09XY2OjJCkQCCg1NVW1tbXatm2bfvjDHyorK0tHjx7Vo48+qgULFmjWrFmD8gUAAIYnowLavHmzpAsvNv26LVu2aPXq1UpOTtZHH32kF198UR0dHSooKNDKlSv15JNPxmzBAICRwfhPcJdTUFCgysrKq1oQAGB0YBq2hUmTJhlnbKbdjhs3zjjDEzownCQkmL8U0WZK/PTp040zkpSRkWGcGTPG/NeqzXEYCUbnVw0AcI4CAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATjCM1MLGjRuNMwO9G+yV3HjjjcaZY8eOGWfi6UoT1TG6xOt8WLZsmVWuoKDAONPW1macqa6uNs6MBNwDAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATgy5WXDDYVZYb2+vcSYcDhtnurq6jDM9PT3GGcCVeP289/X1WeVsftbPnz9vnIlEIsaZ4eBK31+fN8R+4588edJqACAAYGipr6/XxIkTL3n9kCugSCSi06dPKz09XT6fL+q6UCikgoIC1dfXa/z48Y5W6B7H4QKOwwUchws4DhcMhePgeZ7a2toUDAaVkHDpR3qG3J/gEhISLtuYkjR+/PhRfYJ9heNwAcfhAo7DBRyHC1wfh0AgcMXb8CQEAIATFBAAwIlhVUB+v18bN26U3+93vRSnOA4XcBwu4DhcwHG4YDgdhyH3JAQAwOgwrO4BAQBGDgoIAOAEBQQAcIICAgA4MWwKaNOmTbruuuuUkpKi4uJi/e1vf3O9pLh75pln5PP5orbp06e7Xtag279/v+68804Fg0H5fD7t2LEj6nrP8/T0008rPz9fqampKikp0fHjx90sdhBd6TisXr36ovNj6dKlbhY7SCoqKnTrrbcqPT1dOTk5Wr58uaqrq6Nu093drbKyMmVlZWncuHFauXKlmpqaHK14cHyb47Bw4cKLzoeHHnrI0YoHNiwK6O2339aGDRu0ceNGffrpp5o9e7aWLFmiM2fOuF5a3N18881qaGjo3z755BPXSxp0HR0dmj17tjZt2jTg9c8995xeeuklvfLKKzp06JDGjh2rJUuWqLu7O84rHVxXOg6StHTp0qjz480334zjCgdfZWWlysrKdPDgQX344Yfq7e3V4sWL1dHR0X+bRx99VO+//77effddVVZW6vTp01qxYoXDVcfetzkOkrRmzZqo8+G5555ztOJL8IaBuXPnemVlZf0f9/X1ecFg0KuoqHC4qvjbuHGjN3v2bNfLcEqSt3379v6PI5GIl5eX5z3//PP9l7W0tHh+v9978803HawwPr55HDzP81atWuUtW7bMyXpcOXPmjCfJq6ys9Dzvwvc+KSnJe/fdd/tv869//cuT5B04cMDVMgfdN4+D53ne//3f/3k///nP3S3qWxjy94B6enp0+PBhlZSU9F+WkJCgkpISHThwwOHK3Dh+/LiCwaAmT56s+++/XydOnHC9JKfq6urU2NgYdX4EAgEVFxePyvNj3759ysnJ0Q033KCHH35Yzc3Nrpc0qFpbWyVJmZmZkqTDhw+rt7c36nyYPn26CgsLR/T58M3j8JU33nhD2dnZmjFjhsrLy9XZ2elieZc05IaRftPZs2fV19en3NzcqMtzc3P1xRdfOFqVG8XFxdq6datuuOEGNTQ06Nlnn9Xtt9+uY8eOKT093fXynGhsbJSkAc+Pr64bLZYuXaoVK1aoqKhItbW1+tWvfqXS0lIdOHBAiYmJrpcXc5FIROvXr9f8+fM1Y8YMSRfOh+TkZGVkZETddiSfDwMdB0m67777NGnSJAWDQR09elRPPPGEqqur9d577zlcbbQhX0D4n9LS0v5/z5o1S8XFxZo0aZLeeecdPfjggw5XhqHgnnvu6f/3zJkzNWvWLE2ZMkX79u3TokWLHK5scJSVlenYsWOj4nHQy7nUcVi7dm3/v2fOnKn8/HwtWrRItbW1mjJlSryXOaAh/ye47OxsJSYmXvQslqamJuXl5Tla1dCQkZGhadOmqaamxvVSnPnqHOD8uNjkyZOVnZ09Is+PdevWadeuXfr444+j3r4lLy9PPT09amlpibr9SD0fLnUcBlJcXCxJQ+p8GPIFlJycrDlz5mjPnj39l0UiEe3Zs0fz5s1zuDL32tvbVVtbq/z8fNdLcaaoqEh5eXlR50coFNKhQ4dG/flx8uRJNTc3j6jzw/M8rVu3Ttu3b9fevXtVVFQUdf2cOXOUlJQUdT5UV1frxIkTI+p8uNJxGMiRI0ckaWidD66fBfFtvPXWW57f7/e2bt3qff75597atWu9jIwMr7Gx0fXS4uoXv/iFt2/fPq+urs77y1/+4pWUlHjZ2dnemTNnXC9tULW1tXmfffaZ99lnn3mSvBdeeMH77LPPvP/+97+e53ne7373Oy8jI8PbuXOnd/ToUW/ZsmVeUVGR19XV5XjlsXW549DW1uY99thj3oEDB7y6ujrvo48+8r773e96119/vdfd3e166THz8MMPe4FAwNu3b5/X0NDQv3V2dvbf5qGHHvIKCwu9vXv3elVVVd68efO8efPmOVx17F3pONTU1Hi//vWvvaqqKq+urs7buXOnN3nyZG/BggWOVx5tWBSQ53neyy+/7BUWFnrJycne3LlzvYMHD7peUtzdfffdXn5+vpecnOxde+213t133+3V1NS4Xtag+/jjjz1JF22rVq3yPO/CU7GfeuopLzc31/P7/d6iRYu86upqt4seBJc7Dp2dnd7ixYu9CRMmeElJSd6kSZO8NWvWjLj/pA309UvytmzZ0n+brq4u72c/+5l3zTXXeGlpad5dd93lNTQ0uFv0ILjScThx4oS3YMECLzMz0/P7/d7UqVO9X/7yl15ra6vbhX8Db8cAAHBiyD8GBAAYmSggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgxP8D6FCxSHxmmMoAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 4\n"
          ]
        }
      ],
      "source": [
        "# Display image and label.\n",
        "# 繰り返し実行\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transforms\n",
        "機械学習アルゴリズムの学習に必要なデータは、必ずしも最終的に加工された形で提供されるわけではありません。\n",
        "機械学習アルゴリズムの学習に必要な最終的な加工が施された状態で提供されるとは限りません。私たちは、**変換**を使用して、データに何らかの操作を加え\n",
        "を使用して、データに何らかの加工を施し、学習に適した状態にします。\n",
        "\n",
        "すべてのTorchVisionデータセットには2つのパラメータがあります -``transform`` で特徴量を変更し\n",
        "ラベルを変更するための ``target_transform`` という2つのパラメータがあり、変換ロジックを含むcallableを受け取ります。\n",
        "[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)モジュールは、以下のような変換ロジックを提供します。\n",
        "モジュールは、よく使われるいくつかの変換をそのまま提供します。\n",
        "\n",
        "FashionMNISTの特徴はPIL Imageフォーマットで、ラベルは整数値です。\n",
        "学習には、特徴量を正規化したテンソル、ラベルを一光子符号化したテンソルが必要です。\n",
        "これらの変換を行うために、 ``ToTensor`` と ``Lambda`` を使用します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ToTensor()\n",
        "\n",
        "[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n",
        "PIL 画像または NumPy の ``ndarray`` を ``FloatTensor`` に変換し、画像のピクセル強度の値を [0., 1.] の範囲でスケーリングします．\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lambda Transforms\n",
        "\n",
        "ラムダ変換は、ユーザーが定義した任意のラムダ関数を適用する。ここでは、整数を一発符号化されたテンソルに変換する関数\n",
        "を定義し、整数をワンホットエンコードされたテンソルに変換する。\n",
        "まず、サイズ10（データセットのラベル数）のゼロテンソルを生成し[scatter_](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html)を呼び出す。\n",
        "ラベル ``y`` で与えられるインデックスに ``value=1`` を代入する。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ニューラルネットワークの構築\n",
        "\n",
        "ニューラルネットワークはデータに対して演算を行う層/モジュールで構成されています。\n",
        "[torch.nn](https://pytorch.org/docs/stable/nn.html)の名前空間は、あなたが独自のニューラルネットワークを構築するために必要なすべてのビルディングブロックを提供します。\n",
        "名前空間は、あなた自身のニューラルネットワークを構築するために必要なすべてのビルディングブロックを提供します。PyTorchのすべてのモジュールは[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)をサブクラスとしています。\n",
        "ニューラルネットワークは、他のモジュール(層)から構成されるモジュールそのものです。この入れ子構造により、複雑なアーキテクチャの構築と管理が容易になります。\n",
        "\n",
        "以下の章では、FashionMNISTデータセットの画像を分類するためのニューラルネットワークを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 学習用デバイスの取得\n",
        "GPUのようなハードウェアアクセラレータでモデルを学習できるようにしたい。\n",
        "が利用可能であれば 以下のようにチェックします。\n",
        "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html)が利用可能かどうか確認してみましょう。\n",
        "CPUを使い続けます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## クラスを定義する\n",
        "ニューラルネットワークを ``nn.Module`` のサブクラスとして定義し、 ``__init__`` でニューラルネットワークのレイヤーを初期化します。\n",
        "ニューラルネットワークのレイヤーを ``__init__`` で初期化します。各 ``nn.Module`` のサブクラスは、 ``forward`` で入力データに対する操作を実装しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``NeuralNetwork`` のインスタンスを生成して ``device`` に移動し、その構造を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "モデルを使用するには、入力データを渡します。これにより、モデルの ``forward`` といくつかの [バックグラウンド処理](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866) が実行されます。\n",
        "直接 ``model.forward()`` を呼び出さないでください!\n",
        "\n",
        "入力に対してモデルを呼び出すと、各クラスの10個の生の予測値の各出力に対応する dim=0 と、各出力の個々の値に対応する dim=1 の2次元のテンソルが返されます。\n",
        "これを ``nn.Softmax`` モジュールのインスタンスに渡すことで、予測確率を得ることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: tensor([3])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## モデルのレイヤー\n",
        "\n",
        "FashionMNISTモデルのレイヤーを分解してみましょう。これを説明するために\n",
        "サイズ28x28の3つの画像からなるミニバッチのサンプルを取り、それをネットワークに通すとどうなるかを見る。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Flatten\n",
        "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)を初期化する。\n",
        "レイヤーを初期化し，各2次元28x28画像を784ピクセル値の連続した配列に変換します (\n",
        "ミニバッチの次元(dim=0)は維持されます)に変換します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Linear\n",
        "[線形層](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "は、保存された重みとバイアスを使用して入力に線形変換を適用するモジュールです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.ReLU\n",
        "非線形アクティベーションは、モデルの入力と出力の間の複雑なマッピングを作成するものです。\n",
        "線形変換の後に適用することで、非線形性を導入し、ニューラルネットワークが様々な現象を学習できるようにします。\n",
        "様々な現象を学習させることができます。\n",
        "\n",
        "このモデルでは、線形層の間に[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)を使用しています。\n",
        "しかし、あなたのモデルに非直線性を導入するために他の活性化があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[ 0.0061,  0.3501, -0.2831, -0.0748, -0.5628, -0.3663,  0.5070, -0.0542,\n",
            "          0.0074, -0.3425,  0.2388,  0.5941, -0.0052, -0.2502,  0.1010, -0.1993,\n",
            "         -0.5807,  0.0267,  0.3028, -0.2059],\n",
            "        [ 0.1252,  0.2977, -0.2495, -0.3253, -0.0790, -0.4491,  0.4364, -0.4828,\n",
            "         -0.0539, -0.4670,  0.3034,  0.5885, -0.0623, -0.3835,  0.1391, -0.2973,\n",
            "         -0.8129,  0.3551,  0.4593, -0.5179],\n",
            "        [ 0.2823,  0.1273, -0.3789,  0.0602, -0.0666, -0.7040,  0.0302, -0.5546,\n",
            "          0.1214, -0.2027,  0.2214,  0.5720, -0.2314, -0.3769, -0.0344,  0.1990,\n",
            "         -0.9422,  0.2650,  0.2597, -0.3966]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0061, 0.3501, 0.0000, 0.0000, 0.0000, 0.0000, 0.5070, 0.0000, 0.0074,\n",
            "         0.0000, 0.2388, 0.5941, 0.0000, 0.0000, 0.1010, 0.0000, 0.0000, 0.0267,\n",
            "         0.3028, 0.0000],\n",
            "        [0.1252, 0.2977, 0.0000, 0.0000, 0.0000, 0.0000, 0.4364, 0.0000, 0.0000,\n",
            "         0.0000, 0.3034, 0.5885, 0.0000, 0.0000, 0.1391, 0.0000, 0.0000, 0.3551,\n",
            "         0.4593, 0.0000],\n",
            "        [0.2823, 0.1273, 0.0000, 0.0602, 0.0000, 0.0000, 0.0302, 0.0000, 0.1214,\n",
            "         0.0000, 0.2214, 0.5720, 0.0000, 0.0000, 0.0000, 0.1990, 0.0000, 0.2650,\n",
            "         0.2597, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Sequential\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)は、順番に並べられた\n",
        "モジュールのコンテナです。データは、定義されたとおりの順序ですべてのモジュールに渡されます。を使うことができます。\n",
        "シーケンシャルコンテナを使って、 ``seq_modules`` のように素早くネットワークを構築することができます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Softmax\n",
        "ニューラルネットワークの最後の線形層は `logits` - [-\\infty, \\infty] の実数- を返し、[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) モジュールに渡されます。logitsはモデルを表す[0, 1]の値にスケーリングされる。\n",
        "[0, 1]の値にスケーリングされ、各クラスに対するモデルの予測確率を表します。パラメータ ``dim`` は次元を表します。\n",
        "パラメータは，値の合計が1になる次元を表します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Parameters\n",
        "ニューラルネットワークの多くの層はパラメータ化されており、学習時に最適化された重みとバイアスがあります。\n",
        "とバイアスがあり、学習中に最適化されます。``nn.Module`` のサブクラスは自動的に\n",
        "をサブクラス化すると、モデルオブジェクト内で定義されたすべてのフィールドを追跡し、すべてのパラメータにアクセスできるようになります。\n",
        "また、モデルの ``parameters()`` や ``named_parameters()`` メソッドを用いて、すべてのパラメータにアクセスできるようになります。\n",
        "\n",
        "この例では、各パラメータを繰り返し処理し、そのサイズと値のプレビューを表示します。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0171, -0.0329, -0.0112,  ..., -0.0309,  0.0098, -0.0147],\n",
            "        [-0.0060,  0.0020,  0.0290,  ...,  0.0055,  0.0288, -0.0051]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0354, 0.0317], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0202, -0.0007, -0.0076,  ...,  0.0421,  0.0263,  0.0313],\n",
            "        [-0.0230,  0.0301, -0.0362,  ..., -0.0008, -0.0017, -0.0152]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0075, -0.0016], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0351, -0.0084,  0.0174,  ...,  0.0097, -0.0415, -0.0077],\n",
            "        [-0.0044, -0.0135,  0.0315,  ...,  0.0411, -0.0083, -0.0179]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0072, -0.0017], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic Differentiation with ``torch.autograd``\n",
        "\n",
        "ニューラルネットワークを学習させる場合、最もよく使われるアルゴリズムが**逆伝播法**である。このアルゴリズムでは、パラメータ（モデルの重み）が損失関数の**勾配**にしたがって調整される。\n",
        "\n",
        "この勾配を計算するために、PyTorchは ``torch.authime`` という微分エンジンを内蔵しています。\n",
        "と呼ばれる微分エンジンを内蔵しています。これは、任意の計算グラフに対して勾配の自動計算をサポートします。\n",
        "計算グラフの勾配の自動計算をサポートします。\n",
        "\n",
        "例えば、最も単純な1層のニューラルネットワークを考えてみましょう。\n",
        "パラメータ ``w`` と ``b`` 、そして何らかの損失関数を持つ、最も単純な 1 層のニューラルネットワークを考えてみましょう。これは\n",
        "PyTorch では次のように定義できる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensors, Functions and Computational graph\n",
        "\n",
        "このコードは、次のような**計算グラフ**を定義しています。\n",
        "\n",
        "![図](https://pytorch.org/tutorials/_images/comp-graph.png)\n",
        "\n",
        "このネットワークでは、``w`` と ``b`` が **パラメータ** であり、これを最適化する必要があります。\n",
        "最適化する必要があります。したがって、これらの変数に関する損失関数の勾配を計算できる必要があります。\n",
        "関数の勾配を計算できるようにする必要があります。これを実現するために\n",
        "を設定する。\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``requires_grad`` の値は、テンソルを作成するときに設定するか、後で ``x.requires_grad_(True)`` メソッドを使用して設定することができる。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "テンソルに適用して計算グラフを構築する関数は、実際には ``Function`` クラスのオブジェクトである。このオブジェクトは、*forward* 方向の関数の計算方法と、*backward propagation* ステップでのその微分の計算方法を持っています。後方伝搬関数への参照は、テンソルの ``grad_fn`` プロパティに格納される。関数 ``Function`` の詳細については、[ドキュメント](https://pytorch.org/docs/stable/autograd.html#function)を参照してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x17ed279d0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x17ed27e80>\n"
          ]
        }
      ],
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Gradients\n",
        "\n",
        "ニューラルネットワークのパラメータの重みを最適化するためには、損失関数のパラメータに関する導関数を計算する必要があります。つまり、固定値 ``x``と``y``の下で、$\\frac{\\partial loss}{\\partial w}$ と $\\frac{\\partial loss}{\\partial b}$ が必要です。\n",
        "これらの導関数を計算するために、 ``loss.backward()`` を呼び出し、 ``w.grad`` と ``b.grad`` から値を取得します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2489, 0.3117, 0.3249],\n",
            "        [0.2489, 0.3117, 0.3249],\n",
            "        [0.2489, 0.3117, 0.3249],\n",
            "        [0.2489, 0.3117, 0.3249],\n",
            "        [0.2489, 0.3117, 0.3249]])\n",
            "tensor([0.2489, 0.3117, 0.3249])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Disabling Gradient Tracking\n",
        "\n",
        "デフォルトでは、 ``requires_grad=True`` のテンソルはすべて計算履歴を追跡し、勾配計算をサポートする。\n",
        "しかし、その必要がない場合もある。例えば、モデルを学習させ、それを入力データに適用したい場合、つまり、ネットワークを通して*フォワード*計算を行いたいだけという場合である。計算コードを ``torch.no_grad()`` ブロックで囲むことで、トラッキング計算を停止させることができます。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "同じ結果を得るためのもう一つの方法は、テンソルに対して ``detach()`` メソッドを使用することです。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "グラディエント・トラッキングを無効にしたい理由があります。\n",
        "  - ニューラルネットワークのいくつかのパラメータを**frozen parameters**としてマークするため。これは\n",
        "    これは非常に一般的なシナリオです。\n",
        "    [finetuning a pretrained network](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
        "  - 勾配を追跡しないテンソルの計算の方がより高速になるため、フォワードパスだけを行う場合の計算を**高速化する。\n",
        "    勾配を追跡しないテンソルの計算がより効率的であるため。\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More on Computational Graphs\n",
        "概念的には、autogradはデータ(テンソル)と実行されたすべての操作(結果の新しいテンソルも含む)の記録を\n",
        "[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) オブジェクトからなる有向非循環グラフ (DAG) に記録する。この DAG において、葉は入力テンソル、根は出力テンソ ルである。このグラフを根から葉までたどれば、鎖の法則を使って自動的に勾配を計算することができる。\n",
        "\n",
        "フォワードパスにおいて、autogradは同時に2つのことを行う。\n",
        "\n",
        "- 要求された演算を実行し、結果のテンソルを計算する\n",
        "- DAGの中で操作の*勾配関数*を維持する。\n",
        "\n",
        "バックワードパスは ``.backward()`` がDAGルートで呼ばれたときにキックオフされます。その後、 ``autograd`` が呼び出される。\n",
        "\n",
        "- 各 ``.grad_fn`` から勾配を計算する。\n",
        "- 各テンソルの ``.grad`` 属性に蓄積する。\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional Reading: テンソル勾配とヤコビアン積\n",
        "\n",
        "多くの場合，スカラー損失関数があり，あるパラメータに対する勾配 を計算する必要があります．しかし は、出力関数が任意のテンソルである場合です。このような場合、PyTorch では、実際の勾配ではなく、いわゆる **ヤコビアン積** を計算することができます。を計算することができます。\n",
        "\n",
        "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ と $\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$とする、ベクトル関数 $\\vec{y}=f(\\vec{x})$ の $\\vec{x}$ に対する $\\vec{y}$ の勾配は **Jacobian行列**で与えられる。\n",
        "\n",
        "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
        "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "      \\vdots & \\ddots & \\vdots\\\\\n",
        "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "      \\end{array}\\right)\\end{align}\n",
        "\n",
        "\n",
        "PyTorchでは、ヤコビアン行列そのものを計算する代わりに、与えられた入力ベクトル $v=(v_1 \\dots v_m)$ に対する **Jacobian Product** $v^T\\cdot J$ を計算することが可能です。これは、$v$を引数として ``backward`` を呼び出すことで実現されます。 $v$ のサイズは、積を計算したい元のテンソルのサイズと同じでなければなりません。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First call\n",
            "tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.]])\n",
            "\n",
            "Second call\n",
            "tensor([[8., 4., 4., 4., 4.],\n",
            "        [4., 8., 4., 4., 4.],\n",
            "        [4., 4., 8., 4., 4.],\n",
            "        [4., 4., 4., 8., 4.]])\n",
            "\n",
            "Call after zeroing gradients\n",
            "tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.]])\n"
          ]
        }
      ],
      "source": [
        "inp = torch.eye(4, 5, requires_grad=True)\n",
        "out = (inp+1).pow(2).t()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"First call\\n{inp.grad}\")\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nSecond call\\n{inp.grad}\")\n",
        "inp.grad.zero_()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "同じ引数で2回目の ``backward`` を呼び出したとき、勾配の値が異なることに注意してください。これは、 ``backward`` 伝搬を行う際に、PyTorch が **勾配を累積する** ためで、つまり、計算された勾配の値は、計算グラフのすべてのリーフノードの ``grad`` プロパティに追加されることになります。つまり、計算された勾配の値は、計算グラフのすべてのリーフノードの ``grad`` プロパティに追加されます。適切な勾配を計算したい場合は、事前に ``grad`` プロパティをゼロにする必要があります。実際のトレーニングでは、*最適化ツール* がこれを手助けしてくれます。\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "以前は ``backward()`` 関数をパラメータなしで呼び出していました。これは本質的には ``backward(torch.tensor(1.0))`` を呼び出すことと同じで、ニューラルネットワークの学習における損失のようなスカラー値を持つ関数の場合に勾配を計算するのに便利な方法です。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizing Model Parameters\n",
        "\n",
        "モデルとデータが揃ったので、次はデータに対してモデルのパラメータを最適化することで、モデルを訓練、検証、テストする番です。モデルの学習は反復プロセスです。各反復において、モデルは出力について推測し、その推測の誤差（*損失*）を計算し、その誤差のパラメータに対する導関数を収集し（前のセクションで見たように）、勾配降下を使用してこれらのパラメータを**最適化**します。\n",
        "\n",
        "## 前提となるコード\n",
        "データセットとデータローダー、モデルの構築で使ったコードを再利用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "ハイパーパラメーターは、モデルの最適化プロセスを制御するための調整可能なパラメーターです。ハイパーパラメータの値が異なると、モデルの学習や収束率に影響を与えます（ハイパーパラメータのチューニングについては[続きを読む](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)をご覧ください）。\n",
        "\n",
        "ここでは、学習用のハイパーパラメータを以下のように定義しています。\n",
        " - **エポック数** - データセットに対して反復処理を行う回数\n",
        " - **バッチサイズ** - パラメータが更新される前にネットワークに伝搬されるデータサンプルの個数\n",
        " - **学習速度** - 各バッチ/エポックにおいて、モデルのパラメータをどれだけ更新するか。小さい値を設定すると学習速度が遅くなり、大きい値を設定すると学習時に予測不可能な挙動をする可能性がある。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization Loop\n",
        "\n",
        "ハイパーパラメータを設定したら、ループ処理でモデルを学習し最適化することができます。ループ処理の各反復は、**エポック**と呼ばれます。\n",
        "\n",
        "各エポックは主に2つの部分から構成されています。\n",
        " - **訓練ループ** - 訓練データセットを繰り返し、最適なパラメータに収束させようとする。\n",
        " - **検証/テストループ** - テストデータセットを繰り返し処理し、モデルの性能が向上しているかどうかを確認します。\n",
        "\n",
        "トレーニングループで使われるいくつかの概念について簡単に説明します。\n",
        "\n",
        "### 損失関数\n",
        "\n",
        "学習データを提示されたとき、学習していないネットワークは正しい答えを出さない可能性が高いです。**損失関数**は、得られた結果の目標値に対する非類似度を測定するもので、学習中に最小化したい損失関数です。損失を計算するために、与えられたデータサンプルの入力を使って予測を行い、真のデータラベルの値と比較します。\n",
        "\n",
        "一般的な損失関数は回帰タスクの[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error)、分類の[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood)などです。\n",
        "[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) は ``nn.LogSoftmax`` と ``nn.NLLoss`` を結合したものです。\n",
        "\n",
        "モデルの出力ロジットを ``nn.CrossEntropyLoss`` に渡すと、ロジットが正規化されて予測誤差が計算されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer\n",
        "\n",
        "最適化とは、各トレーニングステップにおいて、モデルの誤差を減らすためにモデルのパラメータを調整するプロセスです。**最適化アルゴリズム** は、このプロセスの実行方法を定義します（この例では、Stochastic Gradient Descent を使用します）。\n",
        "すべての最適化ロジックは ``optimizer`` オブジェクトにカプセル化されています。ここでは、SGDオプティマイザを使用しています。さらに、PyTorchにはADAMやRMSPropなど多くの[異なるオプティマイザ](https://pytorch.org/docs/stable/optim.html)があります。\n",
        "\n",
        "学習が必要なモデルのパラメータを登録し、学習率ハイパーパラメータを渡すことでオプティマイザを初期化します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "学習ループの内部では、3つのステップで最適化が行われます。\n",
        " * モデルパラメータの勾配をリセットするために ``optimizer.zero_grad()`` をコールします。勾配はデフォルトで加算されます。二重計算を防ぐために、各反復で明示的にゼロにします。\n",
        " * 予測損失は ``loss.backward()`` のコールでバックプロパゲートされます。PyTorchは、各パラメータに対する損失の勾配を記録します。\n",
        " * 勾配が得られたら、 ``optimizer.step()`` を呼び出して、バックワードパスで収集した勾配を元にパラメータを調整します。\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Full Implementation\n",
        "最適化コードをループする ``train_loop`` と、テストデータに対してモデルの性能を評価する ``test_loop`` を定義する。\n",
        "テストデータに対してモデルの性能を評価する ``test_loop`` を定義する。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "損失関数とオプティマイザを初期化し、 ``train_loop`` と ``test_loop`` に渡します。\n",
        "エポック数を増やして、モデルの性能が向上していることを自由に確認することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.306156  [    0/60000]\n",
            "loss: 2.292078  [ 6400/60000]\n",
            "loss: 2.267754  [12800/60000]\n",
            "loss: 2.263288  [19200/60000]\n",
            "loss: 2.249501  [25600/60000]\n",
            "loss: 2.219507  [32000/60000]\n",
            "loss: 2.227329  [38400/60000]\n",
            "loss: 2.200620  [44800/60000]\n",
            "loss: 2.196187  [51200/60000]\n",
            "loss: 2.156735  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 2.158037 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.176250  [    0/60000]\n",
            "loss: 2.165834  [ 6400/60000]\n",
            "loss: 2.103708  [12800/60000]\n",
            "loss: 2.116235  [19200/60000]\n",
            "loss: 2.072739  [25600/60000]\n",
            "loss: 2.015505  [32000/60000]\n",
            "loss: 2.035472  [38400/60000]\n",
            "loss: 1.967360  [44800/60000]\n",
            "loss: 1.971847  [51200/60000]\n",
            "loss: 1.887933  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.6%, Avg loss: 1.894769 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.936851  [    0/60000]\n",
            "loss: 1.907815  [ 6400/60000]\n",
            "loss: 1.786523  [12800/60000]\n",
            "loss: 1.819150  [19200/60000]\n",
            "loss: 1.716983  [25600/60000]\n",
            "loss: 1.668361  [32000/60000]\n",
            "loss: 1.684486  [38400/60000]\n",
            "loss: 1.591605  [44800/60000]\n",
            "loss: 1.621162  [51200/60000]\n",
            "loss: 1.503565  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.1%, Avg loss: 1.526652 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.599309  [    0/60000]\n",
            "loss: 1.566296  [ 6400/60000]\n",
            "loss: 1.412187  [12800/60000]\n",
            "loss: 1.479218  [19200/60000]\n",
            "loss: 1.362955  [25600/60000]\n",
            "loss: 1.353800  [32000/60000]\n",
            "loss: 1.372866  [38400/60000]\n",
            "loss: 1.294803  [44800/60000]\n",
            "loss: 1.337255  [51200/60000]\n",
            "loss: 1.232377  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.8%, Avg loss: 1.256171 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.332974  [    0/60000]\n",
            "loss: 1.318480  [ 6400/60000]\n",
            "loss: 1.150413  [12800/60000]\n",
            "loss: 1.255623  [19200/60000]\n",
            "loss: 1.128314  [25600/60000]\n",
            "loss: 1.147510  [32000/60000]\n",
            "loss: 1.177960  [38400/60000]\n",
            "loss: 1.109096  [44800/60000]\n",
            "loss: 1.156478  [51200/60000]\n",
            "loss: 1.069754  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.1%, Avg loss: 1.086581 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.154161  [    0/60000]\n",
            "loss: 1.159597  [ 6400/60000]\n",
            "loss: 0.978046  [12800/60000]\n",
            "loss: 1.114968  [19200/60000]\n",
            "loss: 0.983694  [25600/60000]\n",
            "loss: 1.011436  [32000/60000]\n",
            "loss: 1.056219  [38400/60000]\n",
            "loss: 0.991454  [44800/60000]\n",
            "loss: 1.040035  [51200/60000]\n",
            "loss: 0.967489  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.978182 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.032165  [    0/60000]\n",
            "loss: 1.057613  [ 6400/60000]\n",
            "loss: 0.861701  [12800/60000]\n",
            "loss: 1.021787  [19200/60000]\n",
            "loss: 0.893645  [25600/60000]\n",
            "loss: 0.917785  [32000/60000]\n",
            "loss: 0.976655  [38400/60000]\n",
            "loss: 0.915925  [44800/60000]\n",
            "loss: 0.961324  [51200/60000]\n",
            "loss: 0.900178  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.0%, Avg loss: 0.905578 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.944213  [    0/60000]\n",
            "loss: 0.988327  [ 6400/60000]\n",
            "loss: 0.780012  [12800/60000]\n",
            "loss: 0.956527  [19200/60000]\n",
            "loss: 0.834480  [25600/60000]\n",
            "loss: 0.851019  [32000/60000]\n",
            "loss: 0.921271  [38400/60000]\n",
            "loss: 0.866388  [44800/60000]\n",
            "loss: 0.905864  [51200/60000]\n",
            "loss: 0.853128  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.854391 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.877829  [    0/60000]\n",
            "loss: 0.937604  [ 6400/60000]\n",
            "loss: 0.720345  [12800/60000]\n",
            "loss: 0.908911  [19200/60000]\n",
            "loss: 0.792998  [25600/60000]\n",
            "loss: 0.801731  [32000/60000]\n",
            "loss: 0.879747  [38400/60000]\n",
            "loss: 0.832421  [44800/60000]\n",
            "loss: 0.864920  [51200/60000]\n",
            "loss: 0.818006  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.816222 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.825544  [    0/60000]\n",
            "loss: 0.898037  [ 6400/60000]\n",
            "loss: 0.674777  [12800/60000]\n",
            "loss: 0.872726  [19200/60000]\n",
            "loss: 0.762026  [25600/60000]\n",
            "loss: 0.764157  [32000/60000]\n",
            "loss: 0.846403  [38400/60000]\n",
            "loss: 0.807614  [44800/60000]\n",
            "loss: 0.833236  [51200/60000]\n",
            "loss: 0.790197  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.786192 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Functions\n",
        "- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "\n",
        "### MSELOSS\n",
        "入力$x$とターゲット$y$の各要素間の平均二乗誤差（L2ノルムの二乗）を測定する基準を作成します。\n",
        "\n",
        "次元を維持する（reduction='none'）場合は、以下のように記述することができます。ここで $N$ はバッチサイズです。\n",
        "\n",
        "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2$ \n",
        "\n",
        "reductionを'none' ではない場合(デフォルトは'mean')は以下のように記述できます。\n",
        "\n",
        "$\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}$\n",
        " \n",
        "$x$と$y$はそれぞれ合計$n$個の要素を持つ任意の形のテンソルです。\n",
        "\n",
        "平均演算はやはり全要素に対して行われ、$n$で除算されます。\n",
        "\n",
        "reduction = 'sum' とすれば，$n$による除算を避けることができます。\n",
        "\n",
        "\n",
        "### NLLLoss\n",
        "負の対数尤度損失（Negative Log Likelihood Loss）。C個のクラスからなる分類問題を学習するのに有用です。\n",
        "\n",
        "オプションの引数 weight は、各クラスに重みを割り当てる 1次元テンソルでなければなりません。これは，アンバランスな学習セットを持っている場合に特に有用です。\n",
        "\n",
        "inputは、( $minibatch, C$ )または( $minibatch, C, d_1, d_2, ..., d_K$ )のいずれかのサイズのTensorでなければならず、K次元の場合は $K \\geq 1$ でなければならない。後者は、2次元画像の画素あたりのNLLLOSSを計算するような高次元の入力に有効である。\n",
        "\n",
        "ニューラルネットワークで対数確率を得るには、ネットワークの最後の層にLogSoftmax層を追加することで簡単に実現できます。余分なレイヤーを追加したくない場合は、代わりにCrossEntropyLossを使用することができます。\n",
        "\n",
        "この損失が期待するターゲットは、$[0, C-1]$ (C = クラス数) の範囲にあるクラスインデックスでなければなりません。\n",
        "\n",
        "また、ignore_indexが指定された場合、この損失はこのクラスインデックスを受け付けます (このインデックスは必ずしもクラスの範囲内でなくても構いません)。\n",
        "\n",
        "$x$がinput, $y$がtarget, $w$がweight, $N$はバッチサイズ.\n",
        "\n",
        "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$,\n",
        "\n",
        "reductionを'none' ではない場合(デフォルトは'mean')は以下のように記述できます。\n",
        "\n",
        "$\\ell(x, y) = \\begin{cases} \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, & \\text{if reduction} = \\text{`mean';}\\\\ \\sum_{n=1}^N l_n, & \\text{if reduction} = \\text{`sum'.} \\end{cases}$\n",
        "\n",
        "### CrossEntropyLoss\n",
        "\n",
        "入力ロジットとターゲット間のクロスエントロピーの損失を計算する。\n",
        "\n",
        "これは、C個のクラスを持つ分類問題を学習するときに有用である。オプションの引数weightは、各クラスに重みを割り当てる1次元テンソルでなければなりません。これは，アンバランスな学習セットを持っている場合に特に有用である．\n",
        "\n",
        "入力は、各クラスの非正規化ロジット（一般に、正であったり、和が1になる必要はない）を含むことが期待される。 inputは、バッチされていないinputではサイズ ($C$)、K次元の場合には$Kを1$とした ($minibatch, C$) または ($minibatch, C, d_1, d_2, ..., d_K$) のTensorでなければならない。最後に、2次元画像のピクセル毎のクロスエントロピー損失を計算するような高次元の入力に有用である。\n",
        "\n",
        "この基準が期待するtargetは、以下のいずれかを含むべきである。\n",
        "\n",
        "- $[0, C)$ ($C$ はクラス数); ignore_index が指定された場合、この損失はこのクラスインデックスも受け入れます（このインデックスは必ずしもクラス範囲内にあるとは限りません）。この場合の未削減(つまり削減を'なし'に設定した場合)の損失は次のように記述できる。\n",
        "\n",
        "    ここで$x$は入力、$y$はターゲット、$w$は重み、$C$はクラス数、$N$はミニバッチ次元と$K$次元の場合の$d_1, ..., d_kd$にまたがるものである。\n",
        "    \n",
        "    $\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}$\n",
        "\n",
        "    reductionが'none'でない場合(デフォルトは'mean')は以下のように記述できる。\n",
        "\n",
        "    $\\ell(x, y) = \\begin{cases} \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, & \\text{if reduction} = \\text{`mean';}\\\\ \\sum_{n=1}^N l_n, & \\text{if reduction} = \\text{`sum'.} \\end{cases}$\n",
        " \n",
        "    この場合、LogSoftmaxとNLLLossの組み合わせと同等であることに注意してください。\n",
        "\n",
        "- 各クラスの確率；混合ラベル、ラベルスムージングなど、ミニバッチアイテムごとに単一クラス以上のラベルが必要な場合に有用。この場合の未削減（つまり、削減を「なし」に設定した場合）の損失は、次のように記述できる。\n",
        "\n",
        "    ここで、$x$は入力、$y$はターゲット、$w$は重み、$C$はクラス数、$N$はミニバッチ次元とK次元の場合の$d_1, ..., d_kd$にまたがる。\n",
        "\n",
        "    $\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}$​\n",
        "\n",
        "    reductionが'none'でない場合(デフォルトは'mean')は以下のように記述できる。\n",
        "\n",
        "    $\\ell(x, y) = \\begin{cases} \\frac{\\sum_{n=1}^N l_n}{N}, & \\text{if reduction} = \\text{`mean';}\\\\ \\sum_{n=1}^N l_n, & \\text{if reduction} = \\text{`sum'.} \\end{cases}$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# torchtext ライブラリを使ったテキスト分類\n",
        "\n",
        "このチュートリアルでは、テキスト分類分析のためのデータセットを構築するために torchtext ライブラリを使用する方法を紹介します。ユーザは以下のような柔軟性を持つことができます。\n",
        "\n",
        "   - イテレータとして生データにアクセスする\n",
        "   - データ処理パイプラインを構築して、生のテキスト文字列を ``torch.Tensor`` に変換し、モデルの学習に使用する。\n",
        "   - [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) を使ってデータをシャッフルし、反復処理する。\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 生のデータセットイテレータへのアクセス\n",
        "\n",
        "torchtext ライブラリには、生のテキスト文字列を返す生のデータセットイテレータがいくつか用意されています。例えば、 ``AG_NEWS`` データセットイテレータは、ラベルとテキストのタプルとして生データを得ることができる。\n",
        "\n",
        "torchtextのデータセットにアクセスするには、https://github.com/pytorch/data の説明に従ってtorchdataをインストールしてください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train_iter = iter(AG_NEWS(split='train'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(train_iter)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データ処理パイプラインを用意する\n",
        "\n",
        "ボキャブラリー、ワードベクター、トークナイザーなど、torchtextライブラリの非常に基本的な構成要素について再確認しました。これらは生のテキスト文字列に対する基本的なデータ処理のビルディングブロックです。\n",
        "\n",
        "ここでは、トークナイザーとボキャブラリーを使った典型的な自然言語処理例を紹介します。まず、生の学習データセットから語彙を作る。ここでは、ビルトインの\n",
        "ファクトリー関数 `build_vocab_from_iterator` を使う。この関数では、トークンのリストかイテレータを受け取る。また、語彙に追加する特殊なシンボルを渡すこともできる。\n",
        "語彙に追加する特別な記号を渡すこともできる。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "vocabularyブロックは，トークンのリストを整数に変換する。\n",
        "::\n",
        "\n",
        "    vocab(['here', 'is', 'an', 'example'])\n",
        "    >>> [475, 21, 30, 5297]\n",
        "\n",
        "トークナイザーとボキャブラリーでテキスト処理パイプラインを準備します。テキストパイプラインとラベルパイプラインは、データセットイテレータからの生データ文字列を処理するために使用されます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "テキストパイプラインは，語彙に定義されたルックアップテーブルに基づいて，文字列を整数のリストに変換する．ラベルパイプラインは、ラベルを整数に変換する。例えば\n",
        "\n",
        "::\n",
        "\n",
        "    text_pipeline('here is the an example')\n",
        "    >>> [475, 21, 2, 30, 5297]\n",
        "    label_pipeline('10')\n",
        "    >>> 9\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データバッチとイテレータを生成する\n",
        "\n",
        "データローダー（DataLoader）は ``getitem()`` と ``len()`` プロトコルを実装したマップ形式のデータセットで動作し、インデックス/キーからデータサンプルへのマップを表現します。また、引数として ``False`` を指定した、反復処理可能なデータセットでも動作します。\n",
        "\n",
        "モデルに送信する前に、 ``collate_fn`` 関数は ``DataLoader`` から生成されたサンプルのバッチを処理する。``collate_fn`` への入力は ``DataLoader`` にあるバッチサイズのデータであり、 ``collate_fn`` は先に宣言したデータ処理パイプラインに従ってそれらを処理する。ここで注意してほしいのは、 ``collate_fn`` がトップレベルの def として宣言されていることだ。 これにより、この関数が各ワーカーで利用できるようになる。\n",
        "\n",
        "この例では、元のデータバッチの入力に含まれるテキストエントリーをリストにパックし、一つのテンソルとして連結して ``nn.EmbeddingBag`` の入力とします。offsetはテキストテンソル中の個々のシーケンスの開始インデックスを表すデリミタのテンソルである。ラベルは個々のテキストエントリのラベルを保存したテンソルである。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## モデルの定義\n",
        "\n",
        "モデルは [nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag)レイヤーと、分類を目的とした線形レイヤーから構成されます。``nn.EmbeddingBag``はデフォルトのモードが \"mean \"で、埋め込みの \"bag \"の平均値を計算する。ここでは、テキストのエントリは異なる長さを持っていますが、テキストの長さはオフセットで保存されているので、nn.EmbeddingBagモジュールはここでパディングを必要としません。\n",
        "\n",
        "さらに、``nn.EmbeddingBag`` は埋め込み全体の平均をその場で蓄積するので、パディングは必要ありません。\n",
        "さらに、``nn.EmbeddingBag`` は、埋め込み全体の平均をオンザフライで蓄積するため、一連の処理を行う際のパフォーマンスとメモリ効率を向上させることができる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## インスタンスを起動する\n",
        "\n",
        "``AG_NEWS`` のデータセットには4つのラベルがあり、したがってクラスの数は4である。\n",
        "\n",
        "::\n",
        "\n",
        "   1 : 世界\n",
        "   2 : スポーツ\n",
        "   3 : ビジネス\n",
        "   4 : 科学/技術\n",
        "\n",
        "埋め込み次元を64としたモデルを構築する。vocab sizeはvocabulary instanceの長さと等しい。クラス数はラベルの数に等しい。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## モデルの学習と結果の評価のための関数を定義する。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データセットの分割とモデルの実行\n",
        "\n",
        "元の ``AG_NEWS`` には検証用データセットがないため、学習用データセットを分割する。\n",
        "データセットを訓練用と検証用に分割し、分割比率を 0.95 (train) と\n",
        "0.05 (valid) とする。ここで使用するのは\n",
        "[torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)というPyTorchのコアライブラリにある関数を使います。\n",
        "\n",
        "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)を使用します。\n",
        "この基準は、 ``nn.LogSoftmax()`` と ``nn.NLLLoss()`` を1つのクラスに統合したものです。\n",
        "これは，C個のクラスからなる分類問題を学習するときに役立ちます．\n",
        "[SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)は、最適化手法として確率的勾配降下法を実装しています。初期学習率は 5.0 に設定されています。\n",
        "[ステップLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)は、エポックを通して学習率を調整するために使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.711\n",
            "| epoch   1 |  1000/ 1782 batches | accuracy    0.864\n",
            "| epoch   1 |  1500/ 1782 batches | accuracy    0.885\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  5.65s | valid accuracy    0.858 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | accuracy    0.902\n",
            "| epoch   2 |  1000/ 1782 batches | accuracy    0.906\n",
            "| epoch   2 |  1500/ 1782 batches | accuracy    0.908\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  5.45s | valid accuracy    0.868 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | accuracy    0.919\n",
            "| epoch   3 |  1000/ 1782 batches | accuracy    0.917\n",
            "| epoch   3 |  1500/ 1782 batches | accuracy    0.919\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  5.45s | valid accuracy    0.898 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | accuracy    0.929\n",
            "| epoch   4 |  1000/ 1782 batches | accuracy    0.925\n",
            "| epoch   4 |  1500/ 1782 batches | accuracy    0.924\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  5.51s | valid accuracy    0.902 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | accuracy    0.934\n",
            "| epoch   5 |  1000/ 1782 batches | accuracy    0.933\n",
            "| epoch   5 |  1500/ 1782 batches | accuracy    0.929\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  5.51s | valid accuracy    0.895 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | accuracy    0.951\n",
            "| epoch   6 |  1000/ 1782 batches | accuracy    0.949\n",
            "| epoch   6 |  1500/ 1782 batches | accuracy    0.951\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  5.46s | valid accuracy    0.905 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | accuracy    0.951\n",
            "| epoch   7 |  1000/ 1782 batches | accuracy    0.952\n",
            "| epoch   7 |  1500/ 1782 batches | accuracy    0.952\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  5.42s | valid accuracy    0.907 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | accuracy    0.953\n",
            "| epoch   8 |  1000/ 1782 batches | accuracy    0.952\n",
            "| epoch   8 |  1500/ 1782 batches | accuracy    0.953\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  5.39s | valid accuracy    0.907 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | accuracy    0.954\n",
            "| epoch   9 |  1000/ 1782 batches | accuracy    0.953\n",
            "| epoch   9 |  1500/ 1782 batches | accuracy    0.954\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  5.42s | valid accuracy    0.907 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | accuracy    0.957\n",
            "| epoch  10 |  1000/ 1782 batches | accuracy    0.954\n",
            "| epoch  10 |  1500/ 1782 batches | accuracy    0.956\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  5.42s | valid accuracy    0.908 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## テストデータセットでモデルを評価する\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "テストデータセットの結果を確認する...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.907\n"
          ]
        }
      ],
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ランダムなニュースでテストする\n",
        "\n",
        "今までの最適なモデルを使って、ゴルフのニュースをテストしてみましょう。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ]
        }
      ],
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "eaac4ee8e735b6cee17c1b0358cfc8773b6176eb143c6cf3f1e52e7612ffbee2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
