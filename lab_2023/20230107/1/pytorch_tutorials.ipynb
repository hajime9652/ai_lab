{"cells":[{"cell_type":"markdown","metadata":{"id":"8SB1lcfTtXQ4"},"source":["# Tensor\n","\n","Tensor（テンソル）は配列や行列に非常によく似た特殊なデータ構造です。\n","PyTorchでは、モデルの入力と出力、およびモデルのパラメータをエンコードするためにテンソルを使用します。\n","\n","テンソルは[NumPy](https://numpy.org/)のndarrayに似ていますが、テンソルはGPUや他のハードウェアアクセラレータ上で処理することができる点が異なります。一方で、テンソルとNumPyの配列は同じメモリを共有することができ、データをコピーする必要がありません（`bridge-to-np-label`を参照）。\n","さらにテンソルは、自動微分のために最適化されています（これについては後ほどAutogradのセクションで詳しく説明します）。"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TE2uWO0MtXQ9","executionInfo":{"status":"ok","timestamp":1673022978257,"user_tz":-540,"elapsed":334,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}}},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"9MJwxVq3tXQ-"},"source":["## Tensorの初期化\n","\n","\n","テンソルは様々な方法で初期化することができます。以下の例を見てみましょう。\n","\n","**データから直接作成する**\n","\n","テンソルはデータから直接作成することができます。このとき、データ型は自動的に推論されます。\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Qs2NKuDrtXQ_","executionInfo":{"status":"ok","timestamp":1673022978556,"user_tz":-540,"elapsed":9,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}}},"outputs":[],"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)"]},{"cell_type":"markdown","metadata":{"id":"37OWm3ketXQ_"},"source":["**NumPy array から生成する**\n","\n","テンソルはNumPyの配列から作成することができます（逆も同様です）。\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"aPivTWovtXRA","executionInfo":{"status":"ok","timestamp":1673022978556,"user_tz":-540,"elapsed":7,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}}},"outputs":[],"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)"]},{"cell_type":"markdown","metadata":{"id":"gJuxnVv0tXRA"},"source":["**他のテンソルから生成する**\n","\n","新しいテンソルは、明示的に上書きされない限り、引数のテンソルの特性(形状、データ型)を保持します。\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VY5OQPFjtXRB","executionInfo":{"status":"ok","timestamp":1673022978557,"user_tz":-540,"elapsed":8,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"acd32a2e-6319-432a-9402-1ccc796bf884"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.2047, 0.1791],\n","        [0.4233, 0.2341]]) \n","\n"]}],"source":["x_ones = torch.ones_like(x_data) # x_dataの特性(形状、データ型)を保持します\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # x_dataのデータ型を上書きします\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"7PcZfSNLtXRC"},"source":["**乱数や定数値を生成する**\n","\n","``shape`` はテンソルの次元を表すタプルです。  \n","``shape``を使ってテンソルの次元を設定して乱数や定数を生成してみます。\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALmwy-1ktXRD","executionInfo":{"status":"ok","timestamp":1673022979823,"user_tz":-540,"elapsed":1272,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"9638adb7-b873-49f0-ca4a-a45d38caba6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.1634, 0.7509, 0.8228],\n","        [0.5666, 0.2860, 0.9644]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"]},{"cell_type":"markdown","metadata":{"id":"NQxTC7lGtXRE"},"source":["## テンソルの属性\n","\n","テンソルは属性（attribute）として、次の3つを保持しています。\n","- 形状：`shape`\n","- データ型：`dtype`\n","- 保持されているデバイス：`device`"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f01Px7TUtXRE","executionInfo":{"status":"ok","timestamp":1673022979827,"user_tz":-540,"elapsed":1275,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"05c6dd77-8270-4c3d-aa9f-e3f6cfc7155d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"RGF4U1FztXRF"},"source":["## テンソルの演算\n","\n","算術、線形代数、行列操作（転置、インデックス、スライス）、サンプリングなど、100以上のテンソル演算が可能です。  \n","公式のドキュメントは[こちら](https://pytorch.org/docs/stable/torch.html)です。\n","\n","これらの操作はそれぞれGPUで実行できます(当然ですが、CPUよりも高速に実行できます)。  \n","Colabを使っている場合、`ランタイム→ランタイムのタイプを変更→ハードウェアアクセラレータ`でGPUを割り当てます。\n","\n","デフォルトでは、テンソルは CPU 上で作成されます。そのため、テンソルを明示的に GPU に移動させる必要があります。\n","``.to`` メソッドを用いると GPU に移動することができます。  \n","※大きいテンソルをCPU→GPUあるいはGPU→CPUへと転送させると時間とメモリの両面でボトルネックになる可能性があります。\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"VdB8BVUDtXRF","executionInfo":{"status":"ok","timestamp":1673022979828,"user_tz":-540,"elapsed":29,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}}},"outputs":[],"source":["# この記述でGPUが使える場合はテンソルをGPUへ転送することができます\n","if torch.cuda.is_available():\n","    tensor = tensor.to(\"cuda\")"]},{"cell_type":"markdown","metadata":{"id":"ZCmAH81ytXRG"},"source":["NumPy APIIと操作が似ているので、NumPy APIに慣れていればTensor APIは簡単に使うことができます。"]},{"cell_type":"markdown","metadata":{"id":"IeuT6PC6tXRG"},"source":["**NumPyライクな配列・スライスの操作をする**\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCKP1Yo5tXRG","executionInfo":{"status":"ok","timestamp":1673022979828,"user_tz":-540,"elapsed":29,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"253887b7-c23d-48fc-c245-7b37ba3db16a"},"outputs":[{"output_type":"stream","name":"stdout","text":["First row: tensor([1., 1., 1., 1.])\n","First column: tensor([1., 1., 1., 1.])\n","Last column: tensor([1., 1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["tensor = torch.ones(4, 4)\n","print(f\"First row: {tensor[0]}\")\n","print(f\"First column: {tensor[:, 0]}\")\n","print(f\"Last column: {tensor[..., -1]}\")\n","tensor[:,1] = 0\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"Aq0hskOQtXRH"},"source":["**テンソルを結合する**\n","\n","``torch.cat`` を使うと、一連のテンソルを指定した次元に沿って連結することができます。\n","[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)も参照してください。  \n","※``torch.cat`` とは微妙に異なる結合方法もあります。\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFBwPQOhtXRH","executionInfo":{"status":"ok","timestamp":1673022979828,"user_tz":-540,"elapsed":27,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"fc3af8a6-de19-45e5-9d6d-5ad9f4162bec"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"]}],"source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"]},{"cell_type":"markdown","metadata":{"id":"JFuKcJxZtXRH"},"source":["**算術演算子を使う**\n","\n","2つのテンソル間の行列の掛け算を計算します。y1, y2, y3 は同じ値になります。  \n","`tensor.T` はテンソルの転置を返します。\n","@は行列のための演算子です(参考：[Qiitaの記事](https://qiita.com/sci_Haru/items/aa0b35fb0fb71cabc808))。\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYAeSaGstXRI","executionInfo":{"status":"ok","timestamp":1673022979829,"user_tz":-540,"elapsed":26,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"815f018d-17d4-4437-c757-07f54a626ceb"},"outputs":[{"output_type":"stream","name":"stdout","text":["y1: tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n","y2: tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n","y3: tensor([[0.3484, 0.6255, 0.2917, 0.9612],\n","        [0.2573, 0.7901, 0.9924, 0.1935],\n","        [0.2392, 0.4113, 0.9303, 0.4336],\n","        [0.9871, 0.4340, 0.6279, 0.5244]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])"]},"metadata":{},"execution_count":14}],"source":["y1 = tensor @ tensor.T\n","print(f\"y1: {y1}\")\n","y2 = tensor.matmul(tensor.T)\n","print(f\"y2: {y2}\")\n","y3 = torch.rand_like(y1)\n","print(f\"y3: {y3}\")\n","\n","torch.matmul(tensor, tensor.T, out=y3)"]},{"cell_type":"markdown","source":["これは要素ごとの積を計算します。z1, z2, z3 は同じ値になります。\n"],"metadata":{"id":"Ofjtgs7Y0dhd"}},{"cell_type":"code","source":["z1 = tensor * tensor\n","print(f\"z1: {z1}\")\n","z2 = tensor.mul(tensor)\n","print(f\"z2: {z2}\")\n","z3 = torch.rand_like(tensor)\n","print(f\"z3: {z3}\")\n","\n","torch.mul(tensor, tensor, out=z3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNjhN1fP0dqA","executionInfo":{"status":"ok","timestamp":1673022979829,"user_tz":-540,"elapsed":24,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"ee95c12e-fe35-4324-d497-6ce8bdfbee93"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["z1: tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n","z2: tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n","z3: tensor([[0.7557, 0.3293, 0.0132, 0.2565],\n","        [0.9940, 0.3879, 0.0986, 0.4425],\n","        [0.2598, 0.8808, 0.2925, 0.9191],\n","        [0.5158, 0.3605, 0.7257, 0.7024]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"n-vANYW4tXRI"},"source":["**シングルエレメントテンソル、単元テンソルを使う**  \n","要素が一つのテンソルは`item()`メソッドを使うことでPythonの数値に変換することができます。  \n","例）合計値を算出する`sum()`の結果を格納した`agg`の場合\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5AYfo2UvtXRI","executionInfo":{"status":"ok","timestamp":1673022979829,"user_tz":-540,"elapsed":23,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"1d095c64-9f08-47cd-b21c-a3ef1dd6221d"},"outputs":[{"output_type":"stream","name":"stdout","text":["12.0 <class 'float'>\n"]}],"source":["agg = tensor.sum()\n","agg_item = agg.item()\n","print(agg_item, type(agg_item))"]},{"cell_type":"markdown","metadata":{"id":"BdNBTsBatXRJ"},"source":["**インプレース演算を使う**\n","\n","結果を元の変数（＝オペランド）に格納する演算をインプレース演算と呼びます。  \n","これらは接尾辞 ``_`` で表現されます。\n","例えば、 ``x.copy_(y)`` や ``x.t_()`` は ``x``(=\n","オペランド)\n"," を変更することになります。"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HI7DzCn1tXRJ","executionInfo":{"status":"ok","timestamp":1673022979830,"user_tz":-540,"elapsed":23,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"9889de32-7276-4700-e3e5-88cdef2dbfa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]]) \n","\n","tensor([[6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.]])\n"]}],"source":["print(f\"{tensor} \\n\")\n","tensor.add_(5)\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"Noug704GtXRJ"},"source":["インプレース演算はメモリを節約できますが、自動微分のアルゴリズムに問題が生じる可能性があるので使用はお勧めしません。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VPX6d88UtXRJ"},"source":["## NumPyとのブリッジ\n","CPU上のテンソルとNumPyの配列は、メモリを共有することができます。\n","一方を変更するともう一方も変更されます。"]},{"cell_type":"markdown","metadata":{"id":"Ffw-MZJ-tXRJ"},"source":["### TensorからNumPyへの変換\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0D8O8RwdtXRJ","executionInfo":{"status":"ok","timestamp":1673022979830,"user_tz":-540,"elapsed":19,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"f5d8371e-7e1e-4ce5-ebbb-f9b5352a3b43"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([1., 1., 1., 1., 1.])\n","n: [1. 1. 1. 1. 1.]\n"]}],"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"VJdFyesBtXRK"},"source":["テンソルの変更はNumPyの配列に反映される。"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ztwOVqptXRK","executionInfo":{"status":"ok","timestamp":1673022979830,"user_tz":-540,"elapsed":17,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"6356eb3f-4665-402e-e331-84dde8df5f15"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.])\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"vCGhCKAntXRK"},"source":["### NumPyからTensorへの変換\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"AV6-sIUStXRK","executionInfo":{"status":"ok","timestamp":1673022979830,"user_tz":-540,"elapsed":16,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}}},"outputs":[],"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"]},{"cell_type":"markdown","metadata":{"id":"bwdYCaPCtXRK"},"source":["NumPy配列の変更もテンソルに反映される。\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vw9klOKwtXRK","executionInfo":{"status":"ok","timestamp":1673022979831,"user_tz":-540,"elapsed":16,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"f6454882-1675-4cba-8ff0-d2c2c74d737d"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"3Cn9_QtPtXRL"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3QjKoCQYtXRL"},"source":["# Datasets & DataLoaders\n"]},{"cell_type":"markdown","metadata":{"id":"RdHnblCPtXRL"},"source":["データサンプルを処理するコードは煩雑になりやすく、メンテナンスも大変です。\n","読みやすさとモジュール化のために、学習のコードから切り離されることが理想です。  \n","\n","PyTorchは2つのデータプリミティブを提供しています。PyTorch には ``torch.utils.data.DataLoader`` と ``torch.utils.data.Dataset`` という2つのデータプリミティブがあり、あらかじめ用意されたデータセットや独自のデータセットを利用することができます。  \n","\n","``Dataset`` にはデータのサンプルとそれに対応するラベルが格納され、 ``DataLoader`` はサンプルに簡単にアクセスできるようにイテラブル（for文で1つずつ取り出せるように）にラップしています。  \n","\n","PyTorchのライブラリには、いくつかのデータセット（FashionMNISTなど）が用意されていて、モデルのプロトタイプやベンチマークテストに使用することができます。  \n","※画像データセットは[こちら](https://pytorch.org/vision/stable/datasets.html)、自然言語データセットは\n","[こちら](https://pytorch.org/text/stable/datasets.html)、そして音声のデータセットは\n","[こちら](https://pytorch.org/audio/stable/datasets.html)を参考にしてください。\n"]},{"cell_type":"markdown","metadata":{"id":"lF3KVWUztXRL"},"source":["## データセットの読み込み\n","\n","TorchVisionの[Fashion-MNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist/)データセットを読み込む例を見てみましょう。  \n","\n","Fashion-MNISTは、Zalandoの記事画像のデータセットで、60,000の学習データと10,000のテストデータから構成されています。  \n","データは、28×28のグレースケール画像で、10クラスのうちの1つのラベルが付与されています。\n","\n","[FashionMNIST Dataset](https://pytorch.org/vision/stable/datasets.html#fashion-mnist)を以下のパラメータでロードします。\n"," - ``root`` ：学習/テストデータが格納されているパス\n"," - ``train`` ：トレーニングデータセットまたはテストデータセットを指定\n"," - ``download`` ：`True`にするとルートディレクトリにデータがない場合にインターネットからデータをダウンロードしてくる\n"," - ``transform``:データの変換方法を指定\n"," - ``target_transform`` ：ラベルの変換方法を指定"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451,"referenced_widgets":["13581e40c1914133aef13b057a3256bd","39993552128a4cd6949450909713d6d3","ee01185f98d447bca59ef1d23340a98d","f89e54fa786f4af49baf05c849a1b64f","dcc42ea85a40476b91789b8e26b135d7","da49309ceca14704bcf9d23f5138dc97","4669f5ce463b4c3a9a4225ecb3faef20","11de498a53aa46118077cd8a96a0ef74","b981708677cf41eea9b894e474797e5e","2d2ffca374b9433d881e422169a51c24","b2dfdbd81f68477cbf8a7b380a58e8ea","84b9a0f94b944cc38f90ac5660b63069","93ab98191a1d494d8cb846d0605715e1","a9d7fa60e9134822a8ba183d9ad6bde2","18bab18cbc32455cbfc10ee0a9b525a3","3d6eeff1dd89419c9191235b2c8edb4c","684990054f1e47988d9e22598f56a763","d44ba775f8784816be384b354a77080f","a30f93ff436149f382d5f42918188a8f","a57cbb77aa6e4c3c8cddf4e10c5033b0","259599ae794c48d4a40b5cfa2fa1697d","4d8e35abda434e548b4f459f36354298","f6fa3f8a17f845b6be9ab3656209c155","1f11a3764bda4cb98c9663e1258c76da","f178b6d0c95f46e789d8e24378bd6061","853929942d314f62879a200c71c21fc0","070dfa9df9394fadbaa757a12b611bc8","b682567977f94e288a803d6d3ed0f5c2","0041915ec54349e093e58df7b417860f","909530b69474492db57c08d413dc7a3c","0edf8ba937b64795a3bf02a8df099a4c","ca2f32296e974f06bdd96a0af02d8e6b","ea94dc2db77e4770b4cb4a3325c561ef","fa298d8b6ed149fdb84b3e819d30c59e","826e6b91840444d095111badea34d972","f8f85b22a9c94ba18a1e5c898d993175","8367e4540d674a65ac75746f235662fa","55f9d5aad8d14a8f983e99edc623ff60","2ef4172dc3fa46419aaef85fbf2a54f1","60c6aac309cd44d1a00fadee5bc0797e","a348f3c9d3314f0d8f7356d4576efca7","c231dbf09bd944f296c4e55d36c0691f","2f893294129f43e8934d344dc2d8a665","737a35f8c3f7450780a44f6c0c23e27b"]},"id":"8NuqF2cotXRL","executionInfo":{"status":"ok","timestamp":1673022989013,"user_tz":-540,"elapsed":7326,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"d8149413-5c6f-4401-c4a5-c780103888ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/26421880 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13581e40c1914133aef13b057a3256bd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/29515 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b9a0f94b944cc38f90ac5660b63069"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4422102 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6fa3f8a17f845b6be9ab3656209c155"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5148 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa298d8b6ed149fdb84b3e819d30c59e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n"]}],"source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")"]},{"cell_type":"markdown","metadata":{"id":"kRRKupE0tXRM"},"source":["## データセットの反復処理と可視化\n","\n","データセット ``Datasets`` から``training_data[index]`` のようにして、通常のリスト配列のように手動でデータを取得することができます。  \n","ここでは、 ``matplotlib`` を使用して、学習データのサンプルを可視化してみます。\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":482},"id":"gOuJhMJEtXRM","executionInfo":{"status":"ok","timestamp":1673022990761,"user_tz":-540,"elapsed":546,"user":{"displayName":"Akihiro SUZUKI","userId":"10098966903588453974"}},"outputId":"8575d4ee-0ece-4808-92b5-dfb7efc1683d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 576x576 with 9 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    img, label = training_data[sample_idx]\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PH82zUBltXRM"},"source":["## カスタムデータセットの作成\n","\n","``Dataset``クラスを作成するためには、3つの関数を実装する必要があります。``__init__()``、``__len__()``、``__getitem__()`` の3つです。  \n","\n","FashionMNISTの例をみてみましょう。\n","ここでは、画像データが格納されたディレクトリのパスを``img_dir``として与え、ラベルデータが記されたCSVファイルのパスを ``annotations_file``として与えます。\n","\n","それぞれの関数の挙動について解説していきます。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-cZgmL_tXRM"},"outputs":[],"source":["import os\n","import pandas as pd\n","from torchvision.io import read_image\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"_tJ_S7GstXRN"},"source":["### `__init__`関数\n","\n","`__init__` 関数は、Dataset オブジェクトのインスタンスを作成する際に一度だけ実行されます。画像を格納しているディレクトリや教師データ、変換の方法（次のセクションで詳しく説明します）を初期化します。\n","\n","なお、FashionMNISTの教師データのlabels.csvファイルの中身は次のようになっています。\n","\n","\n","    tshirt1.jpg, 0\n","    tshirt2.jpg, 0\n","    ......\n","    ankleboot999.jpg, 9\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZJVxLk0tXRN"},"outputs":[],"source":["def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","    self.img_labels = pd.read_csv(annotations_file)\n","    self.img_dir = img_dir\n","    self.transform = transform\n","    self.target_transform = target_transform"]},{"cell_type":"markdown","metadata":{"id":"FXd3ILFvtXRN"},"source":["### ``__len__``関数\n","\n","``__len__`` 関数は、データセットのサンプル数を返します。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TW24wiFtXRN"},"outputs":[],"source":["def __len__(self):\n","    return len(self.img_labels)"]},{"cell_type":"markdown","metadata":{"id":"JDpH2QyctXRN"},"source":["### ``__getitem__``関数\n","\n","``__getitem__``関数は、データセットから指定されたインデックス ``idx`` にあるサンプルをロードして返します。  \n","\n","このインデックスをもとに画像を検索し、 ``read_image`` を用いてテンソルへの変換を行い、 ``self.img_labels`` の csv データから対応するラベルを取得します。  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ID-AbBFGtXRO"},"outputs":[],"source":["def __getitem__(self, idx):\n","    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","    image = read_image(img_path)\n","    label = self.img_labels.iloc[idx, 1]\n","    if self.transform:\n","        image = self.transform(image)\n","    if self.target_transform:\n","        label = self.target_transform(label)\n","    return image, label"]},{"cell_type":"markdown","metadata":{"id":"8RC0DHpVtXRO"},"source":["## `DataLoader` を使った学習用のデータの準備\n","\n","``Dataset`` クラスは、画像とラベルをセットとして一度に取得します。  \n","\n","さて、モデルの学習では通常、モデルの過学習（オーバーフィッティング）を減らすために、エポック毎にデータをシャッフルします。ここで、Pythonの ``multiprocessing`` を使用してデータの取得を高速化します。\n","\n","``DataLoader`` は上記の複雑な処理を簡単な API として抽象化したイテラブルです。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXW_Jry4tXRO"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"ccx6Ujo-tXRO"},"source":["## ``DataLoader`` のイテレート処理（繰り返し処理）\n","\n","前述の処理でデータセットを ``DataLoader`` にロードしたので、必要に応じてデータセットを繰り返し処理することができます。\n","\n","下記の反復処理では、 ``train_features`` と ``train_labels`` (``batch_size=64``なのでそれぞれ64個ずつの画像とラベルのセット) のバッチが返されます。\n","\n","また、``shuffle=True`` を指定したため、すべてのバッチを繰り返し処理した後に（＝1エポック後に）、データの順番がシャッフルされます（データのロード順序をより細かく制御する場合は、[Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler) を参照してください)。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2wvWdeAxtXRO"},"outputs":[],"source":["# Display image and label.\n","# 繰り返し実行\n","train_features, train_labels = next(iter(train_dataloader))\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_features[0].squeeze()\n","label = train_labels[0]\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"]},{"cell_type":"markdown","metadata":{"id":"W6Yd-KPstXRO"},"source":["# Transforms\n","機械学習アルゴリズムの学習に必要なデータは、必ずしも学習可能な状態へと加工されているわけではありません。\n","\n","そこで`transforms`を使用して、データに何らかの処理を施し、学習可能な状態にします。\n","\n","すべてのTorchVisionデータセットには2つのパラメータがあります。画像を処理するための``transform`` と\n","ラベルを処理するための ``target_transform``です。\n","[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)モジュールによく使われる変換方法が実装されています。\n","\n","さて、FashionMNISTの画像はPIL形式で、ラベルは整数値です。\n","学習させるためには、画像の数値を正規化してテンソル化して、ラベルをone-hotエンコードしてテンソル化する必要があります。\n","これらの変換を行うために、 ``ToTensor`` と ``Lambda`` を使います。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkT7uOG8tXRP"},"outputs":[],"source":["import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda\n","\n","ds = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(),\n","    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",")"]},{"cell_type":"markdown","metadata":{"id":"i3C0iGQytXRP"},"source":["## `ToTensor()`\n","\n","[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n","はPIL形式または NumPy の ``ndarray`` を ``FloatTensor`` に変換し、画像のピクセル値を [0., 1.] の範囲に正規化します。\n","\n","## Lambda Transforms\n","\n","ラムダ変換は、ユーザーが定義した任意のラムダ関数を適用します。  \n","ここでは、整数をone-hotエンコードされたテンソルに変換する関数\n","を定義します。\n","まず、サイズ10（データセットのラベル数と同じ数）で要素が全てゼロのテンソルを生成し[scatter_](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html)を呼び出します。\n","`scatter_`では ``y`` で与えられるインデックスに ``value=1`` を代入します。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tWVjht0tXRP"},"outputs":[],"source":["target_transform = Lambda(lambda y: torch.zeros(\n","    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"]},{"cell_type":"markdown","metadata":{"id":"8G3D-YzHtXRP"},"source":["# ニューラルネットワークの構築\n","\n","ニューラルネットワークはデータに対して演算を行う層/モジュールで構成されています。\n","[torch.nn](https://pytorch.org/docs/stable/nn.html)に独自のニューラルネットワークを構築するために必要な要素が実装されています。\n","\n","また、PyTorchのすべてのモジュールは[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)をサブクラスとしています。\n","\n","他のモジュールや層を使ってニューラルネットをモジュールとして構築することができます。\n","ニューラルネットをこのようにモジュールや層の入れ子構造で実装することで、複雑なアーキテクチャの構築や管理を容易にすることができます。\n","\n","ここでは、FashionMNISTデータセットの画像を分類するためのニューラルネットワークを構築してみます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LjV_fMWtXRQ"},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","metadata":{"id":"29LpZwP6tXRQ"},"source":["## `device`の確認\n","\n","できればGPUのようなハードウェアアクセラレータを使ってモデルの学習を高速化したい．．．  \n","そんな時は[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html)が利用可能かどうか確認してみましょう。\n","`torch.cuda.is_available()`で利用可能か確認することができます。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR1l1IRytXRQ","outputId":"c84d8d23-0a3b-4d33-eb49-97011c4cbee2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"hqNMHzX3tXRQ"},"source":["## クラスの定義\n","ニューラルネットワークを ``nn.Module`` のサブクラスとして定義します。 ``__init__``関数 でニューラルネットワークのレイヤーを初期化します。\n","また、入力データは``forward``関数 で処理します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBO_AQc_tXRQ"},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"R9HNzgHRtXRQ"},"source":["``NeuralNetwork`` のインスタンスを生成して ``device`` に転送し、その構造を表示させてみます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-ab_ya_tXRU","outputId":"d4d4134e-f7af-45be-9f9b-8545ab23b4bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["model = NeuralNetwork().to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"Q8l7AAHUtXRU"},"source":["モデルに入力データを渡すと ``forward``関数といくつかの [バックグラウンド処理](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866) が呼び出されます。  \n","※ただし、``model.forward()`` のようにして直接`forward`関数を呼び出さないでください。\n","\n","さて、`forward`関数を呼び出すと、各クラスの10個の予測値に対する生の出力がそれぞれの入力ごとに返されます。\n","このとき、(予測値、データインデックス)のようなデータ形式になっています。\n","\n","この出力をさらに ``nn.Softmax`` モジュールのインスタンスに渡すして、ソフトマックス演算を行なって予測確率としています。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEpdwB2ItXRU","outputId":"50e002dd-42b8-4528-df66-3599b4c9cdb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted class: tensor([3])\n"]}],"source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"]},{"cell_type":"markdown","metadata":{"id":"SOY-6nvmtXRV"},"source":["## モデルのレイヤー\n","\n","FashionMNISTモデルのレイヤーを分解してみましょう。\n","\n","縦横28x28のノイズ画像を3つ用意して、それをネットワークに通すとどうなるかをみてみます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sABbg4KLtXRV","outputId":"16a66469-d6da-48e2-d507-40c9b6eb798a"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 28, 28])\n"]}],"source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"]},{"cell_type":"markdown","metadata":{"id":"dIxP0zB8tXRV"},"source":["### `nn.Flatten`\n","[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)のオブジェクトを宣言します。\n","これを使って28x28の二次元のノイズ画像を784の一次元の配列に変換します。ここで、ミニバッチの次元(dim=0)は維持されます。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCAemJyLtXRV","outputId":"06901298-c7f8-4a9d-d2d4-72e417a5bb7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 784])\n"]}],"source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"]},{"cell_type":"markdown","metadata":{"id":"HJ1zmOtztXRV"},"source":["### `nn.Linear`\n","[linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","は、保持している重みとバイアスを使用して入力されたデータに線形変換を適用するモジュールです。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05lVa9tOtXRV","outputId":"f10c435b-e451-4991-f4ea-cc2a234e0f4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 20])\n"]}],"source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"]},{"cell_type":"markdown","metadata":{"id":"DtHDR4s1tXRW"},"source":["### `nn.ReLU`\n","活性化関数による非線形変換は、入力を複雑に変換させるためのものです。\n","線形変換の後に適用することで、非線形性を導入し、ニューラルネットワークが様々な現象を学習できるようにします。\n","\n","ここでは例として[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)を使用しますが、これ以外にも他に活性化関数はあります。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGjSBMB8tXRW"},"outputs":[],"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"]},{"cell_type":"markdown","metadata":{"id":"KhjZkvKGtXRW"},"source":["### `nn.Sequential`\n","[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)は、順番に並べられた\n","モジュールのコンテナです。ここで定義したモジュールを使って、ここで定義されたとおりの順序でデータを処理していきます。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BrAkwNktXRW"},"outputs":[],"source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"]},{"cell_type":"markdown","metadata":{"id":"Dd-CBHhitXRW"},"source":["### `nn.Softmax`\n","ニューラルネットワークの最後の線形層は[-infty, \\infty]の実数になっています。\n","ここでは出力を保持している`logits`を[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) モジュールに渡します。ソフトマックス関数は与えられた行列の全ての値を[0,1]に、その合計を1にします。よって、ソフトマックス関数の出力を受け取った`pred_probab`は[0, 1]の値を持ち、それは各クラスに対するモデルの予測確率を表します。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rb11lPiJtXRW"},"outputs":[],"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"]},{"cell_type":"markdown","metadata":{"id":"Gq7uNHcPtXRW"},"source":["## モデルパラメータ\n","ニューラルネットワークの多くの層はパラメータ化されており、その重みとバイアスは学習によって最適化されていきます。  \n","\n","``nn.Module`` のサブクラスは自動的にモデルオブジェクト内で定義されたすべての層を追跡し、すべてのパラメータにアクセスできる状態になります。\n","また、モデルの ``parameters()`` や ``named_parameters()`` メソッドを用いて、すべてのパラメータにアクセスすることもできます。\n","\n","ここでは、各パラメータをイテレートし、そのサイズと値を表示してみます。\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QFEJ8yKtXRW"},"outputs":[],"source":["print(f\"Model structure: {model}\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"4tiaxotLtXRX"},"source":["# ``torch.autograd``を使った自動微分\n","\n","ニューラルネットワークを学習させる際に、最もよく使われるアルゴリズムが**誤差逆伝播法**です。このアルゴリズムでは、パラメータ（モデルの重み）が損失関数の**勾配**にしたがって調整されます。\n","\n","この勾配を計算するために、PyTorchは ``torch.authime`` という微分エンジンを内蔵しています。\n","これは、任意の計算グラフに対して勾配の自動計算を行うものです。\n","\n","例として、パラメータ ``w`` と ``b`` 、そして何らかの損失関数を持つ、最も単純な 1 層のニューラルネットワークを考えてみましょう。これはPyTorch では次のように定義されます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2M5pXPatXRX"},"outputs":[],"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"]},{"cell_type":"markdown","metadata":{"id":"XzeTStjAtXRX"},"source":["## Tensors, Functions and Computational graph\n","\n","このコードは、次のような**計算グラフ**を定義しています。\n","\n","![図](https://pytorch.org/tutorials/_images/comp-graph.png)\n","\n","このネットワークでは、``w`` と ``b`` が **パラメータ** であり、これを最適化する必要があります。\n","したがって、これらの変数に関する損失関数の勾配を計算できる必要があります。\n","そこで、テンソルの`requires_grad`プロパティを適用します。\n"]},{"cell_type":"markdown","metadata":{"id":"ig5BNlRptXRX"},"source":["``requires_grad`` の値は、テンソルを作成するときに設定するか、後で ``x.requires_grad_(True)`` のようにしてメソッドを使用して設定することができます。"]},{"cell_type":"markdown","metadata":{"id":"xyUf-RVUtXRX"},"source":["テンソルに適用して計算グラフを構築する関数は、実際には ``Function`` クラスのオブジェクトです。\n","\n","このオブジェクトは、順伝播の関数の計算方法と、誤差逆伝播時の微分の計算方法を保持しています。誤差逆伝播法の関数の情報は、テンソルの ``grad_fn`` プロパティに格納されています。 ``Function`` 自体の詳細については、[ドキュメント](https://pytorch.org/docs/stable/autograd.html#function)を参照してください。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6Xi21mWtXRX","outputId":"68b35084-ce5d-4f4c-86a1-4b8b0abd1fbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradient function for z = <AddBackward0 object at 0x17ed279d0>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x17ed27e80>\n"]}],"source":["print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")"]},{"cell_type":"markdown","metadata":{"id":"cxDE-h4atXRY"},"source":["## 勾配の計算\n","\n","ニューラルネットワークのパラメータの重みを最適化するためには、損失関数のパラメータに関する導関数を計算する必要があります。つまり、任意の変数 ``x``と``y``のに対して、$\\frac{\\partial loss}{\\partial w}$ と $\\frac{\\partial loss}{\\partial b}$ が必要です。\n","これらの導関数を計算するために、 ``loss.backward()`` を呼び出し、 ``w.grad`` と ``b.grad`` から値を取得する必要があります。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKRRFdTqtXRY","outputId":"1dcece83-dcd2-4f2e-8cb9-151471b39d1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.2489, 0.3117, 0.3249],\n","        [0.2489, 0.3117, 0.3249],\n","        [0.2489, 0.3117, 0.3249],\n","        [0.2489, 0.3117, 0.3249],\n","        [0.2489, 0.3117, 0.3249]])\n","tensor([0.2489, 0.3117, 0.3249])\n"]}],"source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"markdown","metadata":{"id":"qrkQHAWatXRY"},"source":["## 勾配計算の追跡カット\n","\n","デフォルトでは、 ``requires_grad=True`` のテンソルはすべて計算履歴を追跡し、勾配計算をサポートします。  \n","\n","しかし、その必要がない場合もあります。  \n","例えば、学習後のモデルにデータを入力したいだけの場合、つまり、ニューラルネットワークを通して順伝播計算だけを行いたいという場合である。\n","この場合は、計算コードを ``torch.no_grad()`` ブロックで囲むことで、トラッキング計算を停止させることができます。\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pu2C8iTLtXRY","outputId":"1763a089-789a-496a-9775-8a2da5617210"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","False\n"]}],"source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"hNnC6ko2tXRY"},"source":["別の方法として、テンソルに対して ``detach()`` メソッドを使用すること方法もあります。\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyhICrWotXRY","outputId":"69dead80-1b4d-4766-ca84-6d37e35691b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["z = torch.matmul(x, w)+b\n","z_det = z.detach()\n","print(z_det.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"pOIRWZ9mtXRZ"},"source":["勾配のトラッキングを無効にした方がいい理由はこの通りです。\n","  - ニューラルネットワークのいくつかのパラメータを固定するため。これは[再学習](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)のために必須な処理です\n","  - 勾配を追跡しないテンソルの計算の方が効率が良いことを利用して、順伝播だけを行う場合は高速化させるため\n"]},{"cell_type":"markdown","metadata":{"id":"X0UKvagNtXRZ"},"source":["## 計算グラフについて\n","概念的には、autogradはデータ(テンソル)と実行されたすべての操作(結果の新しいテンソルも含む)の記録を\n","[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) オブジェクトからなる有向非循環グラフ (Directed Acyclic Graph, DAG) に記録します。\n","\n","この DAG において、葉は入力テンソル、根は出力テンソルに該当します。そして、このグラフを根から葉までたどれば、連鎖律を使って自動的に勾配を計算することができます。\n","\n","順伝播計算において、autogradは同時に2つのことを行います。\n","\n","- 演算を実行し、結果のテンソルを計算する\n","- DAGの中で操作の勾配関数を維持する\n","\n","誤差逆伝播は ``.backward()`` がDAGの根本から呼び出された時に計算されます。その後で ``autograd`` が呼び出されます。\n","\n","- 各 ``.grad_fn`` から勾配を計算する\n","- 各テンソルの ``.grad`` 属性に勾配計算の結果を蓄積する\n","- 連鎖律を使って葉となるテンソルまで伝播させる\n"]},{"cell_type":"markdown","metadata":{"id":"6S3Z55BdtXRZ"},"source":["## おまけ: テンソル勾配とヤコビアン積\n","\n","多くの場合，スカラーで計算する損失関数があり，あるパラメータに対する勾配をそれぞれ別個で計算する必要があります．\n","しかし、出力関数が任意のテンソルになっている場合があります。このような場合、PyTorch では、実際の勾配ではなく、いわゆる **ヤコビアン積** を計算することができます。\n","\n","$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ と $\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$とする、ベクトル関数 $\\vec{y}=f(\\vec{x})$ の $\\vec{x}$ に対する $\\vec{y}$ の勾配は **ヤコビアン行列**で与えられます。\n","\n","\\begin{align}J=\\left(\\begin{array}{ccc}\n","      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","      \\vdots & \\ddots & \\vdots\\\\\n","      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","      \\end{array}\\right)\\end{align}\n","\n","\n","PyTorchでは、ヤコビアン行列そのものを計算する代わりに、与えられた入力ベクトル $v=(v_1 \\dots v_m)$ に対する **ヤコビアン積** $v^T\\cdot J$ を計算することが可能です。  \n","これは、$v$を引数として ``backward`` を呼び出すことで実現されます。 なお、$v$ のサイズは、積を計算したい元のテンソルのサイズと同じでなければなりません。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrEvOuh9tXRZ"},"outputs":[],"source":["inp = torch.eye(4, 5, requires_grad=True)\n","out = (inp+1).pow(2).t()\n","out.backward(torch.ones_like(out), retain_graph=True)\n","print(f\"First call\\n{inp.grad}\")\n","out.backward(torch.ones_like(out), retain_graph=True)\n","print(f\"\\nSecond call\\n{inp.grad}\")\n","inp.grad.zero_()\n","out.backward(torch.ones_like(out), retain_graph=True)\n","print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"]},{"cell_type":"markdown","metadata":{"id":"S06S6sOwtXRZ"},"source":["ここで、同じ引数で2回目の ``backward`` を呼び出したとき、勾配の値が異なることに注意してください。  \n","これは、 ``backward`` 伝搬を行う際に、PyTorch が **勾配を累積する** ためです。  \n","つまり、計算された勾配の値は、計算グラフのすべての葉となるノードの ``grad`` に追加されることになります。\n","適切な勾配を計算したい場合は、事前に ``grad`` をゼロにする必要があります。\n","実際の学習では、`optimizer` を使って処理していきます。"]},{"cell_type":"markdown","metadata":{"id":"TB1EghyFtXRZ"},"source":["以前は ``backward()`` 関数をパラメータなしで呼び出していました。\n","これは本質的には ``backward(torch.tensor(1.0))`` を呼び出すことと同じで、ニューラルネットワークの学習における損失のようにスカラー値を持つ関数を使う場合に、勾配計算を簡単に行うことができます。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TFEeQXH9tXRa"},"source":["# モデルパラメータの最適化\n","\n","モデルとデータが揃ったので、次はデータに対してモデルのパラメータを最適化することで、モデルを学習、検証、テストしていきます。\n","\n","モデルの学習は反復プロセスです。  \n","各反復において、モデルは出力値を推測し、その推測と教師データとの誤差（*損失*）を計算します。  \n","前述の通り、その誤差のパラメータに対する導関数を蓄積しておき、勾配降下法によってこれらのパラメータを**最適化**します。\n","\n","[Datasets & DataLoaders](https://colab.research.google.com/drive/196rfEo-N7CzNRjmVIWf-SYzHvEw9B_Ao#scrollTo=3QjKoCQYtXRL)、[ニューラルネットワークの構築](https://colab.research.google.com/drive/196rfEo-N7CzNRjmVIWf-SYzHvEw9B_Ao#scrollTo=8G3D-YzHtXRP)で使ったコードを再利用します。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"acU5wLT9tXRa"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"]},{"cell_type":"markdown","metadata":{"id":"0_20ecGqtXRa"},"source":["## ハイパーパラメータ\n","\n","ハイパーパラメータは、モデルの最適化プロセスを制御するための調整可能なパラメーターです。ハイパーパラメータの値を変えることで、モデルの学習や収束率を制御することができます（ハイパーパラメータのチューニングについては[こちら](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)をご覧ください）。\n","\n","ここでは、学習用のハイパーパラメータを以下のように定義します。\n"," - **エポック数**： データセットに対して反復処理を行う回数\n"," - **バッチサイズ**： パラメータを更新するために一度に渡されるデータサンプルの個数\n"," - **学習率**： - 各バッチ/エポックにおいて、モデルのパラメータをどれだけ更新するかの比率。小さい値を設定すると学習速度が遅くなり、大きい値を設定すると学習時に予測不可能な挙動をする可能性がある\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxw-Yu6NtXRa"},"outputs":[],"source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"]},{"cell_type":"markdown","metadata":{"id":"DT8Q5vvYtXRa"},"source":["## 最適化ループ\n","\n","ハイパーパラメータを設定したら、ループ処理でモデルを学習し最適化することができます。ループ処理の各反復は、**エポック**と呼ばれます。\n","\n","エポックは主に2つの部分から構成されています。\n"," - **学習ループ**： 学習データセットを繰り返し、最適なパラメータに収束させる\n"," - **検証/テストループ**： テストデータセットを繰り返し処理し、モデルの性能が向上しているかどうかを確認する\n","\n","学習ループで使われるいくつかの用語について簡単に説明します。\n","\n","### 損失関数\n","\n","あるデータを入力したとき、学習前のニューラルネットワークは答えが間違っている可能性が高いです。  \n","**損失関数**は、得られた結果と目標値との違いを測定するもので、学習を繰り返して最小化を目指します。  \n","損失を計算するために、与えられたデータサンプルの入力を使って予測を行い、真のデータ・ラベルの値と比較します。\n","\n","一般的な損失関数は回帰タスクの[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error)、分類の[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood)などです。\n","[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) は ``nn.LogSoftmax`` と ``nn.NLLoss`` を結合したものです。\n","\n","モデルの出力を ``nn.CrossEntropyLoss`` に渡すと、値が正規化されて予測誤差が計算されます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJHB2V85tXRa"},"outputs":[],"source":["# 損失関数の初期化\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"j6hJ7ExZtXRb"},"source":["### 最適化器\n","\n","最適化とは、各学習ステップにおいて、モデルの誤差を減らすためにモデルのパラメータを調整する処理です。  \n","**最適化アルゴリズム** は、このプロセスの実行方法を定義します（この例では、確率的勾配降下法, Stochastic Gradient Descent, SGD を使用します）。  \n","最適化ロジックは ``optimizer`` オブジェクトにカプセル化されています。  \n","今回はSGDを使用しますが、PyTorchにはADAMやRMSPropなど多くの[異なる最適化器](https://pytorch.org/docs/stable/optim.html)があります。\n","\n","学習が必要なモデルのパラメータを登録し、学習率ハイパーパラメータを渡すことで最適化器を初期化します。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBi9l7s_tXRb"},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"_mWWPFFEtXRb"},"source":["学習ループの内部では、3つのステップで最適化が行われます。\n","- モデルパラメータの勾配をリセットするために ``optimizer.zero_grad()`` を呼び出します。勾配は加算されていくので、二重計算を防ぐために各反復で明示的にゼロにする必要があります\n","- 予測損失は ``loss.backward()`` を呼び出すと誤差逆伝播します。また、PyTorchは、各パラメータに対する損失の勾配を記録しています\n","- 勾配が得られたら、 ``optimizer.step()`` を呼び出して、誤差逆伝播で収集した勾配を元にパラメータを調整します\n"]},{"cell_type":"markdown","metadata":{"id":"MMOPv_wTtXRb"},"source":["\n","## 最適化の実装の全体像\n","最適化コードをループする ``train_loop`` と、テストデータに対してモデルの性能を評価する ``test_loop`` を定義します。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAceAgXItXRb"},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Compute prediction and loss\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","\n","def test_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"u2sbXUlYtXRb"},"source":["損失関数と最適化器を宣言し、 ``train_loop`` と ``test_loop`` に渡します。  \n","ここで、エポック数を増やしてみましょう。  \n","モデルの性能が向上していることを自由に確認することをできます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsORYDpltXRb"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"lgL2nmv8tXRc"},"source":["## 損失関数\n","- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n"]},{"cell_type":"markdown","source":["### MSELoss\n","\n","\n"],"metadata":{"id":"HsfZEGBMcflV"}},{"cell_type":"markdown","source":["入力$x$とターゲット$y$の各要素間の平均二乗誤差（L2ノルムの二乗）を測定する基準を作成します。\n","\n","次元を維持する（reduction='none'）場合は、以下のように記述することができます。ここで $N$ はバッチサイズです。\n","\n","$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2$ \n","\n","reductionを'none' ではない場合(デフォルトは'mean')は以下のように記述できます。\n","\n","$\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}$\n"," \n","$x$と$y$はそれぞれ合計$n$個の要素を持つ任意の形のテンソルです。\n","\n","平均演算はやはり全要素に対して行われ、$n$で除算されます。\n","\n","reduction = 'sum' とすれば，$n$による除算を避けることができます。\n","\n"],"metadata":{"id":"rNjJW6CMctK_"}},{"cell_type":"markdown","source":["### NLLLoss\n","\n"],"metadata":{"id":"wLwcVWcXcjdO"}},{"cell_type":"markdown","source":["負の対数尤度損失（Negative Log Likelihood Loss）。C個のクラスからなる分類問題を学習するのに有用です。\n","\n","オプションの引数 weight は、各クラスに重みを割り当てる 1次元テンソルでなければなりません。  \n","これは，アンバランスな学習セットを持っている場合に特に有用です。\n","\n","inputは、( $minibatch, C$ )または( $minibatch, C, d_1, d_2, ..., d_K$ )のいずれかのサイズのTensorでなければならず、K次元の場合は $K \\geq 1$ でなければなりません。  \n","後者は、2次元画像の画素あたりのNLLLOSSを計算するような高次元の入力に有効です。\n","\n","ニューラルネットワークで対数確率を得るには、ネットワークの最後の層にLogSoftmax層を追加することで簡単に実現できます。  \n","余分なレイヤーを追加したくない場合は、代わりにCrossEntropyLossを使用することができます。 \n","\n","この損失が期待するターゲットは、$[0, C-1]$ (C = クラス数) の範囲にあるクラスインデックスでなければなりません。\n","\n","また、ignore_indexが指定された場合、この損失はこのクラスインデックスを受け付けます (このインデックスは必ずしもクラスの範囲内でなくても構いません)。\n","\n","$x$がinput, $y$がtarget, $w$がweight, $N$はバッチサイズ.\n","\n","$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$,\n","\n","reductionを'none' ではない場合(デフォルトは'mean')は以下のように記述できます。\n","\n","$\\ell(x, y) = \\begin{cases} \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, & \\text{if reduction} = \\text{`mean';}\\\\ \\sum_{n=1}^N l_n, & \\text{if reduction} = \\text{`sum'.} \\end{cases}$\n"],"metadata":{"id":"kIc8r7C5czc0"}},{"cell_type":"markdown","source":["### CrossEntropyLoss\n","\n"],"metadata":{"id":"C7u1XeU5cjnN"}},{"cell_type":"markdown","source":["入力ロジットとターゲット間のクロスエントロピーの損失を計算します。\n","\n","これは、C個のクラスを持つ分類問題を学習するときに有用です。  \n","オプションの引数weightは、各クラスに重みを割り当てる1次元テンソルでなければなりません。  \n","これは、アンバランスな学習セットを持っている場合に特に有用です．\n","\n","入力は、各クラスの非正規化ロジット（一般に、正であったり、和が1になる必要はない）を含むことが期待されます。  \n","inputは、バッチされていないinputではサイズ ($C$)、K次元の場合には$Kを1$とした ($minibatch, C$) または ($minibatch, C, d_1, d_2, ..., d_K$) のTensorでなければなりません。  \n","また、2次元画像のピクセル毎のクロスエントロピー損失を計算するような高次元の入力に有用です。\n","\n","以下のいずれかを含むことが想定されています。\n","\n","- $[0, C)$ ($C$ はクラス数); ignore_index が指定された場合、この損失はこのクラスインデックスも受け入れます（このインデックスは必ずしもクラス範囲内にあるとは限りません）。この場合の未削減(つまり削減を'なし'に設定した場合)の損失は次のように記述できます。\n","\n","  ここで$x$は入力、$y$はターゲット、$w$は重み、$C$はクラス数、$N$はミニバッチ次元と$K$次元の場合の$d_1, ..., d_{kd}$にまたがるものである。\n","    \n","  $\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}$\n","\n","  reductionが'none'でない場合(デフォルトは'mean')は以下のように記述できる。\n","\n","  $\\ell(x, y) = \\begin{cases} \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, & \\text{if reduction} = \\text{`mean';}\\\\ \\sum_{n=1}^N l_n, & \\text{if reduction} = \\text{`sum'.} \\end{cases} $\n"," \n","  この場合、LogSoftmaxとNLLLossの組み合わせと同等であることに注意してください。\n","\n","- 各クラスの確率；混合ラベル、ラベルスムージングなど、ミニバッチアイテムごとに単一クラス以上のラベルが必要な場合に有用です。この場合の未削減（つまり、削減を「なし」に設定した場合）の損失は、次のように記述できます。\n","\n","  ここで、$x$は入力、$y$はターゲット、$w$は重み、$C$はクラス数、$N$はミニバッチ次元とK次元の場合の$d_1, ..., d_kd$にまたがります。\n","\n","  $\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}$​\n","\n","  reductionが'none'でない場合(デフォルトは'mean')は以下のように記述できます。\n","\n","  $\\ell(x, y) = \\begin{cases} \\frac{\\sum_{n=1}^N l_n}{N}, & \\text{if reduction} = \\text{`mean';}\\\\ \\sum_{n=1}^N l_n, & \\text{if reduction} = \\text{`sum'.} \\end{cases}$"],"metadata":{"id":"GH1Uo3HvdIeQ"}},{"cell_type":"markdown","metadata":{"id":"fvL3v7D0tXRc"},"source":["# torchtext ライブラリを使ったテキスト分類\n","\n","ここでは、テキスト分類のためのデータセットを構築するために torchtext ライブラリを使用する方法を紹介します。  \n","ユーザは以下のような柔軟性を持つことができます。\n","\n","   - イテレータとして生データにアクセスする\n","   - データ処理パイプラインを構築して、生のテキスト文字列を ``torch.Tensor`` に変換し、モデルの学習に使用する\n","   - [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) を使ってデータをシャッフルし、反復処理する\n"]},{"cell_type":"markdown","metadata":{"id":"Ufig2_OttXRc"},"source":["## 生のデータセットイテレータへのアクセス\n","\n","torchtext ライブラリには、生のテキスト文字列を返す生のデータセットイテレータがいくつか用意されています。  \n","例えば、 ``AG_NEWS`` データセットイテレータは、ラベルとテキストのタプルとして生データを得ることができます。  \n","\n","torchtextのデータセットにアクセスするには、https://github.com/pytorch/data の説明に従ってtorchdataをインストールしてください。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xs5Kcc-ttXRc"},"outputs":[],"source":["import torch\n","from torchtext.datasets import AG_NEWS\n","train_iter = iter(AG_NEWS(split='train'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iN_SL9j-tXRc","outputId":"acf48928-573a-42ff-a69a-9b104da27b94"},"outputs":[{"data":{"text/plain":["(3,\n"," \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["next(train_iter)"]},{"cell_type":"markdown","metadata":{"id":"lvFWGNvDtXRd"},"source":["## データ処理パイプラインの用意\n","\n","ボキャブラリー、ワードベクター、トークナイザーなど、torchtextライブラリの非常に基本的な構成要素について再確認しました。  \n","これらは生のテキスト文字列に対する基本的なデータ処理のビルディングブロックです。\n","\n","ここでは、トークナイザーとボキャブラリーを使った典型的な自然言語処理例を紹介します。  \n","まず、生の学習データセットから語彙を作る。ここでは、ビルトインの\n","ファクトリー関数 `build_vocab_from_iterator` を使う。この関数では、トークンのリストかイテレータを受け取ります。  \n","また、語彙に追加する特殊なシンボルを渡すこともできます。\n","語彙に追加する特別な記号を渡すこともできます。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVSoIuNAtXRd"},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","tokenizer = get_tokenizer('basic_english')\n","train_iter = AG_NEWS(split='train')\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"]},{"cell_type":"markdown","metadata":{"id":"sHO9DRvLtXRd"},"source":["vocabularyブロックは，トークンのリストを整数に変換できます。\n","\n","    vocab(['here', 'is', 'an', 'example'])\n","    >>> [475, 21, 30, 5297]\n","\n","トークナイザーとボキャブラリーでテキスト処理パイプラインを準備します。  \n","テキストパイプラインとラベルパイプラインは、データセットイテレータからの生データ文字列を処理するために使用します。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVeajuZUtXRd"},"outputs":[],"source":["text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"]},{"cell_type":"markdown","metadata":{"id":"1Jl2OjvctXRd"},"source":["テキストパイプラインは，語彙に定義されたルックアップテーブルに基づいて，文字列を整数のリストに変換します。  \n","ラベルパイプラインは、ラベルを整数に変換します。  \n","例えば\n","\n","    text_pipeline('here is the an example')\n","    >>> [475, 21, 2, 30, 5297]\n","    label_pipeline('10')\n","    >>> 9\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YvtSskXLtXRd"},"source":["## データバッチとイテレータを生成する\n","\n","データローダー（DataLoader）は ``getitem()`` と ``len()`` プロトコルを実装したマップ形式のデータセットで動作し、インデックス/キーからデータサンプルへのマップを表現します。また、引数として ``False`` を指定した、反復処理可能なデータセットでも動作します。\n","\n","モデルに送信する前に、 ``collate_fn`` 関数は ``DataLoader`` から生成されたサンプルのバッチを処理する。``collate_fn`` への入力は ``DataLoader`` にあるバッチサイズのデータであり、 ``collate_fn`` は先に宣言したデータ処理パイプラインに従ってそれらを処理する。ここで注意してほしいのは、 ``collate_fn`` がトップレベルの def として宣言されていることだ。 これにより、この関数が各ワーカーで利用できるようになる。\n","\n","この例では、元のデータバッチの入力に含まれるテキストエントリーをリストにパックし、一つのテンソルとして連結して ``nn.EmbeddingBag`` の入力とします。offsetはテキストテンソル中の個々のシーケンスの開始インデックスを表すデリミタのテンソルである。ラベルは個々のテキストエントリのラベルを保存したテンソルである。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiwx_hbJtXRd"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for (_label, _text) in batch:\n","         label_list.append(label_pipeline(_label))\n","         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","         text_list.append(processed_text)\n","         offsets.append(processed_text.size(0))\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return label_list.to(device), text_list.to(device), offsets.to(device)\n","\n","train_iter = AG_NEWS(split='train')\n","dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"]},{"cell_type":"markdown","metadata":{"id":"iHHSyDnQtXRd"},"source":["## モデルの定義\n","\n","モデルは [nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag)レイヤーと、分類を目的とした線形レイヤーから構成されます。  \n","``nn.EmbeddingBag``はデフォルトのモードが \"mean \"で、埋め込みの \"bag \"の平均値を計算します。  \n","ここでは、テキストのエントリは異なる長さを持っていますが、テキストの長さはオフセットで保存されているので、nn.EmbeddingBagモジュールはここでパディングを必要としません。  \n","さらに、``nn.EmbeddingBag`` は、一連の処理を行う際のパフォーマンスとメモリ効率を向上させることができます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iD-yo6iFtXRe"},"outputs":[],"source":["from torch import nn\n","\n","class TextClassificationModel(nn.Module):\n","\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super(TextClassificationModel, self).__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)"]},{"cell_type":"markdown","metadata":{"id":"chDdK3e5tXRe"},"source":["## インスタンスの起動\n","\n","``AG_NEWS`` のデータセットには4つのラベルがあるので、クラスの数は4です。\n","\n","   1 : 世界\n","   2 : スポーツ\n","   3 : ビジネス\n","   4 : 科学/技術\n","\n","埋め込み次元を64としたモデルを構築します。  \n","vocab sizeはvocabulary instanceの長さと等しく、クラス数はラベルの数に等しくなります。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4WReX2ttXRe"},"outputs":[],"source":["train_iter = AG_NEWS(split='train')\n","num_class = len(set([label for (label, text) in train_iter]))\n","vocab_size = len(vocab)\n","emsize = 64\n","model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"]},{"cell_type":"markdown","metadata":{"id":"07B6uDXltXRe"},"source":["## モデルの学習と結果の評価のための関数の定義\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OE-LdYxtXRe"},"outputs":[],"source":["import time\n","\n","def train(dataloader):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 500\n","    start_time = time.time()\n","\n","    for idx, (label, text, offsets) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        predicted_label = model(text, offsets)\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        optimizer.step()\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches '\n","                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n","                                              total_acc/total_count))\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(dataloader):\n","            predicted_label = model(text, offsets)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc/total_count"]},{"cell_type":"markdown","metadata":{"id":"y44PbtWztXRe"},"source":["## データセットの分割とモデルの実行\n","\n","元の ``AG_NEWS`` には検証用データセットがないため、学習用データセットを分割します。  \n","データセットを訓練用と検証用に分割し、分割比率を 0.95 (train) と0.05 (valid) とします。  \n","ここで使用するのは\n","[torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)というPyTorchのコアライブラリにある関数を使います。\n","\n","[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)を使用します。  \n","この基準は、 ``nn.LogSoftmax()`` と ``nn.NLLLoss()`` を1つのクラスに統合したものです。  \n","これは，C個のクラスからなる分類問題を学習するときに役立ちます。  \n","最適化器には[SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)を使います。初期学習率は 5.0 に設定します。  \n","[ステップLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)は、エポックを通して学習率を調整するために使います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_x-l6XRtXRf"},"outputs":[],"source":["from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","# Hyperparameters\n","EPOCHS = 10 # epoch\n","LR = 5  # learning rate\n","BATCH_SIZE = 64 # batch size for training\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","train_iter, test_iter = AG_NEWS()\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = \\\n","    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n","\n","train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                             shuffle=True, collate_fn=collate_batch)\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"]},{"cell_type":"markdown","metadata":{"id":"ZhbyfpwttXRf"},"source":["## テストデータセットでモデルを評価\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0h7fvJhEtXRf"},"source":["テストデータセットの結果を確認します"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5M6DpzxtXRf","outputId":"6315869c-ee13-42cd-af50-9f0ebab27c99"},"outputs":[{"name":"stdout","output_type":"stream","text":["Checking the results of test dataset.\n","test accuracy    0.907\n"]}],"source":["print('Checking the results of test dataset.')\n","accu_test = evaluate(test_dataloader)\n","print('test accuracy {:8.3f}'.format(accu_test))"]},{"cell_type":"markdown","metadata":{"id":"51gnP8AvtXRf"},"source":["## ランダムなニュースでテスト\n","\n","学習済みモデルを使って、ゴルフのニュースをテストしてみましょう。\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZPaEJb5tXRf","outputId":"ab3ff406-393b-4b24-a40e-b57b3258fa8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is a Sports news\n"]}],"source":["ag_news_label = {1: \"World\",\n","                 2: \"Sports\",\n","                 3: \"Business\",\n","                 4: \"Sci/Tec\"}\n","\n","def predict(text, text_pipeline):\n","    with torch.no_grad():\n","        text = torch.tensor(text_pipeline(text))\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n","\n","ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n","    enduring the season’s worst weather conditions on Sunday at The \\\n","    Open on his way to a closing 75 at Royal Portrush, which \\\n","    considering the wind and the rain was a respectable showing. \\\n","    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n","    was another story. With temperatures in the mid-80s and hardly any \\\n","    wind, the Spaniard was 13 strokes better in a flawless round. \\\n","    Thanks to his best putting performance on the PGA Tour, Rahm \\\n","    finished with an 8-under 62 for a three-stroke lead, which \\\n","    was even more impressive considering he’d never played the \\\n","    front nine at TPC Southwind.\"\n","\n","model = model.to(\"cpu\")\n","\n","print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"eaac4ee8e735b6cee17c1b0358cfc8773b6176eb143c6cf3f1e52e7612ffbee2"}},"colab":{"provenance":[],"collapsed_sections":["8SB1lcfTtXQ4","9MJwxVq3tXQ-","NQxTC7lGtXRE","RGF4U1FztXRF","VPX6d88UtXRJ","Ffw-MZJ-tXRJ","vCGhCKAntXRK","lF3KVWUztXRL","kRRKupE0tXRM","PH82zUBltXRM","_tJ_S7GstXRN","FXd3ILFvtXRN","JDpH2QyctXRN","8RC0DHpVtXRO","ccx6Ujo-tXRO","W6Yd-KPstXRO","i3C0iGQytXRP","29LpZwP6tXRQ","hqNMHzX3tXRQ","SOY-6nvmtXRV","dIxP0zB8tXRV","HJ1zmOtztXRV","DtHDR4s1tXRW","KhjZkvKGtXRW","Dd-CBHhitXRW","qrkQHAWatXRY","6S3Z55BdtXRZ","TFEeQXH9tXRa","0_20ecGqtXRa","DT8Q5vvYtXRa","j6hJ7ExZtXRb","MMOPv_wTtXRb","lgL2nmv8tXRc","HsfZEGBMcflV","wLwcVWcXcjdO","C7u1XeU5cjnN"]},"widgets":{"application/vnd.jupyter.widget-state+json":{"13581e40c1914133aef13b057a3256bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39993552128a4cd6949450909713d6d3","IPY_MODEL_ee01185f98d447bca59ef1d23340a98d","IPY_MODEL_f89e54fa786f4af49baf05c849a1b64f"],"layout":"IPY_MODEL_dcc42ea85a40476b91789b8e26b135d7"}},"39993552128a4cd6949450909713d6d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da49309ceca14704bcf9d23f5138dc97","placeholder":"​","style":"IPY_MODEL_4669f5ce463b4c3a9a4225ecb3faef20","value":"100%"}},"ee01185f98d447bca59ef1d23340a98d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11de498a53aa46118077cd8a96a0ef74","max":26421880,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b981708677cf41eea9b894e474797e5e","value":26421880}},"f89e54fa786f4af49baf05c849a1b64f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d2ffca374b9433d881e422169a51c24","placeholder":"​","style":"IPY_MODEL_b2dfdbd81f68477cbf8a7b380a58e8ea","value":" 26421880/26421880 [00:02&lt;00:00, 17237960.29it/s]"}},"dcc42ea85a40476b91789b8e26b135d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da49309ceca14704bcf9d23f5138dc97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4669f5ce463b4c3a9a4225ecb3faef20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11de498a53aa46118077cd8a96a0ef74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b981708677cf41eea9b894e474797e5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d2ffca374b9433d881e422169a51c24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2dfdbd81f68477cbf8a7b380a58e8ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84b9a0f94b944cc38f90ac5660b63069":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93ab98191a1d494d8cb846d0605715e1","IPY_MODEL_a9d7fa60e9134822a8ba183d9ad6bde2","IPY_MODEL_18bab18cbc32455cbfc10ee0a9b525a3"],"layout":"IPY_MODEL_3d6eeff1dd89419c9191235b2c8edb4c"}},"93ab98191a1d494d8cb846d0605715e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_684990054f1e47988d9e22598f56a763","placeholder":"​","style":"IPY_MODEL_d44ba775f8784816be384b354a77080f","value":"100%"}},"a9d7fa60e9134822a8ba183d9ad6bde2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a30f93ff436149f382d5f42918188a8f","max":29515,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a57cbb77aa6e4c3c8cddf4e10c5033b0","value":29515}},"18bab18cbc32455cbfc10ee0a9b525a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_259599ae794c48d4a40b5cfa2fa1697d","placeholder":"​","style":"IPY_MODEL_4d8e35abda434e548b4f459f36354298","value":" 29515/29515 [00:00&lt;00:00, 200669.93it/s]"}},"3d6eeff1dd89419c9191235b2c8edb4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"684990054f1e47988d9e22598f56a763":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d44ba775f8784816be384b354a77080f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a30f93ff436149f382d5f42918188a8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a57cbb77aa6e4c3c8cddf4e10c5033b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"259599ae794c48d4a40b5cfa2fa1697d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d8e35abda434e548b4f459f36354298":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6fa3f8a17f845b6be9ab3656209c155":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f11a3764bda4cb98c9663e1258c76da","IPY_MODEL_f178b6d0c95f46e789d8e24378bd6061","IPY_MODEL_853929942d314f62879a200c71c21fc0"],"layout":"IPY_MODEL_070dfa9df9394fadbaa757a12b611bc8"}},"1f11a3764bda4cb98c9663e1258c76da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b682567977f94e288a803d6d3ed0f5c2","placeholder":"​","style":"IPY_MODEL_0041915ec54349e093e58df7b417860f","value":"100%"}},"f178b6d0c95f46e789d8e24378bd6061":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_909530b69474492db57c08d413dc7a3c","max":4422102,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0edf8ba937b64795a3bf02a8df099a4c","value":4422102}},"853929942d314f62879a200c71c21fc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca2f32296e974f06bdd96a0af02d8e6b","placeholder":"​","style":"IPY_MODEL_ea94dc2db77e4770b4cb4a3325c561ef","value":" 4422102/4422102 [00:01&lt;00:00, 5503311.18it/s]"}},"070dfa9df9394fadbaa757a12b611bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b682567977f94e288a803d6d3ed0f5c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0041915ec54349e093e58df7b417860f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"909530b69474492db57c08d413dc7a3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0edf8ba937b64795a3bf02a8df099a4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca2f32296e974f06bdd96a0af02d8e6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea94dc2db77e4770b4cb4a3325c561ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa298d8b6ed149fdb84b3e819d30c59e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_826e6b91840444d095111badea34d972","IPY_MODEL_f8f85b22a9c94ba18a1e5c898d993175","IPY_MODEL_8367e4540d674a65ac75746f235662fa"],"layout":"IPY_MODEL_55f9d5aad8d14a8f983e99edc623ff60"}},"826e6b91840444d095111badea34d972":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ef4172dc3fa46419aaef85fbf2a54f1","placeholder":"​","style":"IPY_MODEL_60c6aac309cd44d1a00fadee5bc0797e","value":"100%"}},"f8f85b22a9c94ba18a1e5c898d993175":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a348f3c9d3314f0d8f7356d4576efca7","max":5148,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c231dbf09bd944f296c4e55d36c0691f","value":5148}},"8367e4540d674a65ac75746f235662fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f893294129f43e8934d344dc2d8a665","placeholder":"​","style":"IPY_MODEL_737a35f8c3f7450780a44f6c0c23e27b","value":" 5148/5148 [00:00&lt;00:00, 312963.30it/s]"}},"55f9d5aad8d14a8f983e99edc623ff60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ef4172dc3fa46419aaef85fbf2a54f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c6aac309cd44d1a00fadee5bc0797e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a348f3c9d3314f0d8f7356d4576efca7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c231dbf09bd944f296c4e55d36c0691f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f893294129f43e8934d344dc2d8a665":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"737a35f8c3f7450780a44f6c0c23e27b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}