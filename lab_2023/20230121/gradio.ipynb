{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gAkI4QsJKv_v"
      },
      "source": [
        "# Gradioでデモ構築"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uooHs4iKv_y"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets evaluate transformers[sentencepiece]\n",
        "# !pip install gradio"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradioの構文に慣れよう\n",
        "\n",
        "1. メインの関数を定義する：ここではgreet()という関数。機械学習のアプリケーションでは、この関数がモデルを呼び出して入力に対して予測を行い、その出力を返す。\n",
        "2. fn、inputs、outputsの3つの引数を持つGradio Interfaceを作成する：これらの引数は、入力と出力のコンポーネントのタイプを定義します。下の例では、両方のコンポーネントが単純なテキストボックスになっています。\n",
        "3. 作成した Interface に対して launch() メソッドを呼び出す\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZVZDwXcLKv_y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7869\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name\n",
        "\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "入力コンポーネントとしてクラスオブジェクトをインスタンス化することができます。\n",
        "ここではTextboxという入力コンポーネントで、ラベル、プレースホルダー、行数を設定した入力テキストボックスを作成できます。\n",
        "\n",
        "出力コンポーネントも同様にいじれます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYuxmNBoKv_z"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name\n",
        "\n",
        "\n",
        "# We instantiate the Textbox class\n",
        "textbox = gr.Textbox(label=\"Type your name here:\", placeholder=\"John Doe\", lines=2)\n",
        "\n",
        "gr.Interface(fn=greet, inputs=textbox, outputs=\"text\").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT-2のようなテキスト生成モデルをデモするための簡単なインターフェースを作ってみましょう。\n",
        "\n",
        "まず、テキストプロンプトを取り込み、テキスト補完を返す予測関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuHuGXsdKv_z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# GPT-2を使ったテキスト生成を実装してみてください。\n",
        "def predict(prompt):\n",
        "    completion = None\n",
        "    return completion\n",
        "\n",
        "\n",
        "gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J0zaAjsKKx74"
      },
      "source": [
        "# Interface class詳細\n",
        "\n",
        "Interfaceの作成に必須である引数\n",
        "1. fn: Gradioインターフェースによってラップされる関数。\n",
        "2. inputs: 入力コンポーネントのタイプ（複数可）。Gradioは、\"image \"や \"mic \"などの多くのビルド済みコンポーネントを提供してくれます。\n",
        "3. outputs: 出力コンポーネントの種類（複数可）。入力と同様。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### オーディオを使った簡単な例\n",
        "地味にすごいのはマイクとスピーカーを制御して音声を処理できることです。ここでは、オーディオファイルを受け取り、それを単純に反転させるaudio-to-audio関数を作成します。\n",
        "\n",
        "入力には、Audio コンポーネントを使用します。Audioコンポーネントを使用する場合、オーディオのソースとして、ユーザがアップロードするファイルか、ユーザが自分の声を録音するマイクかを指定することができます。今回は、「マイク」に設定してみましょう。\n",
        "\n",
        "さらに、音声をnumpyの配列として受け取り、簡単に「逆引き」できるようにしたいと思います。そこで、\"type \"を \"numpy \"に設定し、入力データを(sample_rate, data)のタプルとして関数に渡します。\n",
        "\n",
        "また、サンプルレートとnumpy配列のデータを持つタプルを、再生可能なオーディオファイルとして自動的にレンダリングできるAudio出力コンポーネントを使用します。この場合、カスタマイズは必要ないので、文字列のショートカットである \"audio \"を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwji5MwVKx77"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "def reverse_audio(audio):\n",
        "    sr, data = audio  # 音声データは大体このように扱ってきた。それが埋め込まれているのは便利。\n",
        "    reversed_audio = (sr, np.flipud(data))  # https://note.nkmk.me/python-numpy-flip-flipud-fliplr/\n",
        "    return reversed_audio\n",
        "\n",
        "\n",
        "mic = gr.Audio(source=\"microphone\", type=\"numpy\", label=\"Speak here...\")\n",
        "gr.Interface(reverse_audio, mic, \"audio\").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 複数の入出力の処理\n",
        "もっと複雑な、複数の入力と出力を持つ関数があるとしましょう。以下の例では、ドロップダウンのインデックス、スライダーの値、および数値を受け取り、音楽の音声サンプルを返す関数があります。\n",
        "\n",
        "入出力コンポーネントのリストをどのように渡しているかを見て、何が起こっているかを追ってみてください。\n",
        "\n",
        "ここで重要なのは、入力コンポーネントのリストを渡すとき\n",
        "\n",
        "1. 入力コンポーネントのリスト、各コンポーネントは順番にパラメータに対応する。\n",
        "2. 出力コンポーネントのリストでは、各コンポーネントが返される値に対応します。\n",
        "3. 以下のコードでは、3つの入力コンポーネントがgenerate_tone()関数の3つの引数にどのように並んでいるのかを示しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZjgcemyKx77"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]  # 音の名前、indexがノートナンバー\n",
        "\n",
        "\n",
        "def generate_tone(note, octave, duration):\n",
        "    sr = 48000  # sr: sampling frequency, サンプリング周波数。１秒間に48000点のデータ点をもつ。\n",
        "    a4_freq = 440  # A4音、ラの音（https://acoustics.jp/qanda/answer/168.html）\n",
        "    tones_from_a4 = 12 * (octave - 4) + (note - 9)  # オクターブは12音なので、鍵盤のどこのブロックか。notesのA（９）を基準に、ブロック内のどこか。https://web.quizknock.com/octave\n",
        "    frequency = a4_freq * 2 ** (tones_from_a4 / 12) # 生成したい音の周波数を計算（https://www.asahi-net.or.jp/~hb9t-ktd/music/Japan/Research/DTM/freq_map.html）\n",
        "    duration = int(duration)\n",
        "    audio = np.linspace(0, duration, duration * sr)  # https://note.nkmk.me/python-numpy-arange-linspace/\n",
        "    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)  # http://www.slp.k.hosei.ac.jp/~itou/lecture/2011/DigitalData/01_text.pdf\n",
        "    return (sr, audio)\n",
        "\n",
        "\n",
        "gr.Interface(\n",
        "    generate_tone,\n",
        "    [\n",
        "        gr.Dropdown(notes, type=\"index\"),\n",
        "        gr.Slider(minimum=4, maximum=6, step=1),\n",
        "        gr.Number(type=\"number\", value=1, label=\"Duration in seconds\"),\n",
        "    ],\n",
        "    \"audio\",\n",
        ").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "音声認識は、このような処理を土台に前処理を行います（大変だった）。Transformer以降、音声認識の分野ではE2Eモデルと言われていましたが、前処理も簡素になりました。以前は、言語モデルだけでなく音素辞書や発音辞書などが必要だったり、Deeplearningのモデルは一部での利用でした。全てを取り払ってDeeplearningで解決してしまうのが当たり前になりましたが、チューニングのしやすさなどから、以前の音声認識技術はまだまだ活用されそうです。\n",
        "\n",
        "launch()の引数を試してみてください。\n",
        "- inline=False\n",
        "- inbrowser=True\n",
        "- share=True\n",
        "\n",
        "share=Trueで、公開された共有可能なリンクが生成され、誰にでも送ることができます。このリンクを送ると、相手側のユーザーは最大72時間、自分のブラウザでモデルを試用することができます。処理はあなたのデバイスで行われるため（デバイスの電源が入っている限り！）、依存関係のパッケージングを心配する必要はありません。Google Colabノートブックで作業している場合、共有リンクは常に自動的に作成されます。通常はこのような形です。XXXXX.gradio.app. このリンクはGradioのリンクを通じて提供されますが、私たちはあなたのローカルサーバーのプロキシに過ぎず、インターフェースを通じて送られたデータを保存することはありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa4O-orvKx78"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "model = pipeline(\"automatic-speech-recognition\")\n",
        "\n",
        "\n",
        "def transcribe_audio(mic=None, file=None):\n",
        "    if mic is not None:\n",
        "        audio = mic\n",
        "    elif file is not None:\n",
        "        audio = file\n",
        "    else:\n",
        "        return \"You must either provide a mic recording or a file\"\n",
        "    transcription = model(audio)[\"text\"]\n",
        "    return transcription\n",
        "\n",
        "\n",
        "gr.Interface(\n",
        "    fn=transcribe_audio,\n",
        "    inputs=[\n",
        "        gr.Audio(source=\"microphone\", type=\"filepath\", optional=True),\n",
        "        gr.Audio(source=\"upload\", type=\"filepath\", optional=True),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        ").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここで入力した`音声データはダウンロード`しておいてください。後で使います。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hbUbr0osKz_u"
      },
      "source": [
        "# デモをシェアする\n",
        "\n",
        "デモだけあっても不親切なので、案内情報を使いできるようになっています。\n",
        "\n",
        "![fig](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview.png)\n",
        "\n",
        "- title: 入出力コンポーネントの上に表示されるデモのタイトル\n",
        "- description: インターフェイスの説明 (テキスト、Markdown または HTML) \n",
        "- article: インターフェイスを説明する記事(テキスト、Markdown、HTMLのいずれか)。入力と出力のコンポーネントの下に表示されます。\n",
        "- theme: テーマ（default, huggingface, grass, peach）。- dark- prefix を追加することもできます。\n",
        "- examples: あなたのデモをより使いやすくするために、関数への入力例をいくつか用意することができます。これらはネストしたリストとして提供する必要があり、外側のリストはサンプルで構成され、内側のリストは各入力コンポーネントに対応する入力で構成されています。\n",
        "- live: デモを「ライブ」にしたい場合、つまり、入力が変わるたびにモデルを再実行したい場合は、live=True を設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltfAkjv2Kz_x"
      },
      "outputs": [],
      "source": [
        "title = \"Ask Rick a Question\"\n",
        "description = \"\"\"\n",
        "The bot was trained to answer questions based on Rick and Morty dialogues. Ask Rick anything!\n",
        "<img src=\"https://huggingface.co/spaces/course-demos/Rick_and_Morty_QA/resolve/main/rick.png\" width=200px>\n",
        "\"\"\"\n",
        "\n",
        "article = \"Check out [the original Rick and Morty Bot](https://huggingface.co/spaces/kingabzpro/Rick_and_Morty_Bot) that this demo is based off of.\"\n",
        "\n",
        "gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=\"textbox\",\n",
        "    outputs=\"text\",\n",
        "    title=title,\n",
        "    description=description,\n",
        "    article=article,\n",
        "    examples=[[\"What are you doing?\"], [\"Where should we time travel to?\"]],\n",
        ").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hugging Face Spacesでデモをホストする\n",
        "\n",
        "共有リンクは一時的なもので実際には使い物にはなりません。永続的なホスティング環境が必要です。\n",
        "\n",
        "Hugging Face Spacesは、Gradioデモをインターネット上で恒久的にホストするためのインフラを、無料で提供します。Spacesは、あなたのGradioインターフェイスのコードがapp.pyファイルに存在する（パブリックまたはプライベート）レポを作成し、そこにプッシュすることを可能にします。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "以下はスケッチ認識のデモで、HuggingFaceを使わない例です。以下のファイルをダウンロードする必要があります。\n",
        "\n",
        "- [class_names.txt](https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/class_names.txt)\n",
        "- [pytorch_model.bin](https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/pytorch_model.bin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/spaces/course-demos/Sketch-Recognition/raw/main/class_names.txt\n",
        "# !wget https://huggingface.co/spaces/course-demos/Sketch-Recognition/resolve/main/pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtwABez9Kz_x"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import gradio as gr\n",
        "from torch import nn\n",
        "\n",
        "LABELS = Path(\"class_names.txt\").read_text().splitlines()\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(32, 64, 3, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(64, 128, 3, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1152, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, len(LABELS)),\n",
        ")\n",
        "state_dict = torch.load(\"pytorch_model.bin\", map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def predict(im):\n",
        "    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "    values, indices = torch.topk(probabilities, 5)\n",
        "    return {LABELS[i]: v.item() for i, v in zip(indices, values)}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "このinputs=\"sketchpad\"はサブミットボタンなしにデータをモデルに行って、推論を出すことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvOGZmzEKz_y"
      },
      "outputs": [],
      "source": [
        "interface = gr.Interface(\n",
        "    predict,\n",
        "    inputs=\"sketchpad\",\n",
        "    outputs=\"label\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"Sketch Recognition\",\n",
        "    description=\"Who wants to play Pictionary? Draw a common object like a shovel or a laptop, and the algorithm will guess in real time!\",\n",
        "    article=\"<p style='text-align: center'>Sketch Recognition | Demo Model</p>\",\n",
        "    live=True,\n",
        ")\n",
        "interface.launch(share=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N90y8CPKK1Tt"
      },
      "source": [
        "# Hugging Face Hubとの連携\n",
        "\n",
        "Interface.load() メソッドを使用して、\"model/\" (もしくは \"huggingface/\") とモデル名を続けて渡します。\n",
        "\n",
        "これによって、モデルを直接ロードすることができます。piplineと同等の処理が実装されています。\n",
        "\n",
        "それだけでなく、メモリにモデルをロードしていません。Hugging Faceの推論APIを使用しています。これはGPT-JやT0ppのような、多くのRAMを必要とする巨大なモデルに理想的です。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPDOcnyoK1Tw"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "title = \"GPT-J-6B\"\n",
        "description = \"Gradio Demo for GPT-J 6B, a transformer model trained using Ben Wang's Mesh Transformer JAX. 'GPT-J' refers to the class of model, while '6B' represents the number of trainable parameters. To use it, simply add your text, or click one of the examples to load them. Read more at the links below.\"\n",
        "article = \"<p style='text-align: center'><a href='https://github.com/kingoflolz/mesh-transformer-jax' target='_blank'>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</a></p>\"\n",
        "examples = [\n",
        "    [\"The tower is 324 metres (1,063 ft) tall,\"],\n",
        "    [\"The Moon's orbit around Earth has\"],\n",
        "    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n",
        "]\n",
        "gr.Interface.load(\n",
        "    \"huggingface/EleutherAI/gpt-j-6B\",\n",
        "    inputs=gr.Textbox(lines=5, label=\"Input Text\"),\n",
        "    title=title,\n",
        "    description=description,\n",
        "    article=article,\n",
        "    examples=examples,\n",
        "    enable_queue=True,\n",
        ").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hugging Faceハブから任意のSpaceをロードしてローカルに再作成することも一発でできます。\n",
        "\n",
        " `Interface` に `spaces/` を渡し、その後にSpaceの名前を指定することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbtBLvvyK1Tw"
      },
      "outputs": [],
      "source": [
        "gr.Interface.load(\"spaces/abidlabs/remove-bg\").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "HubやSpacesからデモを読み込む際に便利なのが、パラメータをオーバーライドしてカスタマイズすることです。ここでは、タイトルを追加し、代わりにWebカメラで動作するようにしています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "miKAxu1kK1Tw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching Space from: https://huggingface.co/spaces/abidlabs/remove-bg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hajime/projects/ai_lab/.venv/lib/python3.9/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/Users/hajime/projects/ai_lab/.venv/lib/python3.9/site-packages/gradio/interface.py:329: UserWarning: Currently, only the 'default' theme is supported.\n",
            "  warnings.warn(\"Currently, only the 'default' theme is supported.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7868\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gr.Interface.load(\n",
        "    \"spaces/abidlabs/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\"\n",
        ").launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sf1Ru4-iK2ZZ"
      },
      "source": [
        "# Advanced Interface features\n",
        "\n",
        "ここまでで基本的な機能を動かしてきました。ここではstateを導入します。\n",
        "\n",
        "## state: データを持続させるためにステートを使用する\n",
        "Gradioは、ページロード内の複数のサブミットにわたってデータを永続化するセッションステートをサポートしています。セッションステートは、例えばチャットボットのデモを作成する際に、ユーザーがモデルとインタラクトする際にデータを持続させたい場合に便利です。\n",
        "\n",
        "セッションステートにデータを保存するには、3つのことを行う必要があります。\n",
        "\n",
        "1. 関数に、インターフェースの状態を表す追加パラメータを渡す。\n",
        "2. 関数の最後で、状態の更新された値を追加の戻り値として返す。\n",
        "3. インターフェースの作成時に、「state」入力コンポーネントと「state」出力コンポーネントを追加します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOSrBGu6K2Zc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def chat(message, history):\n",
        "    history = history or []\n",
        "    if message.startswith(\"How many\"):\n",
        "        response = random.randint(1, 10)\n",
        "    elif message.startswith(\"How\"):\n",
        "        response = random.choice([\"Great\", \"Good\", \"Okay\", \"Bad\"])\n",
        "    elif message.startswith(\"Where\"):\n",
        "        response = random.choice([\"Here\", \"There\", \"Somewhere\"])\n",
        "    else:\n",
        "        response = \"I don't know\"\n",
        "    history.append((message, response))\n",
        "    return history, history\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    chat,\n",
        "    [\"text\", \"state\"],\n",
        "    [\"chatbot\", \"state\"],\n",
        "    allow_screenshot=False,\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "iface.launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4d1dYEaK4CN"
      },
      "source": [
        "# Blocks class\n",
        "Interface classとは違う、低レベルAPIであるBlocks classはデータフローとレイアウトを制御できます。\n",
        "\n",
        "- Interface：入力と出力のリストを提供するだけで、完全な機械学習デモを作成できる高レベルのAPIです。\n",
        "- Blocks：Blocks（「積み木」の意）を使って、非常に複雑で多段階のアプリケーションを構築することができます。\n",
        "\n",
        "\n",
        "以下のような要求に応えることができます。\n",
        "- 関連するデモを1つのWebアプリケーションで複数のタブとしてグループ化したい。\n",
        "- デモのレイアウトを変更したい（例：入出力の位置を指定したい）。\n",
        "- あるモデルの出力が次のモデルの入力になるようなマルチステップインターフェースなど、より柔軟なデータフローが欲しい。\n",
        "- ユーザーの入力に基づき、コンポーネントのプロパティ（例えば、ドロップダウンの選択肢）やその可視性を変更したい。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGxpJRpjK4CQ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def flip_text(x):\n",
        "    return x[::-1]\n",
        "\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "with demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "    # Flip Text!\n",
        "    Start typing below to see the output.\n",
        "    \"\"\"\n",
        "    )\n",
        "    input = gr.Textbox(placeholder=\"Flip this text\")\n",
        "    output = gr.Textbox()\n",
        "\n",
        "    input.change(fn=flip_text, inputs=input, outputs=output)\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "この簡単な例では、Blocksの根底にある4つの概念を紹介しています。\n",
        "\n",
        "1. Blocksを使うと、with gradio.Blocksコンテキスト内でPythonのオブジェクトをインスタンス化するだけで、マークダウン、HTML、ボタン、インタラクティブなコンポーネントを組み合わせたWebアプリケーションを構築することができるようになります。コンポーネントをインスタンス化する順番は、各要素が作成された順番にWebアプリにレンダリングされるため、重要です。(より複雑なレイアウトについては後述します)\n",
        "\n",
        "2. 通常のPython関数は、コードのどこにでも定義でき、ブロックを使ってユーザーの入力で実行することができます。この例では入力されたテキストを「反転」させる簡単な関数を書いていますが、単純な計算から機械学習モデルによる予測処理まで、あらゆるPython関数を書くことができます。\n",
        "\n",
        "3. Blocksのコンポーネントには、イベントを割り当てることができます。これにより、そのコンポーネントがクリックされたり、変更されたりしたときに、関数が実行されます。イベントを割り当てる際には、fn: 呼び出される関数、inputs: 入力コンポーネントのリスト、outputs: 呼び出される出力コンポーネントのリストという3つのパラメータを渡すことになります。\n",
        "\n",
        "4. Blocksは、定義したイベントトリガーをもとに、コンポーネントがインタラクティブ（ユーザーからの入力を受け付ける）であるべきかどうかを自動的に判断します。この例では、最初のテキストボックスの値は flip_text() 関数で使用されるため、インタラクティブになります。2 番目のテキストボックスは、その値が入力として使用されることはないので、インタラクティブではありません。これを上書きしたい場合は、コンポーネントの interactive パラメータにブール値を渡します（例： gr.Textbox(placeholder=\"Flip this text\", interactive=True))。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## デモのレイアウトをカスタマイズする\n",
        "デフォルトでは、Blocksは作成したコンポーネントを1列に縦に並べてレンダリングします。これを変更するには、 gradio.Column(): で列を、 gradio.Row(): で行を追加し、それらのコンテキスト内でコンポーネントを作成します。\n",
        "\n",
        "ここで注意しなければならないのは、Column の下に作成されたコンポーネント（これはデフォルトでもあります）は、縦にレイアウトされるということです。Rowの下に作成されたコンポーネントは、ウェブ開発におけるフレックスボックスモデルと同様に、水平方向にレイアウトされます。\n",
        "\n",
        "最後に、with gradio.Tabs()コンテキストマネージャを使用して、デモ用のタブを作成することもできます。このコンテキスト内では、with gradio.TabItem(name_of_tab): children を指定することで複数のタブを作成することができます。with gradio.TabItem(name_of_tab): コンテキスト内に作成されたコンポーネントは、そのタブに表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx9QN960K4CQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "\n",
        "def flip_text(x):\n",
        "    return x[::-1]\n",
        "\n",
        "\n",
        "def flip_image(x):\n",
        "    return np.fliplr(x)\n",
        "\n",
        "\n",
        "with demo:\n",
        "    gr.Markdown(\"Flip text or image files using this demo.\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Flip Text\"):\n",
        "            with gr.Row():\n",
        "                text_input = gr.Textbox()\n",
        "                text_output = gr.Textbox()\n",
        "            text_button = gr.Button(\"Flip\")\n",
        "        with gr.TabItem(\"Flip Image\"):\n",
        "            with gr.Row():\n",
        "                image_input = gr.Image()\n",
        "                image_output = gr.Image()\n",
        "            image_button = gr.Button(\"Flip\")\n",
        "\n",
        "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
        "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## イベントと状態を調べる\n",
        "レイアウトをコントロールするのと同じように、Blocksではどのイベントが関数呼び出しのトリガーになるかを細かくコントロールすることができます。各コンポーネントや多くのレイアウトには、それぞれ対応する特定のイベントがあります。\n",
        "\n",
        "例えばTextboxコンポーネントには、change() （テキストボックス内の値が変化したとき）とsubmit() （テキストボックスにフォーカスした状態でユーザがエンターキーを押したとき）の2つのイベントがあります。より複雑なコンポーネントでは、さらに多くのイベントを持つことができます。たとえば、Audioコンポーネントでは、オーディオファイルが再生、クリア、一時停止されたときなどにも別のイベントが発生します。各コンポーネントがサポートするイベントについては、ドキュメントを参照してください。\n",
        "\n",
        "イベントトリガーは、これらのイベントのいずれか、または複数にアタッチすることができます。イベントトリガーを作成するには、コンポーネントインスタンスのイベント名を関数として呼び出します（例： textbox.change(...) や btn.click(...) ）。この関数は、前述したように3つのパラメータを受け取ります。\n",
        "\n",
        "- fn: 実行する関数\n",
        "- inputs: 関数の入力パラメータとして値を与えるコンポーネント（のリスト）。各コンポーネントの値は、順番に対応する関数パラメータにマップされます。関数がパラメータを取らない場合、このパラメータは None にすることができます。\n",
        "- outputs: 関数から返される値に基づいて値が更新されるコンポーネント（のリスト）。各戻り値は、順番に、対応するコンポーネントの値を設定します。このパラメータは、関数が何も返さない場合は None にすることができます。\n",
        "\n",
        "GPTモデルを使ってテキスト補完を行うこの例のように、入力と出力のコンポーネントを同じコンポーネントにすることもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTMAfv-2K4CQ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "api = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n",
        "\n",
        "\n",
        "def complete_with_gpt(text):\n",
        "    # Use the last 50 characters of the text as context\n",
        "    return text[:-50] + api(text[-50:])\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    textbox = gr.Textbox(placeholder=\"Type here and press enter...\", lines=4)\n",
        "    btn = gr.Button(\"Generate\")\n",
        "\n",
        "    # イベントトリガー\n",
        "    btn.click(complete_with_gpt, textbox, textbox)\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## マルチステップ・デモの作成\n",
        "ある関数の出力を次の関数の入力として再利用するような、マルチステップのデモを実装します。あるイベント・トリガーの入力にコンポーネントを使い、別のイベント・トリガーの出力にコンポーネントを使うことができるので、これはBlockでとても簡単に実現できます。\n",
        "\n",
        "以下の例のtextコンポーネントを見てください。その値は音声テキストモデルの結果ですが、センチメント分析モデルにも渡されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD9U1bjnK4CR"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "asr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
        "classifier = pipeline(\"text-classification\")\n",
        "\n",
        "\n",
        "def speech_to_text(speech):\n",
        "    text = asr(speech)[\"text\"]\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_to_sentiment(text):\n",
        "    return classifier(text)[0][\"label\"]\n",
        "\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "with demo:\n",
        "    audio_file = gr.Audio(type=\"filepath\")\n",
        "    text = gr.Textbox()\n",
        "    label = gr.Label()\n",
        "\n",
        "    b1 = gr.Button(\"Recognize Speech\")\n",
        "    b2 = gr.Button(\"Classify Sentiment\")\n",
        "\n",
        "    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n",
        "    b2.click(text_to_sentiment, inputs=text, outputs=label)\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## コンポーネントのプロパティを更新する\n",
        "他のコンポーネントの値を更新するためのイベントを作成する方法について見てきました。\n",
        "\n",
        "しかし、テキストボックスの可視性やラジオボタングループの選択肢のように、コンポーネントの他のプロパティを変更したい場合はどうすればよいのでしょうか。この場合、関数から通常の戻り値ではなく、コンポーネントクラスの update() メソッドを返すことで対応できます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Gd9gJWK4CR"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def change_textbox(choice):\n",
        "    if choice == \"short\":\n",
        "        return gr.Textbox.update(lines=2, visible=True)\n",
        "    elif choice == \"long\":\n",
        "        return gr.Textbox.update(lines=8, visible=True)\n",
        "    else:\n",
        "        return gr.Textbox.update(visible=False)\n",
        "\n",
        "\n",
        "with gr.Blocks() as block:\n",
        "    radio = gr.Radio(\n",
        "        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n",
        "    )\n",
        "    text = gr.Textbox(lines=2, interactive=True)\n",
        "\n",
        "    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n",
        "    block.launch()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# おまけ。挙動をフラグする\n",
        "\n",
        "機械学習モデルのデモを行う際、そのモデルを試したユーザーからデータ、特にモデルが期待通りに動作していないときがあります。\n",
        "\n",
        "この時に入力データは？いつ？などの情報を収集したくなります。モデルにとって「難しい」データを収集することは、機械学習モデルを改善することができます。\n",
        "\n",
        "Gradioは、すべてのInterfaceにFlagボタンを含めることで、このデータの収集を簡素化します。これにより、ユーザーやテスターは、デモが実行されているマシンに簡単にデータを送り返すことができます。\n",
        "\n",
        "このボタンを押すと、ローカルにflaggedというフォルダが作成され、log.csvに記録されます。\n",
        "音声データなども保存させることができます。\n",
        "\n",
        "詳細はこちらです。\n",
        "https://gradio.app/using_flagging/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Building your first demo",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "eaac4ee8e735b6cee17c1b0358cfc8773b6176eb143c6cf3f1e52e7612ffbee2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
