{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デモ実装（テキスト生成）\n",
    "\n",
    "### 環境構築\n",
    "今回は必要なライブラリは三つです。Mecabは使用しません。\n",
    "- transformers\n",
    "- sentencepiece\n",
    "- gradio\n",
    "#### 課題その１（対象：全員）\n",
    "４パターンのデコード方法の結果を同時に出力するデモを実装してください。\n",
    "\n",
    "#### 課題その２（対象：その１が早めに終わった人）\n",
    "４パターンのデコード方法の結果から、どれか一つ選んで、続きを出力するデモを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import T5Tokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. モデルとトークナイザーの定義\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
    "tokenizer.do_lower_case = True  # rinna/japanese-gpt2特有のハック\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"rinna/japanese-gpt2-small\",\n",
    "    pad_token_id=tokenizer.eos_token_id  # warningを避けるために、padにEOSトークンを割りあてる\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 課題その１（対象：全員）\n",
    "４パターンのデコード方法の結果を同時に出力するデモを実装してください。\n",
    "### テキスト生成の関数generation()の実装\n",
    "\n",
    "下記のコメントを参考に、必要な処理を実装してください。コメントのある箇所は全て実装が必要な箇所です。\n",
    "```python\n",
    "def generate(text, max_length, num_beams, p):\n",
    "    generate_config_list = [\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Greedyの設定\n",
    "        ),\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Smaplingの設定\n",
    "        ),\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Beam Searchの設定\n",
    "        ),\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Top-p Smaplingの設定\n",
    "        )\n",
    "    ]\n",
    "    generated_texts = []\n",
    "\n",
    "    inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    for generate_config in generate_config_list:\n",
    "        output = ...  # modelを使ってテキスト生成を行います。\n",
    "\n",
    "        generated = ...  # tokenizerを使って、outputを単語に変換します。\n",
    "        generated_texts.append(\"。\\n\".join(generated.replace(\" \", \"\").split(\"。\")))\n",
    "    return tuple(generated_texts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gradioのコンポーネントのイベント処理用の関数の定義\n",
    "def generate(text, max_length, num_beams, p):\n",
    "    \"\"\"初回のテキスト生成\n",
    "\n",
    "    テキスト生成を行うが、デコード方法によって異なる結果になることを示すための処理を行う。\n",
    "    指定されたパラメタを使って、異なる４つデコード方法を同時に出力する。\n",
    "\n",
    "    Args:\n",
    "        str: Stateから取得（続きを生成するためのプロンプト）\n",
    "        int: Sliderから取得（全てのデコード方法に共通のパラメタ。生成する単語数）\n",
    "        int: Sliderから取得（Beam Searchのパラメタ）\n",
    "        int: Sliderから取得（Top-p Samplingのパラメタ）\n",
    "    \n",
    "    Returns:\n",
    "        str: State（生成結果を入出力の状態に反映）\n",
    "        str: TextArea（全文表示用のコンポーネントで使用）\n",
    "        str: TextArea（今回生成した文を表示するコンポーネントで使用）\n",
    "    \"\"\"\n",
    "    # テキスト生成用のconfigクラスを使って、４パターンの設定を定義する。\n",
    "    generate_config_list = [\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Greedyの設定\n",
    "        ),\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Smaplingの設定\n",
    "        ),\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Beam Searchの設定\n",
    "        ),\n",
    "        GenerationConfig(\n",
    "            max_new_tokens=max_length,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Top-p Smaplingの設定\n",
    "        )\n",
    "    ]\n",
    "    generated_texts = []\n",
    "\n",
    "    inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    for generate_config in generate_config_list:\n",
    "        output = ...  # modelを使ってテキスト生成を行います。\n",
    "\n",
    "        generated = ...  # tokenizerを使って、outputを単語に変換します。\n",
    "\n",
    "        # 読みやすくさの処理を行なって、リストに追加\n",
    "        generated_texts.append(\"。\\n\".join(generated.replace(\" \", \"\").split(\"。\")))\n",
    "\n",
    "    # gradioはtupleを想定している。\n",
    "    # これと同じ処理：return generated_texts[0], generated_texts[1], generated_texts[2]\n",
    "    # pythonのタプルは「,」によって生成される。丸括弧は省略可能。\n",
    "    # 参考：https://note.nkmk.me/python-function-return-multiple-values/\n",
    "    return tuple(generated_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradio appの実装\n",
    "- UIの実装パート（課題対象）\n",
    "- イベント処理の実装パート（課題対象外）\n",
    "\n",
    "### Gradioのコンポーネントの実装\n",
    "下記のコメントを参考に、必要な処理を実装してください。\n",
    "コメントのある箇所は全て実装が必要な箇所です。\n",
    "コンポーネントの引数は参考です。そのままで与えても動きません。\n",
    "```python\n",
    "with gr.Row():\n",
    "    with gr.Column():\n",
    "        input_text = ...  # テキストを入力するコンポーネント\n",
    "        max_length = ...  # 数値を入力するコンポーネント.参考：（min=100, max=1000, step=100, default=100）\n",
    "        num_beams = ...  # 数値を入力するコンポーネント.参考：（min=1, max=10, step=1, default=6）\n",
    "        p = ...    # テキストを入力するコンポーネント.参考：（min=0, max=1, step=0.01, default=0.92）\n",
    "        btn = gr.Button(\"Decode\")\n",
    "    \n",
    "    with gr.Column():\n",
    "        out1 = ...  # Greedy decode outputを表示するコンポーネント\n",
    "        out2 = ...  # Smapling decode outputを表示するコンポーネント\n",
    "        out3 = ...  # Beam Search decode outputを表示するコンポーネント\n",
    "        out4 = ...  # Top-p Sampling decode outputを表示するコンポーネント\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GradioによるUI/イベント処理の定義\n",
    "with gr.Blocks() as demo:\n",
    "    # 2.1. UI\n",
    "    gr.Markdown('''\n",
    "    # テキスト生成\n",
    "    テキストを入力すると、４パターンのデコード方法でテキスト生成を実行します。\n",
    "    ## ４つのパターン（入門編）\n",
    "    1. Greedy: ビームサーチもサンプリングも行いません。毎回、最も確率の高い単語を選択します。\n",
    "    2. Sampling: モデルによって与えられた語彙全体の確率分布に基づいて次の単語を選択します。\n",
    "    3. Beam Search: 各タイムステップで複数の仮説を保持し、最終的に仮説ごとのシーケンス全体で最も高い確率を持つ仮説を選択します。\n",
    "    4. Top-p Sampling: 2の方法に関して、確率の和がpになる最小の単語にフィルタリングすることで、確率が低い単語が選ばれる可能性を無くします。\n",
    "    ''')\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = ...  # テキストを入力するコンポーネント\n",
    "            max_length = ...  # 数値を入力するコンポーネント.参考：（min=100, max=1000, step=100, default=100）\n",
    "            num_beams = ...  # 数値を入力するコンポーネント.参考：（min=1, max=10, step=1, default=6）\n",
    "            p = ...    # テキストを入力するコンポーネント.参考：（min=0, max=1, step=0.01, default=0.92）\n",
    "            btn = gr.Button(\"Decode\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            out1 = ...  # Greedy decode outputを表示するコンポーネント\n",
    "            out2 = ...  # Smapling decode outputを表示するコンポーネント\n",
    "            out3 = ...  # Beam Search decode outputを表示するコンポーネント\n",
    "            out4 = ...  # Top-p Sampling decode outputを表示するコンポーネント\n",
    "\n",
    "    # 2.2 イベント処理\n",
    "    btn.click(fn=generate, inputs=[input_text, max_length, num_beams, p], outputs=[out1, out2, out3, out4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradio appの立ち上げ\n",
    "https://localhost:7860/\n",
    "（もしくは出力に表示された別のURL）をクリックして、アクセスしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 課題その２（対象：その１が早めに終わった人）\n",
    "４パターンのデコード方法の結果から、どれか一つ選んで、続きを出力するデモを実装してください。\n",
    "\n",
    "以下の要求を満たすように実装してください。\n",
    "- ４つのデコード結果から１つを選んで、初回のデコード結果を取得できる\n",
    "- 初回のデコード結果を読み込んで、続きを生成する際に、４つのデコード方法から１つ選び、続きのテキスト生成が実行できる\n",
    "- 以後、続きのテキストを入力して、実行するたびに、結果が増えていく\n",
    "- 続きのテキスト生成を実行した際には、続きの部分を含めて全文を確認することができる（入力の確認）\n",
    "- 続きのテキスト生成を実行した際には、続きの部分だけを確認することができる（出力の確認）\n",
    "\n",
    "以下の条件を使って実装してください。\n",
    "- `gr.State()`を使って、入出力の変数（状態）を管理する\n",
    "    - 初期値はout1の結果とするために、out1が生成された時に入出力の変数数に代入する\n",
    "    - 実行後は生成結果を入出力の変数数に代入し、次回実行時の入力とする\n",
    "- 入出力のテキストの確認には、`gr.TextArea()`を使う\n",
    "- イベント処理として以下の三つの関数を使う。`generate_next()`は処理の実装が必要\n",
    "```python\n",
    "def select_out1(out1):\n",
    "    \"\"\"out1が生成された時に、out1を後続の処理のデフォルト値に入力\n",
    "    \"\"\"\n",
    "    return out1, out1, out1\n",
    "```\n",
    "```python\n",
    "def select_out(radio, out1, out2, out3, out4):\n",
    "    \"\"\"後続の処理に使用する、初回の処理結果を選択する\n",
    "    \"\"\"\n",
    "    if radio == \"1.Greedy\":\n",
    "        out = out1\n",
    "    elif radio == \"2.Sampling\":\n",
    "        out = out2\n",
    "    elif radio == \"3.Beam Search\":\n",
    "        out = out3\n",
    "    else:\n",
    "        out = out4\n",
    "    return out, out, out\n",
    "```\n",
    "```python\n",
    "def generate_next(now_text, radio, max_length, num_beams, p):\n",
    "    \"\"\"続き生成\n",
    "\n",
    "    これまで出力したテキストを入力して受け取り、続きを生成する。\n",
    "    デコード方法を指定することができるが、そのパラメタは初回のテキスト生成と同じになる。\n",
    "\n",
    "    Args:\n",
    "        str: Stateから取得（続きを生成するためのプロンプト）\n",
    "        str: Radioから取得（使用するデコード方法の名前）\n",
    "        int: Sliderから取得（初回のテキスト生成で使用した値をここでも使用）\n",
    "        int: Sliderから取得（初回のテキスト生成で使用した値をここでも使用）\n",
    "        int: Sliderから取得（初回のテキスト生成で使用した値をここでも使用）\n",
    "    \n",
    "    Returns:\n",
    "        str: State（生成結果を入出力の状態に反映）\n",
    "        str: TextArea（全文表示用のコンポーネントで使用）\n",
    "        str: TextArea（今回生成した文を表示するコンポーネントで使用）\n",
    "\n",
    "    Todo:\n",
    "        * generate_config, inputsを実装\n",
    "    \"\"\"\n",
    "    generate_config = ...\n",
    "    # テキスト生成\n",
    "    inputs = ...\n",
    "    output = model.generate(inputs, generation_config=generate_config)\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # 結果の整形処理\n",
    "    next_text = \"。\\n\".join(generated.replace(\" \", \"\").split(\"。\"))\n",
    "    gen_text = next_text[len(now_text)+1:]  # 今回生成したテキストを抽出\n",
    "\n",
    "    return next_text, next_text, gen_text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_out1(out1):\n",
    "    \"\"\"out1が生成された時に、out1を後続の処理のデフォルト値に入力\n",
    "    \"\"\"\n",
    "    return out1, out1, out1\n",
    "\n",
    "def select_out(radio, out1, out2, out3, out4):\n",
    "    \"\"\"後続の処理に使用する、初回の処理結果を選択する\n",
    "    \"\"\"\n",
    "    if radio == \"1.Greedy\":\n",
    "        out = out1\n",
    "    elif radio == \"2.Sampling\":\n",
    "        out = out2\n",
    "    elif radio == \"3.Beam Search\":\n",
    "        out = out3\n",
    "    else:\n",
    "        out = out4\n",
    "    return out, out, out\n",
    "\n",
    "def generate_next(now_text, radio, max_length, num_beams, p):\n",
    "    \"\"\"続き生成\n",
    "\n",
    "    これまで出力したテキストを入力して受け取り、続きを生成する。\n",
    "    デコード方法を指定することができるが、そのパラメタは初回のテキスト生成と同じになる。\n",
    "\n",
    "    Args:\n",
    "        str: Stateから取得（続きを生成するためのプロンプト）\n",
    "        str: Radioから取得（使用するデコード方法の名前）\n",
    "        int: Sliderから取得（初回のテキスト生成で使用した値をここでも使用）\n",
    "        int: Sliderから取得（初回のテキスト生成で使用した値をここでも使用）\n",
    "        int: Sliderから取得（初回のテキスト生成で使用した値をここでも使用）\n",
    "    \n",
    "    Returns:\n",
    "        str: State（生成結果を入出力の状態に反映）\n",
    "        str: TextArea（全文表示用のコンポーネントで使用）\n",
    "        str: TextArea（今回生成した文を表示するコンポーネントで使用）\n",
    "\n",
    "    Todo:\n",
    "        * generate_config, inputsを実装\n",
    "    \"\"\"\n",
    "    generate_config = ...\n",
    "    # テキスト生成\n",
    "    inputs = ...\n",
    "    output = model.generate(inputs, generation_config=generate_config)\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # 結果の整形処理\n",
    "    next_text = \"。\\n\".join(generated.replace(\" \", \"\").split(\"。\"))\n",
    "    gen_text = next_text[len(now_text)+1:]  # 今回生成したテキストを抽出\n",
    "\n",
    "    return next_text, next_text, gen_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradio appの実装\n",
    "- UIの実装パート（課題対象）\n",
    "- イベント処理の実装パート（課題対象）\n",
    "\n",
    "課題１のコードを使って、二個目の`with gr.Row()`から実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GradioによるUI/イベント処理の定義\n",
    "with gr.Blocks() as demo:\n",
    "    # 2.1. UI\n",
    "    gr.Markdown('''\n",
    "    # テキスト生成\n",
    "    テキストを入力すると、４パターンのデコード方法でテキスト生成を実行します。\n",
    "    ## ４つのパターン（入門編）\n",
    "    1. Greedy: ビームサーチもサンプリングも行いません。毎回、最も確率の高い単語を選択します。\n",
    "    2. Sampling: モデルによって与えられた語彙全体の確率分布に基づいて次の単語を選択します。\n",
    "    3. Beam Search: 各タイムステップで複数の仮説を保持し、最終的に仮説ごとのシーケンス全体で最も高い確率を持つ仮説を選択します。\n",
    "    4. Top-p Sampling: 2の方法に関して、確率の和がpになる最小の単語にフィルタリングすることで、確率が低い単語が選ばれる可能性を無くします。\n",
    "    ''')\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = ...  # テキストを入力するコンポーネント\n",
    "            max_length = ...  # 数値を入力するコンポーネント.参考：（min=100, max=1000, step=100, default=100）\n",
    "            num_beams = ...  # 数値を入力するコンポーネント.参考：（min=1, max=10, step=1, default=6）\n",
    "            p = ...    # テキストを入力するコンポーネント.参考：（min=0, max=1, step=0.01, default=0.92）\n",
    "            btn = gr.Button(\"Decode\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            out1 = ...  # Greedy decode outputを表示するコンポーネント\n",
    "            out2 = ...  # Smapling decode outputを表示するコンポーネント\n",
    "            out3 = ...  # Beam Search decode outputを表示するコンポーネント\n",
    "            out4 = ...  # Top-p Sampling decode outputを表示するコンポーネント\n",
    "\n",
    "    with gr.Row():\n",
    "        pass  # レイアウト（gr.Column()の使い方）\n",
    "        pass  # 初回の結果としてout1を表示させる\n",
    "        pass  # どの結果を使うか、radioで選択できる\n",
    "\n",
    "        pass  # 生成した続きを、これまでの結果を含めて、表示する\n",
    "        pass  # どのデコード方法で、続きを生成するか、radioで選択する  \n",
    "        pass  # 生成した続きを、これまでの結果を含めずに、表示する\n",
    "\n",
    "    # 2.2 イベント処理\n",
    "    btn.click(fn=generate, inputs=[input_text, max_length, num_beams, p], outputs=[out1, out2, out3, out4])\n",
    "    pass  # out1が変化した時の処理\n",
    "    pass  # radioが変化した時の処理\n",
    "    pass  # 続きを生成する処理\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradio appの立ち上げ\n",
    "https://localhost:7860/\n",
    "（もしくは出力に表示された別のURL）をクリックして、アクセスしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "デモ実装：テキスト生成",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
