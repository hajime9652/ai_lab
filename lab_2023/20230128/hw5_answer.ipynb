{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install gensim polars\n",
        "# !pip install fugashi[unidic]\n",
        "# !python -m unidic download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 演習\n",
        "## LDAモデルを使って、類似文書の検索\n",
        "1. livedoor newsコーパスのカテゴリーごとに８：２の割合でデータセットを学習用・テスト用に分割してください\n",
        "1. トピック数を2~10の中で、最もパープレキシティの低いLDAモデルを作ってください\n",
        "1. 各カテゴリーごとのテストデータを入力として使って、LDAモデルによって最も類似度の高い文書を学習データから選定してください。\n",
        "1. 上記で、選定されたデータが入力したデータと同じカテゴリーかどうかを判定してください。同じカテゴリーの場合は成功とします。\n",
        "1. カテゴリーごとに成功率を計算してください。\n",
        "\n",
        "## word2vec/k-meansとLDAの比較\n",
        "1. 上記で学習させたLDAの各トピックの上位10個ずつ単語を抽出してください。\n",
        "1. これらの単語をword2vecで単語ベクトルに変換してください。\n",
        "1. これらの単語ベクトル集合をk-meansでクラスタリングしてください。ただし、k-meansのクラスタ数はLDAのトピック数と同じにしてください。\n",
        "1. 1で抽出したLDAのトピックの単語集合とk-meansのクラスタの単語集合を比較してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import polars as pl\n",
        "\n",
        "# Load the livedoor news corpus\n",
        "path_to_corpus = '../../text'  # 事前にDLして解凍が必要（https://www.rondhuit.com/download.html）\n",
        "data = {}\n",
        "train_cat, train_url, train_date, train_title, train_documents = [], [], [], [], []\n",
        "test_cat, test_url, test_date, test_title, test_documents = [], [], [], [], []\n",
        "for category in os.listdir(path_to_corpus):\n",
        "    if category in ['CHANGES.txt', 'README.txt']:\n",
        "        continue\n",
        "    category_path = os.path.join(path_to_corpus, category)\n",
        "    for i, file in enumerate(os.listdir(category_path)):\n",
        "        if file in ['LICENSE.txt']:\n",
        "            continue\n",
        "        if i < len(os.listdir(category_path))*0.8:\n",
        "            file_path = os.path.join(category_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                train_cat.append(category_path.split(\"/\")[-1])\n",
        "                f.readline()  # １行目：記事のURL\n",
        "                f.readline()  # ２行目：記事の日付\n",
        "                f.readline()  # ３行目：記事のタイトル\n",
        "                train_documents.append(f.read())  # ４行目以降：記事の本文\n",
        "        else:\n",
        "            file_path = os.path.join(category_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                test_cat.append(category_path.split(\"/\")[-1])\n",
        "                f.readline()  # １行目：記事のURL\n",
        "                f.readline()  # ２行目：記事の日付\n",
        "                f.readline()  # ３行目：記事のタイトル\n",
        "                test_documents.append(f.read())  # ４行目以降：記事の本文\n",
        "\n",
        "\n",
        "df_train = pl.DataFrame({\"CATEGORY\": train_cat, \"DOCUMENT\": train_documents})\n",
        "df_test = pl.DataFrame({\"CATEGORY\": test_cat, \"DOCUMENT\": test_documents})\n",
        "df_train.write_csv(\"raw_corpus_train.csv\")\n",
        "df_test.write_csv(\"raw_corpus_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8012759603637845"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.shape[0] / (df_train.shape[0] + df_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape: (1, 2)\n",
            "┌─────────────┬─────────────────────────────────────────────────────────┐\n",
            "│ CATEGORY    ┆ DOCUMENT                                                │\n",
            "│ ---         ┆ ---                                                     │\n",
            "│ str         ┆ str                                                     │\n",
            "╞═════════════╪═════════════════════════════════════════════════════════╡\n",
            "│ movie-enter ┆ \"　2005年11月から翌2006年7月まで読売新聞にて連載され... │\n",
            "└─────────────┴─────────────────────────────────────────────────────────┘\n"
          ]
        }
      ],
      "source": [
        "# with open(\"raw_corpus_train.csv\", 'r') as f:\n",
        "#     header = f.readline()\n",
        "#     first_line = f.readline()\n",
        "#     ddf = pl.DataFrame({header.split(',')[0]:first_line.split(',')[0], header.split(',')[1].strip():''.join(first_line.split(',')[1:])} )\n",
        "#     print(ddf)\n",
        "#     for line in f.readlines():\n",
        "#         ddf = pl.concat([ddf, pl.DataFrame({header.split(',')[0]:line.split(',')[0], header.split(',')[1].strip():''.join(line.split(',')[1:])})])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "import polars as pl\n",
        "from fugashi import Tagger\n",
        "from gensim.corpora import Dictionary, MmCorpus\n",
        "from gensim import models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class LivedoorCorpus():\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "        # 全角半角文字以外（記号と数字）を正規表現を使って除去\n",
        "        pattern = r\"[^\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff\\u20000-\\u2ffff\\sa-zA-Z]\"\n",
        "        self.raw_documents = [re.sub(pattern, \"\", text) for text in self.df[\"DOCUMENT\"]]\n",
        "        # Mecabで分かち書きして、単語に分割\n",
        "        self.raw_documents = [Tagger('-Owakati').parse(text).split() for text in self.raw_documents]\n",
        "        # ストップワードの除去\n",
        "        self.raw_documents = self._rm_stopwords()\n",
        "        # 1文字は除去\n",
        "        self.raw_documents = [[word for word in text if len(word) > 1]for text in self.raw_documents]\n",
        "\n",
        "        self.dictionary = Dictionary(self.raw_documents)\n",
        "\n",
        "        self.bow = [ self.dictionary.doc2bow(text) for text in self.raw_documents]\n",
        "\n",
        "\n",
        "    def reset_dict_corpus(self):\n",
        "        self.dictionary = Dictionary(self.raw_documents)\n",
        "        self.bow = [ self.dictionary.doc2bow(text) for text in self.raw_documents]\n",
        "\n",
        "    def print_stats(self):\n",
        "        print(f\"文書数: {self.dictionary.num_docs}, \" + f\"語彙数: {len(self.dictionary)}\")\n",
        "\n",
        "    def dict_top_n(self, top_n: int):\n",
        "        most_frequent_ids = (v for v in self.dictionary)\n",
        "        most_frequent_ids = sorted(most_frequent_ids, key=self.dictionary.dfs.get, reverse=True)\n",
        "        most_frequent_ids = most_frequent_ids[:top_n]\n",
        "        return [self.dictionary[idx] for idx in most_frequent_ids]\n",
        "        \n",
        "    def _rm_stopwords(self):\n",
        "        # ストップワードの準備\n",
        "        stopwords_url = \"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\"\n",
        "        r = requests.get(stopwords_url)\n",
        "        tmp = r.text.split('\\r\\n')\n",
        "        stopwords = []\n",
        "        for i in range(len(tmp)):\n",
        "            if len(tmp[i]) < 1:\n",
        "                continue\n",
        "            stopwords.append(tmp[i])\n",
        "\n",
        "        return [[word for word in text if not word in stopwords]for text in self.raw_documents]\n",
        "\n",
        "# CSVが読み込めなかったため、dfを与えるようにしています。\n",
        "train_corpus = LivedoorCorpus(df_train)\n",
        "test_corpus = LivedoorCorpus(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "文書数: 5903, 語彙数: 64506\n",
            "文書数: 5903, 語彙数: 11877\n"
          ]
        }
      ],
      "source": [
        "train_corpus.reset_dict_corpus()\n",
        "train_corpus.print_stats()\n",
        "train_corpus.dictionary.filter_extremes(10,0.5)\n",
        "train_corpus.print_stats()\n",
        "train_bow = [ train_corpus.dictionary.doc2bow(text) for text in train_corpus.raw_documents]\n",
        "# 辞書はtrain_corpusのものを使います。\n",
        "test_bow = [ train_corpus.dictionary.doc2bow(text) for text in test_corpus.raw_documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: training/test = 5903/1464\n",
            "2: perplexity is 3288.4388309458286/3677.975380846817\n",
            "3: perplexity is 3187.0854507084205/3692.974381613391\n",
            "4: perplexity is 2947.683211806905/3566.3653970241276\n",
            "5: perplexity is 2910.6477622901984/3640.1911254850593\n",
            "6: perplexity is 2928.6140713124923/3800.1268442208543\n",
            "7: perplexity is 2758.812666199278/3655.988510858198\n",
            "8: perplexity is 2823.2095331111045/3822.6017413838867\n",
            "9: perplexity is 2840.7071136796785/3944.568517727349\n",
            "Best model: topics=4, perplexity=3566.3653970241276\n"
          ]
        }
      ],
      "source": [
        "topic_range = range(2, 10)\n",
        "\n",
        "def calc_perplexity(m, c):\n",
        "    import numpy as np\n",
        "    return np.exp(-m.log_perplexity(c))\n",
        "\n",
        "def search_model(corpus_train, corpus_test):\n",
        "    most = [1.0e6, None]\n",
        "    print(f\"dataset: training/test = {len(corpus_train)}/{len(corpus_test)}\")\n",
        "\n",
        "    for t in topic_range:\n",
        "        # 辞書はtrain_corpusのものを使います。\n",
        "        m = models.LdaModel(corpus=corpus_train, id2word=train_corpus.dictionary, num_topics=t, iterations=500, passes=5)\n",
        "        p1 = calc_perplexity(m, corpus_train)\n",
        "        p2 = calc_perplexity(m, corpus_test)\n",
        "        print(f\"{t}: perplexity is {p1}/{p2}\")\n",
        "        \n",
        "        if p2 < most[0]:\n",
        "            most[0] = p2\n",
        "            most[1] = m\n",
        "    \n",
        "    return most[0], most[1]\n",
        "\n",
        "perplexity, model_lda = search_model(train_bow, test_bow)\n",
        "print(f\"Best model: topics={model_lda.num_topics}, perplexity={perplexity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim import similarities\n",
        "index = similarities.MatrixSimilarity(model_lda[train_bow], num_features=len(train_corpus.dictionary))\n",
        "\n",
        "acc = []\n",
        "loss = []\n",
        "for i, doc in enumerate(test_bow):\n",
        "    similarity_lda = index[doc]\n",
        "    if df_train[int(np.argmax(similarity_lda))][\"CATEGORY\"][0] == df_test[i][\"CATEGORY\"][0]:\n",
        "        acc.append([df_train[int(np.argmax(similarity_lda))][\"DOCUMENT\"][0], df_test[i][\"DOCUMENT\"][0]])\n",
        "    else:\n",
        "        loss.append([df_train[int(np.argmax(similarity_lda))][\"DOCUMENT\"][0], df_test[i][\"DOCUMENT\"][0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.09631147540983606"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(acc)/(len(acc) + len(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-27 00:57:12,805 : INFO : collecting all words and their counts\n",
            "2023-01-27 00:57:12,808 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2023-01-27 00:57:13,687 : INFO : collected 64506 word types from a corpus of 1657278 raw words and 5903 sentences\n",
            "2023-01-27 00:57:13,688 : INFO : Creating a fresh vocabulary\n",
            "2023-01-27 00:57:13,886 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 64506 unique words (100.00% of original 64506, drops 0)', 'datetime': '2023-01-27T00:57:13.886065', 'gensim': '4.3.0', 'python': '3.9.6 (default, Sep 26 2022, 11:37:49) \\n[Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
            "2023-01-27 00:57:13,887 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 1657278 word corpus (100.00% of original 1657278, drops 0)', 'datetime': '2023-01-27T00:57:13.887305', 'gensim': '4.3.0', 'python': '3.9.6 (default, Sep 26 2022, 11:37:49) \\n[Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
            "2023-01-27 00:57:14,116 : INFO : deleting the raw counts dictionary of 64506 items\n",
            "2023-01-27 00:57:14,117 : INFO : sample=0.001 downsamples 19 most-common words\n",
            "2023-01-27 00:57:14,118 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1569291.1666314108 word corpus (94.7%% of prior 1657278)', 'datetime': '2023-01-27T00:57:14.118925', 'gensim': '4.3.0', 'python': '3.9.6 (default, Sep 26 2022, 11:37:49) \\n[Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
            "2023-01-27 00:57:14,468 : INFO : estimated required memory for 64506 words and 100 dimensions: 83857800 bytes\n",
            "2023-01-27 00:57:14,468 : INFO : resetting layer weights\n",
            "2023-01-27 00:57:14,505 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-01-27T00:57:14.505118', 'gensim': '4.3.0', 'python': '3.9.6 (default, Sep 26 2022, 11:37:49) \\n[Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
            "2023-01-27 00:57:14,505 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 64506 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-01-27T00:57:14.505698', 'gensim': '4.3.0', 'python': '3.9.6 (default, Sep 26 2022, 11:37:49) \\n[Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'train'}\n",
            "2023-01-27 00:57:15,571 : INFO : EPOCH 0 - PROGRESS: at 20.29% examples, 342812 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:16,589 : INFO : EPOCH 0 - PROGRESS: at 44.89% examples, 306435 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:17,605 : INFO : EPOCH 0 - PROGRESS: at 64.76% examples, 316429 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:18,639 : INFO : EPOCH 0 - PROGRESS: at 89.33% examples, 321641 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:19,374 : INFO : EPOCH 0: training on 1657278 raw words (1569073 effective words) took 4.9s, 322535 effective words/s\n",
            "2023-01-27 00:57:20,403 : INFO : EPOCH 1 - PROGRESS: at 18.80% examples, 328479 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:21,414 : INFO : EPOCH 1 - PROGRESS: at 48.20% examples, 339757 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:22,423 : INFO : EPOCH 1 - PROGRESS: at 73.22% examples, 345894 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:23,424 : INFO : EPOCH 1 - PROGRESS: at 91.06% examples, 337328 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:23,990 : INFO : EPOCH 1: training on 1657278 raw words (1568963 effective words) took 4.6s, 340025 effective words/s\n",
            "2023-01-27 00:57:24,996 : INFO : EPOCH 2 - PROGRESS: at 19.82% examples, 355504 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:26,056 : INFO : EPOCH 2 - PROGRESS: at 49.62% examples, 349410 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:27,058 : INFO : EPOCH 2 - PROGRESS: at 75.25% examples, 350201 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:28,121 : INFO : EPOCH 2 - PROGRESS: at 95.00% examples, 351373 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:28,432 : INFO : EPOCH 2: training on 1657278 raw words (1569524 effective words) took 4.4s, 353707 effective words/s\n",
            "2023-01-27 00:57:29,439 : INFO : EPOCH 3 - PROGRESS: at 19.31% examples, 345647 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:30,441 : INFO : EPOCH 3 - PROGRESS: at 49.13% examples, 354656 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:31,473 : INFO : EPOCH 3 - PROGRESS: at 75.25% examples, 353279 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:32,477 : INFO : EPOCH 3 - PROGRESS: at 94.53% examples, 356472 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:32,845 : INFO : EPOCH 3: training on 1657278 raw words (1569354 effective words) took 4.4s, 355936 effective words/s\n",
            "2023-01-27 00:57:33,849 : INFO : EPOCH 4 - PROGRESS: at 19.31% examples, 346584 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:34,910 : INFO : EPOCH 4 - PROGRESS: at 49.62% examples, 349436 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:35,976 : INFO : EPOCH 4 - PROGRESS: at 77.40% examples, 351993 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:37,032 : INFO : EPOCH 4 - PROGRESS: at 96.21% examples, 353226 words/s, in_qsize 5, out_qsize 0\n",
            "2023-01-27 00:57:37,272 : INFO : EPOCH 4: training on 1657278 raw words (1569402 effective words) took 4.4s, 354802 effective words/s\n",
            "2023-01-27 00:57:37,273 : INFO : Word2Vec lifecycle event {'msg': 'training on 8286390 raw words (7846316 effective words) took 22.8s, 344641 effective words/s', 'datetime': '2023-01-27T00:57:37.273133', 'gensim': '4.3.0', 'python': '3.9.6 (default, Sep 26 2022, 11:37:49) \\n[Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'train'}\n",
            "2023-01-27 00:57:37,273 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=64506, vector_size=100, alpha=0.025>', 'datetime': '2023-01-27T00:57:37.273603', 'gensim': '4.3.0', 'python': '3.9.6 (default, Sep 26 2022, 11:37:49) \\n[Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'created'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25655160.0\n"
          ]
        }
      ],
      "source": [
        "class WVCorpus():\n",
        "    def __init__(self, corpus):\n",
        "        self.corpus = corpus\n",
        "    def __iter__(self):\n",
        "        return iter(self.corpus)\n",
        "\n",
        "sentences = WVCorpus(train_corpus.raw_documents)\n",
        "# instantiating and training the Word2Vec model\n",
        "model_wv = models.Word2Vec(\n",
        "    sentences,\n",
        "    min_count=1,\n",
        "    compute_loss=True,\n",
        "    hs=0,\n",
        "    sg=1,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# getting the training loss value\n",
        "training_loss = model_wv.get_latest_training_loss()\n",
        "print(training_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape: (10, 4)\n",
            "┌──────┬──────────────┬──────────┬──────┐\n",
            "│ 0    ┆ 1            ┆ 2        ┆ 3    │\n",
            "│ ---  ┆ ---          ┆ ---      ┆ ---  │\n",
            "│ str  ┆ str          ┆ str      ┆ str  │\n",
            "╞══════╪══════════════╪══════════╪══════╡\n",
            "│ 映画 ┆ 日本         ┆ スマート ┆ 女性 │\n",
            "│ 日本 ┆ 写真         ┆ アプリ   ┆ って │\n",
            "│ 監督 ┆ キャンペーン ┆ フォン   ┆ たい │\n",
            "│ 選手 ┆ 応募         ┆ できる   ┆ いい │\n",
            "│ ...  ┆ ...          ┆ ...      ┆ ...  │\n",
            "│ 作品 ┆ 発表         ┆ 機能     ┆ だけ │\n",
            "│ 世界 ┆ より         ┆ AX       ┆ たら │\n",
            "│ 放送 ┆ 開催         ┆ Android  ┆ たり │\n",
            "│ 番組 ┆ サイト       ┆ SM       ┆ あり │\n",
            "└──────┴──────────────┴──────────┴──────┘\n",
            "0 ['映画', '監督', '公開', '作品', '番組', '写真']\n",
            "1 ['だっ', '世界', '東京', 'より', 'サイト', 'できる', '女性', 'って', 'たい', 'いい', 'なく', '結婚', 'だけ', 'たら', 'たり', 'あり']\n",
            "2 ['キャンペーン', '応募', '2012', '発表', '開催', 'スマート', 'アプリ', 'フォン', '対応', '更新', '機能', 'AX', 'Android', 'SM']\n",
            "3 ['日本', '選手', '放送', '日本']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hajime/projects/ai_lab/.env/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "topic_words = []\n",
        "for t in range(model_lda.num_topics):\n",
        "    x = model_lda.show_topic(t, 10)\n",
        "    topic_words.append([(t, w[0]) for w in x])\n",
        "\n",
        "topic_df = pl.DataFrame({str(x[0][0]):[word[1] for word in x] for x in topic_words})\n",
        "topic_words = np.ravel([[word[1] for word in x] for x in topic_words])\n",
        "\n",
        "word_vectors = [model_wv.wv[x] for x in topic_words]\n",
        "\n",
        "kmeans_clustering = KMeans(n_clusters = model_lda.num_topics)\n",
        "idx = kmeans_clustering.fit_predict(word_vectors)\n",
        "\n",
        "y = []\n",
        "for t in range(model_lda.num_topics):\n",
        "    _y = []\n",
        "    for i, id in enumerate(idx):\n",
        "        if t == id:\n",
        "            _y.append((t, topic_words[i]))\n",
        "    y.append(_y)\n",
        "\n",
        "\n",
        "cluster_words = {str(w[0][0]):[word[1] for word in w] for w in y}\n",
        "print(topic_df)\n",
        "for k,v in cluster_words.items():\n",
        "    print(k,v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\n",
              "    .dataframe td {\n",
              "        white-space: pre;\n",
              "    }\n",
              "\n",
              "    .dataframe td {\n",
              "        padding-top: 0;\n",
              "    }\n",
              "\n",
              "    .dataframe td {\n",
              "        padding-bottom: 0;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "<small>shape: (10, 4)</small>\n",
              "<thead>\n",
              "<tr>\n",
              "<th>\n",
              "0\n",
              "</th>\n",
              "<th>\n",
              "1\n",
              "</th>\n",
              "<th>\n",
              "2\n",
              "</th>\n",
              "<th>\n",
              "3\n",
              "</th>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "str\n",
              "</td>\n",
              "<td>\n",
              "str\n",
              "</td>\n",
              "<td>\n",
              "str\n",
              "</td>\n",
              "<td>\n",
              "str\n",
              "</td>\n",
              "</tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;映画&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;日本&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;スマート&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;女性&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;日本&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;写真&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;アプリ&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;って&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;監督&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;キャンペーン&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;フォン&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;たい&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;選手&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;応募&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;できる&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;いい&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;だっ&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;2012&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;対応&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;なく&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;公開&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;東京&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;更新&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;結婚&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;作品&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;発表&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;機能&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;だけ&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;世界&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;より&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;AX&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;たら&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;放送&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;開催&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;Android&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;たり&quot;\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;番組&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;サイト&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;SM&quot;\n",
              "</td>\n",
              "<td>\n",
              "&quot;あり&quot;\n",
              "</td>\n",
              "</tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "shape: (10, 4)\n",
              "┌──────┬──────────────┬──────────┬──────┐\n",
              "│ 0    ┆ 1            ┆ 2        ┆ 3    │\n",
              "│ ---  ┆ ---          ┆ ---      ┆ ---  │\n",
              "│ str  ┆ str          ┆ str      ┆ str  │\n",
              "╞══════╪══════════════╪══════════╪══════╡\n",
              "│ 映画 ┆ 日本         ┆ スマート ┆ 女性 │\n",
              "│ 日本 ┆ 写真         ┆ アプリ   ┆ って │\n",
              "│ 監督 ┆ キャンペーン ┆ フォン   ┆ たい │\n",
              "│ 選手 ┆ 応募         ┆ できる   ┆ いい │\n",
              "│ ...  ┆ ...          ┆ ...      ┆ ...  │\n",
              "│ 作品 ┆ 発表         ┆ 機能     ┆ だけ │\n",
              "│ 世界 ┆ より         ┆ AX       ┆ たら │\n",
              "│ 放送 ┆ 開催         ┆ Android  ┆ たり │\n",
              "│ 番組 ┆ サイト       ┆ SM       ┆ あり │\n",
              "└──────┴──────────────┴──────────┴──────┘"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 ['映画', '監督', '公開', '作品', '番組', '写真']\n",
            "1 ['日本', '世界', '放送', '日本', 'キャンペーン', '応募', '2012', '東京', '発表', '開催', 'サイト', 'アプリ', '対応', '更新']\n",
            "2 ['選手', 'だっ', 'より', 'できる', '機能', '女性', 'って', 'たい', 'いい', 'なく', '結婚', 'だけ', 'たら', 'たり', 'あり']\n",
            "3 ['スマート', 'フォン', 'AX', 'Android', 'SM']\n"
          ]
        }
      ],
      "source": [
        "for k,v in cluster_words.items():\n",
        "    print(k,v)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "hw5-answer",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "82e52f2285c871893785b6211b1b0fb91a1e90d57630bc3da094798a3ae6fb15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
