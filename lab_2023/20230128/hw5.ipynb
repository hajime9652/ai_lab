{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install gensim polars\n",
        "# !pip install fugashi[unidic]\n",
        "# !python -m unidic download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "# !tar -xzvf ldcc-20140209.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 演習\n",
        "## LDAモデルを使って、類似文書の検索\n",
        "1. livedoor newsコーパスのカテゴリーごとに８：２の割合でデータセットを学習用・テスト用に分割してください\n",
        "1. トピック数を2~10の中で、最もパープレキシティの低いLDAモデルを作ってください\n",
        "1. 各カテゴリーごとのテストデータを入力として使って、LDAモデルによって最も類似度の高い文書を学習データから選定してください。\n",
        "1. 上記で、選定されたデータが入力したデータと同じカテゴリーかどうかを判定してください。同じカテゴリーの場合は成功とします。\n",
        "1. カテゴリーごとに成功率を計算してください。\n",
        "\n",
        "## word2vec/k-meansとLDAの比較\n",
        "1. 上記で学習させたLDAの各トピックの上位10個ずつ単語を抽出してください。\n",
        "1. これらの単語をword2vecで単語ベクトルに変換してください。\n",
        "1. これらの単語ベクトル集合をk-meansでクラスタリングしてください。ただし、k-meansのクラスタ数はLDAのトピック数と同じにしてください。\n",
        "1. 1で抽出したLDAのトピックの単語集合とk-meansのクラスタの単語集合を比較してください。\n",
        "\n",
        "k-meansはこちら：https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import polars as pl\n",
        "\n",
        "# Load the livedoor news corpus\n",
        "# path_to_corpus = '../../text'  # 事前にDLして解凍が必要（https://www.rondhuit.com/download.html）\n",
        "path_to_corpus = './text'  # 事前にDLして解凍が必要（https://www.rondhuit.com/download.html）\n",
        "data = {}\n",
        "train_cat, train_url, train_date, train_title, train_documents = [], [], [], [], []\n",
        "test_cat, test_url, test_date, test_title, test_documents = [], [], [], [], []\n",
        "for category in os.listdir(path_to_corpus):\n",
        "    if category in ['CHANGES.txt', 'README.txt']:\n",
        "        continue\n",
        "    category_path = os.path.join(path_to_corpus, category)\n",
        "    for i, file in enumerate(os.listdir(category_path)):\n",
        "        if file in ['LICENSE.txt']:\n",
        "            continue\n",
        "        if i < len(os.listdir(category_path))*0.8:\n",
        "            file_path = os.path.join(category_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                train_cat.append(category_path.split(\"/\")[-1])\n",
        "                f.readline()  # １行目：記事のURL\n",
        "                f.readline()  # ２行目：記事の日付\n",
        "                f.readline()  # ３行目：記事のタイトル\n",
        "                train_documents.append(f.read())  # ４行目以降：記事の本文\n",
        "        else:\n",
        "            file_path = os.path.join(category_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                test_cat.append(category_path.split(\"/\")[-1])\n",
        "                f.readline()  # １行目：記事のURL\n",
        "                f.readline()  # ２行目：記事の日付\n",
        "                f.readline()  # ３行目：記事のタイトル\n",
        "                test_documents.append(f.read())  # ４行目以降：記事の本文\n",
        "\n",
        "\n",
        "df_train = pl.DataFrame({\"CATEGORY\": train_cat, \"DOCUMENT\": train_documents})\n",
        "df_test = pl.DataFrame({\"CATEGORY\": test_cat, \"DOCUMENT\": test_documents})\n",
        "df_train.write_csv(\"raw_corpus_train.csv\")\n",
        "df_test.write_csv(\"raw_corpus_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.shape[0] / (df_train.shape[0] + df_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "import polars as pl\n",
        "from fugashi import Tagger\n",
        "from gensim.corpora import Dictionary, MmCorpus\n",
        "from gensim import models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class LivedoorCorpus():\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "        # 全角半角文字以外（記号と数字）を正規表現を使って除去\n",
        "        pattern = r\"[^\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff\\u20000-\\u2ffff\\sa-zA-Z]\"\n",
        "        self.raw_documents = [re.sub(pattern, \"\", text) for text in self.df[\"DOCUMENT\"]]\n",
        "        # Mecabで分かち書きして、単語に分割\n",
        "        self.raw_documents = [Tagger('-Owakati').parse(text).split() for text in self.raw_documents]\n",
        "        # ストップワードの除去\n",
        "        self.raw_documents = self._rm_stopwords()\n",
        "        # 1文字は除去\n",
        "        self.raw_documents = [[word for word in text if len(word) > 1]for text in self.raw_documents]\n",
        "\n",
        "        self.dictionary = Dictionary(self.raw_documents)\n",
        "\n",
        "        self.bow = [ self.dictionary.doc2bow(text) for text in self.raw_documents]\n",
        "\n",
        "\n",
        "    def reset_dict_corpus(self):\n",
        "        self.dictionary = Dictionary(self.raw_documents)\n",
        "        self.bow = [ self.dictionary.doc2bow(text) for text in self.raw_documents]\n",
        "\n",
        "    def print_stats(self):\n",
        "        print(f\"文書数: {self.dictionary.num_docs}, \" + f\"語彙数: {len(self.dictionary)}\")\n",
        "\n",
        "    def dict_top_n(self, top_n: int):\n",
        "        most_frequent_ids = (v for v in self.dictionary)\n",
        "        most_frequent_ids = sorted(most_frequent_ids, key=self.dictionary.dfs.get, reverse=True)\n",
        "        most_frequent_ids = most_frequent_ids[:top_n]\n",
        "        return [self.dictionary[idx] for idx in most_frequent_ids]\n",
        "        \n",
        "    def _rm_stopwords(self):\n",
        "        # ストップワードの準備\n",
        "        stopwords_url = \"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\"\n",
        "        r = requests.get(stopwords_url)\n",
        "        tmp = r.text.split('\\r\\n')\n",
        "        stopwords = []\n",
        "        for i in range(len(tmp)):\n",
        "            if len(tmp[i]) < 1:\n",
        "                continue\n",
        "            stopwords.append(tmp[i])\n",
        "\n",
        "        return [[word for word in text if not word in stopwords]for text in self.raw_documents]\n",
        "\n",
        "# CSVが読み込めなかったため、dfを与えるようにしています。\n",
        "train_corpus = LivedoorCorpus(df_train)\n",
        "test_corpus = LivedoorCorpus(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_corpus.reset_dict_corpus()\n",
        "train_corpus.print_stats()\n",
        "train_corpus.dictionary.filter_extremes(10,0.5)\n",
        "train_corpus.print_stats()\n",
        "train_bow = [ train_corpus.dictionary.doc2bow(text) for text in train_corpus.raw_documents]\n",
        "# 辞書はtrain_corpusのものを使います。\n",
        "test_bow = [ train_corpus.dictionary.doc2bow(text) for text in test_corpus.raw_documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# メモリ不足や遅れている場合に使用\n",
        "# import pickle\n",
        "# with open('train_corous.pkl', 'wb') as f:\n",
        "#     pickle.dump(train_corpus, f)\n",
        "# train_corpus.dictionary.save('livedoor.dict')\n",
        "# MmCorpus.serialize('./train_bow.mm', train_bow)\n",
        "# MmCorpus.serialize('./test_bow.mm', test_bow)\n",
        "\n",
        "# with open('train_corpus.pkl', 'rb') as f:\n",
        "#     corpus = pickle.load(f)\n",
        "# dictionary = Dictionary.load('livedoor.dict')\n",
        "# doc_train = MmCorpus('train_bow.mm')\n",
        "# doc_test = MmCorpus('test_bow.mm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_range = ...\n",
        "\n",
        "def calc_perplexity(m, c):\n",
        "    import numpy as np\n",
        "    return None\n",
        "\n",
        "def search_model(corpus_train, corpus_test):\n",
        "    most = [1.0e6, None]\n",
        "    print(f\"dataset: training/test = {len(corpus_train)}/{len(corpus_test)}\")\n",
        "\n",
        "    for t in topic_range:\n",
        "        # 辞書はtrain_corpusのものを使います。\n",
        "        m = ...\n",
        "        p1 = calc_perplexity(m, corpus_train)\n",
        "        p2 = calc_perplexity(m, corpus_test)\n",
        "        print(f\"{t}: perplexity is {p1}/{p2}\")\n",
        "        \n",
        "        if p2 < most[0]:\n",
        "            most[0] = p2\n",
        "            most[1] = m\n",
        "    \n",
        "    return most[0], most[1]\n",
        "\n",
        "perplexity, model_lda = search_model(train_bow, test_bow)\n",
        "print(f\"Best model: topics={model_lda.num_topics}, perplexity={perplexity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim import similarities\n",
        "index = ...\n",
        "\n",
        "acc = []\n",
        "loss = []\n",
        "for i, doc in enumerate(test_bow):\n",
        "    similarity_lda = ...\n",
        "    raise NotImplementedError(\"判定する処理を書いてください。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(acc)/(len(acc) + len(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WVCorpus():\n",
        "    def __init__(self, corpus):\n",
        "        self.corpus = corpus\n",
        "    def __iter__(self):\n",
        "        return iter(self.corpus)\n",
        "\n",
        "sentences = WVCorpus(train_corpus.raw_documents)\n",
        "# instantiating and training the Word2Vec model\n",
        "model_wv = ...\n",
        "\n",
        "# getting the training loss value\n",
        "training_loss = model_wv.get_latest_training_loss()\n",
        "print(training_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "topic_words = []\n",
        "\n",
        "raise NotImplementedError(\"単語の抽出、単語ベクトルへの変換、クラスタリング、ラベル合わせ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k,v in cluster_words.items():\n",
        "    print(k,v)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "hw5",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "82e52f2285c871893785b6211b1b0fb91a1e90d57630bc3da094798a3ae6fb15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
