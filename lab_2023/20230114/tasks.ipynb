{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "piplineは便利ですが、抽象的でわかりにくいです。\n",
    "そこでタスクの理解を深めるために、piplineを使わないでモデルを使用してみましょう。\n",
    "ここを理解することによって、ファインチューニングの実装が容易になります。\n",
    "\n",
    "## Sentiment Analysis：ポジネガ判定\n",
    "事業評価などに特化したポジネガ判定\n",
    "\n",
    "- Model: [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)\n",
    "- Dataset: [chABSA](https://www.kaggle.com/datasets/takahirokubo0/chabsa)\n",
    "    - アスペクトベース感情分析(ABSA)は、文の極性やその根拠を理解するのに重要である。しかし、そのような重要なデータセットであるにもかかわらず、利用可能な言語やドメインが限られています。本論文では、日本語のABSAデータセット（chABSA-dataset）を提供する。chABSA-datasetは企業分析領域に基づいて構築されており、我々の知る限り、日本語のABSAデータセットとしては初めてのものである。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飼養戸数が全ての畜種で減少する中で、1戸当たりの飼養頭羽数は、いずれの畜種でも増加傾向となっています\n",
      "positive: 100%\n",
      "negative: 0%\n",
      "国産小麦は、収穫期が降雨の時期に重なること、赤カビ等の病害が発生しやすく、また、外国産小麦に比べてタンパク含有量のばらつきが大きいこと等、品質上の課題もあります。\n",
      "positive: 100%\n",
      "negative: 0%\n",
      "新型コロナウイルス感染症からの経済活動の回復・在庫の確保などを受け、令和3年の緑茶の輸出額は204億円と、過去最高額を記録しました。\n",
      "positive: 0%\n",
      "negative: 100%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jarvisx17/japanese-sentiment-analysis\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"jarvisx17/japanese-sentiment-analysis\")\n",
    "\n",
    "classes = [\"positive\", \"negative\"]\n",
    "\n",
    "sequence_0 = \"飼養戸数が全ての畜種で減少する中で、1戸当たりの飼養頭羽数は、いずれの畜種でも増加傾向となっています\"\n",
    "sequence_1 = \"国産小麦は、収穫期が降雨の時期に重なること、赤カビ等の病害が発生しやすく、また、外国産小麦に比べてタンパク含有量のばらつきが大きいこと等、品質上の課題もあります。\"\n",
    "sequence_2 = \"新型コロナウイルス感染症からの経済活動の回復・在庫の確保などを受け、令和3年の緑茶の輸出額は204億円と、過去最高額を記録しました。\"\n",
    "\n",
    "for sequence in [sequence_0, sequence_1, sequence_2]:\n",
    "    input = tokenizer(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    classification_logits = model(**input).logits\n",
    "\n",
    "    results = torch.softmax(classification_logits, dim=1).tolist()[0]\n",
    "    \n",
    "    print(sequence)\n",
    "    for i in range(len(classes)):\n",
    "        print(f\"{classes[i]}: {int(round(results[i] * 100))}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### やっていること\n",
    "1. チェックポイント名からトークナイザーとモデルをインスタンス化します。\n",
    "2. モデルはBERTモデルとして識別され、チェックポイントに格納された重みをロードします。\n",
    "3. 判定対象の文を定義します。\n",
    "4. このシーケンスをモデルに通して、2つの利用可能なクラスのうちの1つに分類されるようにする。0（positive）、1（negative）。\n",
    "5. クラスに対する確率を得るために、結果のソフトマックスを計算する。\n",
    "6. 結果を表示する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering：テキストから回答の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20f49d5d0be4ebea71afbbc53938511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人の質問: 太郎の昨日のご飯は？\n",
      "AIの回答: カレー\n",
      "人の質問: 太郎の今日のご飯は？\n",
      "AIの回答: カレー\n",
      "人の質問: \"太郎は人間ですか？\n",
      "AIの回答: チュール\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tsmatz/roberta_qa_japanese\")\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"tsmatz/roberta_qa_japanese\")\n",
    "\n",
    "text = \"太郎は昨日はカレーを食べました。今日はお腹があまり減っていません。\\\n",
    "    街をぶらぶらと歩いていると、立ち飲み屋が目に入りました。太郎はこのお店が前からちょっと気になっていました。\\\n",
    "    しかし、太郎は入ることなく、そのお店の前を通り抜けていきます。太郎は家に帰ると、チュールを飼い主からもらい、食べました。\"\n",
    "\n",
    "questions = [\"太郎の昨日のご飯は？\", \"太郎の今日のご飯は？\", '太郎は人間ですか？']\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"人の質問: {question}\")\n",
    "    print(f\"AIの回答: {answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やっていること\n",
    "1. チェックポイント名からトークナイザーとモデルをインスタンス化します。モデルは、BERT モデルとして識別され、チェックポイントに格納されている重みでロードされます。\n",
    "2. テキストといくつかの質問を定義する。\n",
    "3. 質問を繰り返し、テキストと現在の質問から、正しいモデル固有のセパレータ、トークンタイプID、アテンションマスクを使ってシーケンスを構築します。\n",
    "4. このシーケンスをモデルに渡します。これにより、開始位置と終了位置の両方について、シーケンストークン（質問とテキスト）全体にわたるスコアの範囲が出力される。\n",
    "5. 結果のソフトマックスを計算し、トークンの確率を得る。\n",
    "6. 識別された開始と終了の値からトークンを取得し、それらのトークンを文字列に変換する。\n",
    "7. 結果を表示する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked LM：穴埋め\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「ドラえもん」というアニメは、日本で人気のアニメーション作品です。このアニメは、小学生の主人公・のび太が、\n",
      "【変身】ができるという能力を持つロボット、ドラえもんと出会い、さまざまな冒険をするという話を描いています。\n",
      "「ドラえもん」というアニメは、日本で人気のアニメーション作品です。このアニメは、小学生の主人公・のび太が、\n",
      "【仕事】ができるという能力を持つロボット、ドラえもんと出会い、さまざまな冒険をするという話を描いています。\n",
      "「ドラえもん」というアニメは、日本で人気のアニメーション作品です。このアニメは、小学生の主人公・のび太が、\n",
      "【飛行】ができるという能力を持つロボット、ドラえもんと出会い、さまざまな冒険をするという話を描いています。\n",
      "「ドラえもん」というアニメは、日本で人気のアニメーション作品です。このアニメは、小学生の主人公・のび太が、\n",
      "【遊び】ができるという能力を持つロボット、ドラえもんと出会い、さまざまな冒険をするという話を描いています。\n",
      "「ドラえもん」というアニメは、日本で人気のアニメーション作品です。このアニメは、小学生の主人公・のび太が、\n",
      "【冒険】ができるという能力を持つロボット、ドラえもんと出会い、さまざまな冒険をするという話を描いています。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "sequence = f\"「ドラえもん」というアニメは、日本で人気のアニメーション作品です。このアニメは、小学生の主人公・のび太が、\\n【{tokenizer.mask_token}】ができるという能力を持つロボット、ドラえもんと出会い、さまざまな冒険をするという話を描いています。\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やっていること\n",
    "1. チェックポイント名からトークナイザーとモデルをインスタンス化します。モデルはBERTモデルとして識別され、チェックポイントに格納された重みでロードされます。\n",
    "2. tokenizer.mask_tokenを単語の代わりに置き、マスクされたトークンを持つシーケンスを定義する。\n",
    "3. そのシーケンスをIDのリストにエンコードし、そのリストにおけるマスクされたトークンの位置を見つけます。\n",
    "4. マスクトークンのインデックスで予測値を取得する。このテンソルは語彙と同じサイズであり、値は各トークンに帰着するスコアである。このモデルはその文脈で可能性が高いと思われるトークンに対して高いスコアを与える。\n",
    "5. PyTorchのtopkメソッドやTensorFlowのtop_kメソッドを用いて上位5つのトークンを取得する。\n",
    "6. マスクトークンをトークンで置き換え、結果を表示する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京から新幹線に乗ると、この ホテル の 街 に 住ん て い た 筆者 。 私 を 探す の は 、 「 そんな お家 は いかにも 憧れ てす ね 。 その 土地 に ある もの か ある 。 たから 、 とう て も あり ませ ん てし た か 。 そのほか の 旅行 に関する アンケート に 聞く 。 その 結果 、 筆者 は ホテル の なか から ちょうと 自分 の 不動産 を 購入 し て い た こと は あり ます か? 自分 は 好き たっ た からす 。 「 月 は 、 とても 幸せ に なり まし た ね 」 の コメント を くれ た 。\n",
      "東京から新幹線に乗ると、沢駅までの片道1時間弱の短い時間で片道20～30分の場所に温泉がある。ホテルの温泉は2つ用意されていた。 宿泊の翌日、ビジネスホテルがお決まりに。そんなわけで宿泊先の温泉宿の玄関で「お客さまありがとうございました」という挨拶を交わした。そして1階のロビーへ出ると、私の手が私の肩を掴んでくる。そしてロビーに案内されて「ありがとうございました」と。こうして旅館でマッサージを受けつつ、3人部屋に案内される。 2人で食事をした時に、テーブルに腰かけてもらった。そう、ベッドに入った瞬間に全身が震えていた。 ホテルではマッサージがメインだったが、マッサージを受けることは初めてだった。足の裏を洗ってもらえると、気持ちがリラックス\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, T5Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering\n",
    "from fugashi import Tagger \n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "tokenizer_gpt2 = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
    "tokenizer_gpt2.do_lower_case = True  # due to some bug of tokenizer config loading\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
    "\n",
    "tokenizer_xlnet = XLNetTokenizer.from_pretrained(\"hajime9652/xlnet-japanese\")\n",
    "model_xlnet = XLNetLMHeadModel.from_pretrained(\"hajime9652/xlnet-japanese\")\n",
    "tagger = Tagger('-Owakati') \n",
    "\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"パソコンを立ち上げてGmailを開くと、とあるホテルからメールが届いていた。メールに中にあった冬景色の中に佇む凛とした佇まいの旅館の写真を見て、あの時の旅行のことを思い出した。\\\n",
    "    あの日は金沢への出張だった。出張とは言っても予定はクライアントと食事をするだけだった。時間に余裕のあった私は街からちょっと外れた隠れ家のような旅館を予約していた。\"\n",
    "\n",
    "prompt = \"東京から新幹線に乗ると、\"\n",
    "inputs = tokenizer_xlnet.encode(tagger.parse(PADDING_TEXT + prompt), add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "prompt_length = len(tokenizer_xlnet.decode(inputs[0]))\n",
    "outputs = model_xlnet.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer_xlnet.decode(outputs[0])[prompt_length + 1 :]\n",
    "\n",
    "print(generated)\n",
    "\n",
    "inputs = tokenizer_gpt2(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer_gpt2.decode(inputs[0]))\n",
    "outputs = model_gpt2.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer_gpt2.decode(outputs[0])[prompt_length + 1 :]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やっていること\n",
    "1. チェックポイント名からトークナイザーとモデルをインスタンス化します。\n",
    "2. モデルはGPT2モデル / XLNetモデルとして識別され、チェックポイントに格納された重みでロードされます。\n",
    "3. 生成したいテキストの前文を定義します。（PADDING TEXTはXLNetには必要な処理です。）\n",
    "4. model.generateでテキストを生成しています。\n",
    "5. 結果を表示しています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition：固有表現認識\n",
    "名前付き固有表現認識（NER）は、トークンをあるクラスに従って分類するタスクで、例えば、トークンを人、組織、場所として識別することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c07f9447ce46218cb874cb9cb411d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238a369033734296aedcd86f7f913339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7afcafeb7e435c9cef42bc0e24d047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3bd8ee5fd844d4ae1a37ba2a66d8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb9d4a498044db0a1b1da6b348f9177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d538c658344d1ade4a3e768ef278e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tsmatz/xlm-roberta-ner-japanese\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"tsmatz/xlm-roberta-ner-japanese\")\n",
    "\n",
    "\n",
    "sequence = (\n",
    "    \"太郎は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った\"\n",
    "    \"女性不足を止めるには、どこで働くかより、いつ働くか\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "outputs = model(**inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やっていること\n",
    "1. チェックポイント名からトークナイザーとモデルをインスタンス化します。\n",
    "2. モデルはBERTモデルとして識別され、チェックポイントに格納された重みでロードされます。\n",
    "3. 既知のエンティティでシーケンスを定義します。\n",
    "4. 単語をトークンに分割し、予測にマッピングできるようにします。まず、シーケンスを完全にエンコード、デコードし、特殊トークンを含む文字列を残すという、ちょっとしたハックを使います。\n",
    "5. その文字列をIDにエンコードします（特殊トークンは自動的に追加されます）。\n",
    "6. 入力をモデルに渡し、最初の出力を得ることで予測値を取得します。この結果、各トークンについて9つの可能なクラスに対する分布が得られます。各トークンについて最も可能性の高いクラスを取得するために、argmaxを取ります。\n",
    "7. 各トークンとその予測値を束ねて表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tsmatz/mt5_summarize_japanese\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"tsmatz/mt5_summarize_japanese\")\n",
    "\n",
    "ARTICLE = \"ロジスティック写像（ロジスティックしゃぞう、英語: logistic map）とは、xn+1 = axn(1 − xn) という2次関数の差分方程式（漸化式）で定められた離散力学系である。\\\n",
    "    ロジスティックマップ[1][2][3]や離散型ロジスティック方程式（英語: discrete logistic equation）[4][5][6]とも呼ばれる。\\\n",
    "    単純な2次関数の式でありながら、驚くような複雑な振る舞いを生み出すことで知られる。\\\n",
    "    ロジスティック写像の a はパラメータと呼ばれる定数、x が変数で、適当に a の値を決め、最初の x0 を決めて計算すると、x0, x1, x2, … という数列が得られる。\\\n",
    "    この数列を力学系分野では軌道と呼び、軌道は a にどのような値を与えるかによって変化する。\\\n",
    "    パラメータ a を変化させると、ロジスティック写像の軌道は、一つの値へ落ち着いたり、いくつかの値を周期的に繰り返したり、カオスと呼ばれる非周期的変動を示したりと様々に変化する。\\\n",
    "    ロジスティック写像を生物の個体数を表すモデルとして見る立場からは、変数 xn は1世代目、2世代目…というように世代ごとに表した個体数を意味しており、\\\n",
    "    ロジスティック写像とは現在の個体数 xn から次の世代の個体数 xn+1 を計算する式である。\\\n",
    "    生物個体数モデルとしてのロジスティック写像は、ある生物の個体数がある環境中に生息し、さらにその環境と外部との間で個体の移出入がないような状況を想定しており、\\\n",
    "    xn は正確には個体数そのものではなく、その環境中に存在できる最大個体数に対する割合を意味する。\\\n",
    "    微分方程式で個体数をモデリングするロジスティック方程式の離散化からもロジスティック写像は導出でき、「ロジスティック写像」という名もそのことに由来する。\\\n",
    "    2次関数の力学系としての研究は20世紀初頭からあったが、1970年代、特に数理生物学者ロバート・メイの研究によってロジスティック写像は広く知られるようになった。\\\n",
    "    メイ以外にも、スタニスワフ・ウラムとジョン・フォン・ノイマン、ペッカ・ミュルバーク（フィンランド語版）、\\\n",
    "    オレクサンドル・シャルコフスキー（ウクライナ語版）、ニコラス・メトロポリス（英語版）ら、ミッチェル・ファイゲンバウムなどがロジスティック写像の振る舞い解明に関わる仕事を成している。\"\n",
    "\n",
    "inputs = tokenizer(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やっていること\n",
    "1. チェックポイント名からトークナイザーとモデルをインスタンス化します。要約は通常、Bart や T5 のようなエンコーダ・デコーダモデルを用いて行われます。\n",
    "2. 要約されるべき記事を定義します。\n",
    "4. T5特有の接頭辞 \"summarize: \"が必要です。\n",
    "5. PreTrainedModel.generate() メソッドを用いて要約を生成します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55c175c487542f19980a343c5f1fa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9406f05c3b194f6ba4f3ef2392b0ab12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/494 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93848662e8e34acdbfd8b5f0d5634d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2f12e3901f4e7ab8f03d933bdfadff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a38e0e742a4a8aa63afeb795e1e4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"ken11/mbart-ja-en\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"ken11/mbart-ja-en\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"謹んで新春をお祝い申し上げます。\\\n",
    "    旧年中は大変お世話になり、誠にありがとうございました。○○様におかれましては、今年のお正月も、ご家族と共に楽しくお過ごしのことと存じます。\\\n",
    "    今年は開発部に異動となり、心機一転、あらためて業務に努めていく所存でございます。今年も変わらぬご指導ご鞭撻のほど何卒よろしくお願い申し上げます。\\\n",
    "    本年も御家族の皆様のご多幸を心からお祈り申し上げます。\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=40, num_beams=4, early_stopping=True)\n",
    "\n",
    "translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"], early_stopping=True, max_length=48)\n",
    "pred = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "print(pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やっていること\n",
    "1. チェックポイント名からトークナイザーとモデルをインスタンス化します。要約は通常、Bart や T5 のようなエンコーダ・デコーダモデルを用いて行われます。\n",
    "2. 要約されるべき記事を定義します。\n",
    "3. T5特有のプレフィックス \"translate Japanese to English: \" を追加します。\n",
    "4. PreTrainedModel.generate()メソッドを使用して翻訳を実行します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習\n",
    "- 上記のタスク全てで、オリジナルの入力データを使うことで、挙動を確認してください。\n",
    "- どういった入力で、どういった出力になるかを観察してください。たとえばテキスト生成では、同じ文が繰り返し出力されるなどは既知の問題です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eaac4ee8e735b6cee17c1b0358cfc8773b6176eb143c6cf3f1e52e7612ffbee2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
