{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp3XPuaTu9jl"
      },
      "source": [
        "# 最新版の日本語LLM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLvv6UaPa33"
      },
      "source": [
        "### はじめに\n",
        "日本語のLLMは、オープンソースで公開されたLLMを日本語データセットで学習させることで公開されます。\n",
        "現在はLLaMA 2（Metaによって開発されたLLM）が主な対象となって公開されています。これらのLLMは、ChatGPTの代替となるLLMがオープンソースで公開される流れをとっています。ChatGPT 3.5と比較する形でLLaMA2が公開されました。\n",
        "ChatGPTには4.0があります。それに対応するLLMは、MiteriaというLLMがあります。これの日本語版のLLMも次期に公開されるでしょう。\n",
        "\n",
        "### Swallowの概要\n",
        "Swallow は、Llama 2の日本語能力を強化した大規模言語モデルで、パラメータサイズに応じて 7B、13B、70B の三種類があります。このモデルは、東京工業大学の岡崎研究室と横田研究室、国立研究開発法人産業技術総合研究所が共同で開発しました。モデルの特徴は、英語に加えて日本語の理解・生成能力に特化しており、継続的な事前学習によって日本語データの扱いを改善しています。\n",
        "\n",
        "### 主な特徴\n",
        "- 継続事前学習: Llama 2モデルに日本語の語彙を追加し、新たに開発した日本語データで継続的に学習を行っています。\n",
        "- 高性能: 2023年12月時点で、日本語における最高性能を誇る大規模言語モデルです。\n",
        "- 利用の自由度: LLAMA 2 Community Licenseに従い、研究や商業目的で自由に利用可能です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbzZ_IVTtoQe",
        "outputId": "7a6316f4-4fd1-419f-8ba5-ecee8f2bf59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers sentencepiece accelerate protobuf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ue2kOQhXTAMU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"tokyotech-llm/Swallow-7b-instruct-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, device_map=\"auto\")\n",
        "\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。\"\n",
        "        \"リクエストを適切に完了するための回答を記述してください。\\n\\n\"\n",
        "        \"### 指示:\\n{instruction}\\n\\n### 入力:\\n{input}\\n\\n### 応答:\"\n",
        "\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"以下に、あるタスクを説明する指示があります。\"\n",
        "        \"リクエストを適切に完了するための回答を記述してください。\\n\\n\"\n",
        "        \"### 指示:\\n{instruction}\\n\\n### 応答:\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def create_prompt(instruction, input=None):\n",
        "    \"\"\"\n",
        "    Generates a prompt based on the given instruction and an optional input.\n",
        "    If input is provided, it uses the 'prompt_input' template from PROMPT_DICT.\n",
        "    If no input is provided, it uses the 'prompt_no_input' template.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The instruction describing the task.\n",
        "        input (str, optional): Additional input providing context for the task. Default is None.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt.\n",
        "    \"\"\"\n",
        "    if input:\n",
        "        # Use the 'prompt_input' template when additional input is provided\n",
        "        return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input=input)\n",
        "    else:\n",
        "        # Use the 'prompt_no_input' template when no additional input is provided\n",
        "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "instruction_example = \"以下のトピックに関する詳細な情報をリストアップして、提案とその妥当性を示します。\"\n",
        "input_example = \"福岡のご飯が美味しいと評判ですが、実際に観光地としてどういった魅力があるでしょうか。どんな観光体験が福岡に必要でしょうか\"\n",
        "prompt = create_prompt(instruction_example, input_example)\n",
        "\n",
        "input_ids = tokenizer.encode(\n",
        "    prompt,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "tokens = model.generate(\n",
        "    input_ids.to(device=model.device),\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.99,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "out = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(out.replace(\"。\", \"。\\n\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nekomataの概要\n",
        "rinnaでは、日本語の処理に適したGPT・BERT・HuBERT・CLIP・Stable Diffusionなどのテキスト・音声・画像に関する事前学習済み基盤モデルを公開することで、日本語のAI開発をサポートしてきました。2021年4月からrinnaが公開してきたモデルのダウンロード数は累計450万を超え、多くの研究・開発者にご利用いただいています。2023年10月には、英語を主なターゲットとし優れたパフォーマンスを誇るMeta社のLlama2に、日本語継続事前学習をすることで日本語においても高いパフォーマンスを実現した「Youri」シリーズを開発・公開しています。しかしYouriシリーズは、Llama2の英語を主なターゲットとする語彙サイズ3.2万のトークナイザーを利用しており、日本語を書き表すための語彙が不足しており推論効率が悪いという欠点がありました。Llama2以降には、英語だけでなく多言語をターゲットとした語彙サイズが大きいLLMが次々と公開されました。アリババ社が公開したQwenシリーズは語彙サイズが15.2万であり、日本語の推論においても高い効率を実現しています。\n",
        "\n",
        "そこでrinnaは、Qwen-7Bと14Bに日本語の学習データを用いて継続事前学習を行い、高い日本語テキストの生成性能と推論速度を兼ね備えた「Nekomata」シリーズを開発し公開しました。さらに、汎用言語モデルに対話形式でユーザーの指示に応答するように追加学習した指示応答言語モデルも開発し、合計4モデルを公開しました。これらのモデル公開により、日本のAI研究・開発の更なる発展につながることを願っています。\n",
        "\n",
        "・「Nekomata」シリーズのモデル一覧\n",
        "\n",
        "https://huggingface.co/collections/rinna/nekomata-6582b5134ee85531becbb9a9\n",
        "\n",
        "### 主な特徴\n",
        "Nekomata 7Bと14Bは、70億パラメータのQwen-7B ( https://huggingface.co/Qwen/Qwen-7B ) と140億パラメータのQwen-14B ( https://huggingface.co/Qwen/Qwen-14B ) に対して、日本語と英語の学習データを用いてそれぞれ300億と660億トークンで継続事前学習したモデルです。Qwenの優れたパフォーマンスを日本語に引き継いでおり、日本語のタスクにおいて高い性能を示します。日本語言語モデルの性能を評価するためのベンチマークの一つである Stability-AI/lm-evaluation-harnessの9タスク平均スコアはNekomata 7Bが58.69、Nekomata 14Bが67.38なっています（図1）。また、日本語テキスト1byteに対するトークン数はLlama2/Youriが0.40、Qwen/Nekomataが0.24であり、推論効率が高くなっています。モデル名の由来は、妖怪の「猫又（ねこまた）」からきています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/nekomata-14b-instruction\", trust_remote_code=True)\n",
        "\n",
        "# Use GPU with bf16\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"rinna/nekomata-14b-instruction\", device_map=\"auto\", trust_remote_code=True, bf16=True)\n",
        "\n",
        "# Use GPU with fp16\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"rinna/nekomata-14b-instruction\", device_map=\"auto\", trust_remote_code=True, fp16=True)\n",
        "\n",
        "# Use CPU\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"rinna/nekomata-14b-instruction\", device_map=\"cpu\", trust_remote_code=True)\n",
        "\n",
        "# Automatically select device and precision\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/nekomata-14b-instruction\", device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "instruction_example = \"以下のトピックに関する詳細な情報をリストアップして、提案とその妥当性を示します。\"\n",
        "input_example = \"福岡のご飯が美味しいと評判ですが、実際に観光地としてどういった魅力があるでしょうか。どんな観光体験が福岡に必要でしょうか\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
        "\n",
        "### 指示:\n",
        "{instruction_example}\n",
        "\n",
        "### 入力:\n",
        "{input_example}\n",
        "\n",
        "### 応答:\n",
        "\"\"\"\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "out = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(out.replace(\"。\", \"。\\n\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "82e52f2285c871893785b6211b1b0fb91a1e90d57630bc3da094798a3ae6fb15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
